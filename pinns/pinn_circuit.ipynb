{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c079d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam, LBFGS\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class RLC_PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for RLC Circuit\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, R=10.0, L=0.1, C=0.01):\n",
    "        super(RLC_PINN, self).__init__()\n",
    "        \n",
    "        # Circuit parameters\n",
    "        self.R = R  # Resistance (Ohms)\n",
    "        self.L = L  # Inductance (Henry)\n",
    "        self.C = C  # Capacitance (Farad)\n",
    "        \n",
    "        # Network layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.linears.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n",
    "        for m in self.linears:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Forward pass: t -> Q(t)\n",
    "        Input: time t\n",
    "        Output: charge Q(t)\n",
    "        \"\"\"\n",
    "        x = t\n",
    "        for i, layer in enumerate(self.linears[:-1]):\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.linears[-1](x)  # No activation on output\n",
    "        return x\n",
    "    \n",
    "    def voltage_source(self, t):\n",
    "        \"\"\"\n",
    "        Voltage source function: V(t) = 5V step function\n",
    "        \"\"\"\n",
    "        return 5.0 * torch.ones_like(t)  # 5V DC step\n",
    "    \n",
    "    def compute_current(self, t):\n",
    "        \"\"\"\n",
    "        Compute current I(t) = dQ/dt using automatic differentiation\n",
    "        \"\"\"\n",
    "        t.requires_grad_(True)\n",
    "        Q = self.forward(t)\n",
    "        \n",
    "        I = autograd.grad(Q, t,\n",
    "                         grad_outputs=torch.ones_like(Q),\n",
    "                         create_graph=True)[0]\n",
    "        return I\n",
    "    \n",
    "    def physics_loss(self, t_physics):\n",
    "        \"\"\"\n",
    "        Compute physics loss based on RLC circuit equations\n",
    "        L * d²Q/dt² + R * dQ/dt + Q/C = V_source(t)\n",
    "        \"\"\"\n",
    "        t_physics.requires_grad_(True)\n",
    "        Q = self.forward(t_physics)\n",
    "        \n",
    "        # First derivative: dQ/dt = I(t)\n",
    "        Q_t = autograd.grad(Q, t_physics,\n",
    "                           grad_outputs=torch.ones_like(Q),\n",
    "                           create_graph=True)[0]\n",
    "        \n",
    "        # Second derivative: d²Q/dt² = dI/dt\n",
    "        Q_tt = autograd.grad(Q_t, t_physics,\n",
    "                            grad_outputs=torch.ones_like(Q_t),\n",
    "                            create_graph=True)[0]\n",
    "        \n",
    "        # Voltage source\n",
    "        V_source = self.voltage_source(t_physics)\n",
    "        \n",
    "        # RLC equation residual\n",
    "        # L * d²Q/dt² + R * dQ/dt + Q/C - V_source(t) = 0\n",
    "        residual = self.L * Q_tt + self.R * Q_t + Q / self.C - V_source\n",
    "        \n",
    "        loss_physics = torch.mean(residual**2)\n",
    "        return loss_physics\n",
    "    \n",
    "    def initial_conditions_loss(self, t_initial):\n",
    "        \"\"\"\n",
    "        Enforce initial conditions:\n",
    "        Q(0) = 0 (no initial charge)\n",
    "        I(0) = dQ/dt(0) = 0 (no initial current)\n",
    "        \"\"\"\n",
    "        t_initial.requires_grad_(True)\n",
    "        Q_initial = self.forward(t_initial)\n",
    "        \n",
    "        # dQ/dt at t=0\n",
    "        I_initial = autograd.grad(Q_initial, t_initial,\n",
    "                                 grad_outputs=torch.ones_like(Q_initial),\n",
    "                                 create_graph=True)[0]\n",
    "        \n",
    "        # Initial condition losses\n",
    "        loss_Q_initial = torch.mean(Q_initial**2)  # Q(0) = 0\n",
    "        loss_I_initial = torch.mean(I_initial**2)  # I(0) = 0\n",
    "        \n",
    "        return loss_Q_initial + loss_I_initial\n",
    "    \n",
    "    def total_loss(self, t_physics, t_initial, t_data=None, Q_data=None):\n",
    "        \"\"\"\n",
    "        Compute total loss function\n",
    "        \"\"\"\n",
    "        # Physics loss\n",
    "        loss_physics = self.physics_loss(t_physics)\n",
    "        \n",
    "        # Initial conditions loss\n",
    "        loss_initial = self.initial_conditions_loss(t_initial)\n",
    "        \n",
    "        # Data loss (if available)\n",
    "        loss_data = torch.tensor(0.0)\n",
    "        if t_data is not None and Q_data is not None:\n",
    "            Q_pred = self.forward(t_data)\n",
    "            loss_data = torch.mean((Q_pred - Q_data)**2)\n",
    "        \n",
    "        # Weighted total loss\n",
    "        lambda_physics = 1.0\n",
    "        lambda_initial = 100.0  # Higher weight for initial conditions\n",
    "        lambda_data = 10.0\n",
    "        \n",
    "        total = (lambda_physics * loss_physics + \n",
    "                lambda_initial * loss_initial + \n",
    "                lambda_data * loss_data)\n",
    "        \n",
    "        return total, loss_physics, loss_initial, loss_data\n",
    "\n",
    "def analytical_solution(t, R=10.0, L=0.1, C=0.01, V0=5.0):\n",
    "    \"\"\"\n",
    "    Analytical solution for RLC circuit with step input\n",
    "    \"\"\"\n",
    "    # Calculate circuit parameters\n",
    "    omega_0 = 1.0 / np.sqrt(L * C)  # Natural frequency\n",
    "    alpha = R / (2 * L)             # Damping coefficient\n",
    "    \n",
    "    # Discriminant for determining solution type\n",
    "    discriminant = alpha**2 - omega_0**2\n",
    "    \n",
    "    if discriminant > 0:\n",
    "        # Overdamped case\n",
    "        s1 = -alpha + np.sqrt(discriminant)\n",
    "        s2 = -alpha - np.sqrt(discriminant)\n",
    "        \n",
    "        A = -V0 * C * s2 / (s1 - s2)\n",
    "        B = V0 * C * s1 / (s1 - s2)\n",
    "        \n",
    "        Q = V0 * C + A * np.exp(s1 * t) + B * np.exp(s2 * t)\n",
    "        \n",
    "    elif discriminant < 0:\n",
    "        # Underdamped case (most common)\n",
    "        omega_d = np.sqrt(omega_0**2 - alpha**2)  # Damped frequency\n",
    "        \n",
    "        Q = V0 * C * (1 - np.exp(-alpha * t) * \n",
    "                      (np.cos(omega_d * t) + (alpha / omega_d) * np.sin(omega_d * t)))\n",
    "        \n",
    "    else:\n",
    "        # Critically damped case\n",
    "        Q = V0 * C * (1 - (1 + alpha * t) * np.exp(-alpha * t))\n",
    "    \n",
    "    return Q\n",
    "\n",
    "def train_rlc_pinn():\n",
    "    \"\"\"\n",
    "    Train the RLC circuit PINN\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RLC CIRCUIT MODELING WITH PHYSICS-INFORMED NEURAL NETWORKS\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    print(\"Circuit Parameters:\")\n",
    "    print(\"- Resistance (R): 10.0 Ω\")\n",
    "    print(\"- Inductance (L): 0.1 H\") \n",
    "    print(\"- Capacitance (C): 0.01 F\")\n",
    "    print(\"- Voltage Source: 5V step function\")\n",
    "    print()\n",
    "    \n",
    "    # Circuit parameters\n",
    "    R, L, C = 10.0, 0.1, 0.01\n",
    "    \n",
    "    # Time domain\n",
    "    t_min, t_max = 0.0, 1.0\n",
    "    \n",
    "    # Training points\n",
    "    n_physics = 1000\n",
    "    n_initial = 10\n",
    "    \n",
    "    # Physics collocation points\n",
    "    t_physics = torch.linspace(t_min, t_max, n_physics).reshape(-1, 1)\n",
    "    t_physics.requires_grad_(True)\n",
    "    \n",
    "    # Initial condition points\n",
    "    t_initial = torch.zeros(n_initial, 1)\n",
    "    \n",
    "    # Initialize network\n",
    "    layers = [1, 50, 50, 50, 1]  # 1 input (time), 3 hidden layers, 1 output (charge)\n",
    "    model = RLC_PINN(layers, R=R, L=L, C=C)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 10000\n",
    "    print_interval = 2000\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    print()\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute losses\n",
    "        total_loss, loss_physics, loss_initial, loss_data = model.total_loss(\n",
    "            t_physics, t_initial\n",
    "        )\n",
    "        \n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(total_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % print_interval == 0:\n",
    "            print(f\"Epoch {epoch:5d} | \"\n",
    "                  f\"Total Loss: {total_loss.item():.6f} | \"\n",
    "                  f\"Physics: {loss_physics.item():.6f} | \"\n",
    "                  f\"Initial: {loss_initial.item():.6f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Training completed!\")\n",
    "    print()\n",
    "    \n",
    "    return model, loss_history\n",
    "\n",
    "def fine_tune_with_lbfgs(model, t_physics, t_initial):\n",
    "    \"\"\"\n",
    "    Fine-tune the model using L-BFGS optimizer\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning with L-BFGS optimizer...\")\n",
    "    \n",
    "    # L-BFGS optimizer\n",
    "    optimizer_lbfgs = LBFGS(model.parameters(), \n",
    "                           lr=1.0, \n",
    "                           max_iter=1000,\n",
    "                           max_eval=1000,\n",
    "                           tolerance_grad=1e-7,\n",
    "                           tolerance_change=1.0*np.finfo(float).eps,\n",
    "                           history_size=50,\n",
    "                           line_search_fn=\"strong_wolfe\")\n",
    "    \n",
    "    def closure():\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "        total_loss, _, _, _ = model.total_loss(t_physics, t_initial)\n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "    \n",
    "    optimizer_lbfgs.step(closure)\n",
    "    \n",
    "    # Final loss\n",
    "    with torch.no_grad():\n",
    "        final_loss, _, _, _ = model.total_loss(t_physics, t_initial)\n",
    "        print(f\"Final loss after L-BFGS: {final_loss.item():.8f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def analyze_results(model):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the results\n",
    "    \"\"\"\n",
    "    print(\"Analyzing results...\")\n",
    "    \n",
    "    # Test time points\n",
    "    t_test = torch.linspace(0, 1, 200).reshape(-1, 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # PINN predictions\n",
    "        Q_pred = model.forward(t_test)\n",
    "        I_pred = model.compute_current(t_test)\n",
    "        \n",
    "        # Convert to numpy for plotting\n",
    "        t_np = t_test.numpy().flatten()\n",
    "        Q_pred_np = Q_pred.numpy().flatten()\n",
    "        I_pred_np = I_pred.numpy().flatten()\n",
    "        \n",
    "        # Analytical solution\n",
    "        Q_analytical = analytical_solution(t_np, R=model.R, L=model.L, C=model.C)\n",
    "        I_analytical = np.gradient(Q_analytical, t_np)  # dQ/dt\n",
    "        \n",
    "        # Voltage across components\n",
    "        V_R = model.R * I_pred_np\n",
    "        V_L = model.L * np.gradient(I_pred_np, t_np)\n",
    "        V_C = Q_pred_np / model.C\n",
    "        V_source = 5.0 * np.ones_like(t_np)\n",
    "    \n",
    "    # Calculate errors\n",
    "    mse_Q = np.mean((Q_pred_np - Q_analytical)**2)\n",
    "    mse_I = np.mean((I_pred_np - I_analytical)**2)\n",
    "    \n",
    "    print(f\"Mean Squared Error (Charge): {mse_Q:.8f}\")\n",
    "    print(f\"Mean Squared Error (Current): {mse_I:.8f}\")\n",
    "    print()\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('RLC Circuit Analysis with PINNs', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Charge vs Time\n",
    "    axes[0, 0].plot(t_np, Q_analytical, 'r-', linewidth=2, label='Analytical')\n",
    "    axes[0, 0].plot(t_np, Q_pred_np, 'b--', linewidth=2, label='PINN')\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Charge Q(t) (C)')\n",
    "    axes[0, 0].set_title('Capacitor Charge')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Current vs Time\n",
    "    axes[0, 1].plot(t_np, I_analytical, 'r-', linewidth=2, label='Analytical')\n",
    "    axes[0, 1].plot(t_np, I_pred_np, 'b--', linewidth=2, label='PINN')\n",
    "    axes[0, 1].set_xlabel('Time (s)')\n",
    "    axes[0, 1].set_ylabel('Current I(t) (A)')\n",
    "    axes[0, 1].set_title('Circuit Current')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Error Analysis\n",
    "    error_Q = np.abs(Q_pred_np - Q_analytical)\n",
    "    error_I = np.abs(I_pred_np - I_analytical)\n",
    "    axes[0, 2].plot(t_np, error_Q, 'g-', linewidth=2, label='Charge Error')\n",
    "    axes[0, 2].plot(t_np, error_I, 'm-', linewidth=2, label='Current Error')\n",
    "    axes[0, 2].set_xlabel('Time (s)')\n",
    "    axes[0, 2].set_ylabel('Absolute Error')\n",
    "    axes[0, 2].set_title('Prediction Errors')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].set_yscale('log')\n",
    "    \n",
    "    # Plot 4: Component Voltages\n",
    "    axes[1, 0].plot(t_np, V_source, 'k-', linewidth=2, label='Source V(t)')\n",
    "    axes[1, 0].plot(t_np, V_R, 'r-', linewidth=2, label='Resistor V_R')\n",
    "    axes[1, 0].plot(t_np, V_L, 'g-', linewidth=2, label='Inductor V_L')\n",
    "    axes[1, 0].plot(t_np, V_C, 'b-', linewidth=2, label='Capacitor V_C')\n",
    "    axes[1, 0].set_xlabel('Time (s)')\n",
    "    axes[1, 0].set_ylabel('Voltage (V)')\n",
    "    axes[1, 0].set_title('Component Voltages')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Kirchhoff's Voltage Law Check\n",
    "    V_sum = V_R + V_L + V_C\n",
    "    axes[1, 1].plot(t_np, V_source, 'k-', linewidth=2, label='V_source')\n",
    "    axes[1, 1].plot(t_np, V_sum, 'r--', linewidth=2, label='V_R + V_L + V_C')\n",
    "    axes[1, 1].set_xlabel('Time (s)')\n",
    "    axes[1, 1].set_ylabel('Voltage (V)')\n",
    "    axes[1, 1].set_title(\"Kirchhoff's Voltage Law Verification\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Phase Portrait (Current vs Charge)\n",
    "    axes[1, 2].plot(Q_analytical, I_analytical, 'r-', linewidth=2, label='Analytical')\n",
    "    axes[1, 2].plot(Q_pred_np, I_pred_np, 'b--', linewidth=2, label='PINN')\n",
    "    axes[1, 2].set_xlabel('Charge Q(t) (C)')\n",
    "    axes[1, 2].set_ylabel('Current I(t) (A)')\n",
    "    axes[1, 2].set_title('Phase Portrait (I vs Q)')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print circuit analysis\n",
    "    print(\"Circuit Analysis:\")\n",
    "    print(f\"- Natural frequency ω₀: {1/np.sqrt(model.L * model.C):.2f} rad/s\")\n",
    "    print(f\"- Damping coefficient α: {model.R/(2*model.L):.2f} s⁻¹\")\n",
    "    \n",
    "    discriminant = (model.R/(2*model.L))**2 - (1/(model.L*model.C))\n",
    "    if discriminant > 0:\n",
    "        print(\"- Circuit type: Overdamped\")\n",
    "    elif discriminant < 0:\n",
    "        print(\"- Circuit type: Underdamped\")\n",
    "        omega_d = np.sqrt(1/(model.L*model.C) - (model.R/(2*model.L))**2)\n",
    "        print(f\"- Damped frequency ωd: {omega_d:.2f} rad/s\")\n",
    "    else:\n",
    "        print(\"- Circuit type: Critically damped\")\n",
    "    \n",
    "    print(f\"- Final steady-state charge: {Q_pred_np[-1]:.4f} C\")\n",
    "    print(f\"- Theoretical steady-state: {5.0 * model.C:.4f} C\")\n",
    "\n",
    "def demonstrate_parameter_discovery():\n",
    "    \"\"\"\n",
    "    Demonstrate inverse problem: discovering circuit parameters from data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INVERSE PROBLEM: PARAMETER DISCOVERY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate synthetic \"experimental\" data\n",
    "    t_data = torch.linspace(0, 1, 50).reshape(-1, 1)\n",
    "    R_true, L_true, C_true = 15.0, 0.05, 0.02  # Unknown parameters\n",
    "    \n",
    "    # Generate noisy data\n",
    "    with torch.no_grad():\n",
    "        Q_true = torch.tensor(analytical_solution(t_data.numpy().flatten(), \n",
    "                                                 R=R_true, L=L_true, C=C_true), \n",
    "                             dtype=torch.float32).reshape(-1, 1)\n",
    "        # Add noise\n",
    "        noise_level = 0.02\n",
    "        Q_data = Q_true + noise_level * torch.randn_like(Q_true)\n",
    "    \n",
    "    class InverseRLC_PINN(RLC_PINN):\n",
    "        \"\"\"Extended PINN that learns circuit parameters\"\"\"\n",
    "        def __init__(self, layers):\n",
    "            super().__init__(layers, R=1.0, L=1.0, C=1.0)  # Initial guess\n",
    "            \n",
    "            # Make parameters learnable\n",
    "            self.R = nn.Parameter(torch.tensor(10.0))  # Initial guess\n",
    "            self.L = nn.Parameter(torch.tensor(0.1))   # Initial guess  \n",
    "            self.C = nn.Parameter(torch.tensor(0.01))  # Initial guess\n",
    "    \n",
    "    # Initialize inverse model\n",
    "    inverse_model = InverseRLC_PINN([1, 50, 50, 50, 1])\n",
    "    optimizer = Adam(inverse_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training points\n",
    "    t_physics = torch.linspace(0, 1, 500).reshape(-1, 1)\n",
    "    t_initial = torch.zeros(10, 1)\n",
    "    \n",
    "    print(\"Training inverse model to discover parameters...\")\n",
    "    print(f\"True parameters: R={R_true}Ω, L={L_true}H, C={C_true}F\")\n",
    "    print()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss, _, _, _ = inverse_model.total_loss(\n",
    "            t_physics, t_initial, t_data, Q_data\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {total_loss.item():.6f} | \"\n",
    "                  f\"R: {inverse_model.R.item():.2f} | \"\n",
    "                  f\"L: {inverse_model.L.item():.4f} | \" \n",
    "                  f\"C: {inverse_model.C.item():.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Parameter Discovery Results:\")\n",
    "    print(f\"Discovered R: {inverse_model.R.item():.2f} Ω (True: {R_true} Ω)\")\n",
    "    print(f\"Discovered L: {inverse_model.L.item():.4f} H (True: {L_true} H)\")\n",
    "    print(f\"Discovered C: {inverse_model.C.item():.4f} F (True: {C_true} F)\")\n",
    "    \n",
    "    # Calculate errors\n",
    "    R_error = abs(inverse_model.R.item() - R_true) / R_true * 100\n",
    "    L_error = abs(inverse_model.L.item() - L_true) / L_true * 100  \n",
    "    C_error = abs(inverse_model.C.item() - C_true) / C_true * 100\n",
    "    \n",
    "    print(f\"Parameter Errors: R: {R_error:.1f}%, L: {L_error:.1f}%, C: {C_error:.1f}%\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the RLC circuit PINN example\n",
    "    \"\"\"\n",
    "    print(\"RLC Circuit PINN Example\")\n",
    "    print(\"Series RLC Circuit Diagram:\")\n",
    "    print(\"\"\"\n",
    "    Correct Series RLC Circuit:\n",
    "    \n",
    "    +──────[R=10Ω]──────[L=0.1H]──────[C=0.01F]──────+\n",
    "    |                                                |\n",
    "    |                                                |\n",
    "    +                                                |\n",
    "   V(t)                                              |\n",
    "   5V DC                                             |\n",
    "    -                                                |\n",
    "    |                                                |\n",
    "    +────────────────────────────────────────────────+\n",
    "                         GND\n",
    "\n",
    "    Linear representation:\n",
    "    \n",
    "    V_source ──[R=10Ω]──[L=0.1H]──[C=0.01F]── GND\n",
    "       5V        ↓         ↓         ↓\n",
    "              V_R=I×R   V_L=L×dI/dt  V_C=Q/C\n",
    "\n",
    "    Current flows in ONE PATH: I(t) →→→→→→→→→→→→→→→→→\n",
    "    \n",
    "    Components connected in SERIES (no branches):\n",
    "    • Same current I(t) flows through all components\n",
    "    • Voltages add up: V_source = V_R + V_L + V_C\n",
    "    \"\"\")\n",
    "    \n",
    "    # Train the forward model\n",
    "    model, loss_history = train_rlc_pinn()\n",
    "    \n",
    "    # Fine-tune with L-BFGS\n",
    "    t_physics = torch.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    t_initial = torch.zeros(10, 1)\n",
    "    fine_tune_with_lbfgs(model, t_physics, t_initial)\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_results(model)\n",
    "    \n",
    "    # Demonstrate parameter discovery\n",
    "    demonstrate_parameter_discovery()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
