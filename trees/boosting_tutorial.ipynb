{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617375f9",
   "metadata": {},
   "source": [
    "# Guide to Gradient Boosting Algorithms\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The Core Principle\n",
    "\n",
    "Gradient boosting constructs a strong predictor by sequentially adding weak learners that minimize a differentiable loss function. The ensemble model takes the form:\n",
    "\n",
    "$$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\nu \\cdot h_m(\\mathbf{x})$$\n",
    "\n",
    "where:\n",
    "- $F_m(\\mathbf{x})$ is the ensemble prediction after $m$ iterations\n",
    "- $h_m(\\mathbf{x})$ is the $m$-th weak learner (typically a decision tree)\n",
    "- $\\nu$ is the learning rate (shrinkage parameter)\n",
    "\n",
    "### Loss Function Framework\n",
    "\n",
    "For a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, we minimize the total loss:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\sum_{i=1}^n L(y_i, F(\\mathbf{x}_i))$$\n",
    "\n",
    "The key insight is that each new weak learner should fit the **negative gradients** of the loss function:\n",
    "\n",
    "$$r_{im} = -\\frac{\\partial L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)}$$\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "| Loss Function | $L(y, F)$ | First Derivative | Second Derivative |\n",
    "|---------------|-----------|------------------|-------------------|\n",
    "| **MSE** | $\\frac{1}{2}(y - F)^2$ | $F - y$ | $1$ |\n",
    "| **MAE** | $\\|y - F\\|$ | $\\text{sign}(F - y)$ | $0$ (undefined at $F=y$) |\n",
    "| **LogLoss** | $-[y \\log(p) + (1-y) \\log(1-p)]$ | $p - y$ | $p(1-p)$ |\n",
    "| **Huber** | $\\begin{cases} \\frac{1}{2}(y-F)^2 & \\text{if } \\|y-F\\| \\leq \\delta \\\\ \\delta\\|y-F\\| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$ | $\\begin{cases} F - y & \\text{if } \\|y-F\\| \\leq \\delta \\\\ \\delta \\cdot \\text{sign}(F-y) & \\text{otherwise} \\end{cases}$ | $\\begin{cases} 1 & \\text{if } \\|y-F\\| \\leq \\delta \\\\ 0 & \\text{otherwise} \\end{cases}$ |\n",
    "\n",
    "where $p = \\frac{1}{1 + e^{-F}}$ for LogLoss.\n",
    "\n",
    "### Basic Algorithm Structure\n",
    "\n",
    "```\n",
    "ALGORITHM: Gradient Boosting\n",
    "INPUT: Training data {(x_i, y_i)}, loss function L, number of iterations M\n",
    "OUTPUT: Ensemble model F_M(x)\n",
    "\n",
    "1. Initialize: F_0(x) = arg min_γ Σ L(y_i, γ)\n",
    "\n",
    "2. FOR m = 1 to M:\n",
    "   a) Compute pseudo-residuals:\n",
    "      r_im = -∂L(y_i, F_{m-1}(x_i)) / ∂F_{m-1}(x_i)\n",
    "   \n",
    "   b) Train weak learner h_m:\n",
    "      h_m = TRAIN_TREE({(x_i, r_im)})\n",
    "   \n",
    "   c) Find optimal step size:\n",
    "      γ_m = arg min_γ Σ L(y_i, F_{m-1}(x_i) + γ·h_m(x_i))\n",
    "   \n",
    "   d) Update ensemble:\n",
    "      F_m(x) = F_{m-1}(x) + ν·γ_m·h_m(x)\n",
    "\n",
    "3. RETURN F_M(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## First-Order vs Second-Order Methods\n",
    "\n",
    "### First-Order Methods (Traditional Gradient Boosting)\n",
    "\n",
    "**Mathematical Basis:**\n",
    "Uses only the first derivative of the loss function. Each tree is trained to predict the negative gradients:\n",
    "\n",
    "$$h_m(\\mathbf{x}) \\approx -g_i = -\\frac{\\partial L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)}$$\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and robust implementation\n",
    "- Works with any differentiable loss function\n",
    "- Lower computational cost per iteration\n",
    "\n",
    "**Disadvantages:**\n",
    "- Linear convergence rate\n",
    "- May require more iterations to converge\n",
    "- Less principled approach to optimization\n",
    "\n",
    "```\n",
    "ALGORITHM: First-Order Gradient Boosting\n",
    "FOR each iteration m:\n",
    "    1. gradients[i] = -∂L(y[i], current_prediction[i]) / ∂F\n",
    "    2. tree = TRAIN_TREE(features, gradients)\n",
    "    3. predictions += learning_rate * tree.predict(features)\n",
    "```\n",
    "\n",
    "### Second-Order Methods (Newton's Method)\n",
    "\n",
    "**Mathematical Basis:**\n",
    "Uses both first and second derivatives (Hessian) for more principled optimization. Based on Newton's method for optimization:\n",
    "\n",
    "$$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) - \\frac{g_i}{h_i}$$\n",
    "\n",
    "where:\n",
    "- $g_i = \\frac{\\partial L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)}$ (first derivative)\n",
    "- $h_i = \\frac{\\partial^2 L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)^2}$ (second derivative)\n",
    "\n",
    "**Information Gain Formula:**\n",
    "For split evaluation, the gain becomes:\n",
    "\n",
    "$$\\text{Gain} = \\frac{1}{2}\\left[\\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i + \\lambda}\\right]$$\n",
    "\n",
    "**Optimal Leaf Weight:**\n",
    "The optimal prediction for a leaf containing samples $I$ is:\n",
    "\n",
    "$$w^* = -\\frac{\\sum_{i \\in I} g_i}{\\sum_{i \\in I} h_i + \\lambda}$$\n",
    "\n",
    "**Advantages:**\n",
    "- Quadratic convergence near optimum\n",
    "- More principled split selection\n",
    "- Natural regularization through Hessian\n",
    "- Better numerical properties\n",
    "\n",
    "**Disadvantages:**\n",
    "- Higher computational cost (~2x per iteration)\n",
    "- More complex implementation\n",
    "- Requires computing second derivatives\n",
    "\n",
    "```\n",
    "ALGORITHM: Second-Order Gradient Boosting\n",
    "FOR each iteration m:\n",
    "    1. gradients[i] = ∂L(y[i], current_prediction[i]) / ∂F\n",
    "    2. hessians[i] = ∂²L(y[i], current_prediction[i]) / ∂F²\n",
    "    3. tree = TRAIN_TREE_WITH_NEWTON(features, gradients, hessians)\n",
    "       // Uses both g_i and h_i for split finding and leaf weights\n",
    "    4. predictions += tree.predict(features)\n",
    "```\n",
    "\n",
    "### Convergence Comparison\n",
    "\n",
    "The convergence behavior differs significantly:\n",
    "\n",
    "- **First-order:** $\\mathcal{O}(1/k)$ linear convergence\n",
    "- **Second-order:** $\\mathcal{O}(1/k^2)$ quadratic convergence near optimum\n",
    "\n",
    "---\n",
    "\n",
    "## Tree Building Strategies\n",
    "\n",
    "### The Split-Finding Problem\n",
    "\n",
    "At each node, we need to find the optimal split that maximizes information gain. For feature $j$ and threshold $\\theta$:\n",
    "\n",
    "$$\\text{Gain}(j, \\theta) = G(\\text{parent}) - \\frac{|I_L|}{|I|} G(I_L) - \\frac{|I_R|}{|I|} G(I_R)$$\n",
    "\n",
    "where $G(I)$ is an impurity measure for sample set $I$.\n",
    "\n",
    "### Exact Method (Pre-sorted Algorithm)\n",
    "\n",
    "**Core Idea:** Evaluate ALL possible split points between consecutive feature values.\n",
    "\n",
    "**Process:**\n",
    "1. Sort all samples by each feature value\n",
    "2. For each feature, try all $n-1$ possible splits\n",
    "3. Maintain running sums of gradients/Hessians for efficient gain calculation\n",
    "4. Select split with maximum gain across all features\n",
    "\n",
    "**Time Complexity:** $\\mathcal{O}(n \\times d \\times \\log n)$ per tree level\n",
    "**Space Complexity:** $\\mathcal{O}(n \\times d)$ for pre-sorted arrays\n",
    "\n",
    "```\n",
    "ALGORITHM: Exact Split Finding\n",
    "INPUT: Feature values X[:, j], gradients g, hessians h\n",
    "\n",
    "1. sorted_indices = SORT_BY_FEATURE(X[:, j])\n",
    "2. sorted_gradients = g[sorted_indices]\n",
    "3. sorted_hessians = h[sorted_indices]\n",
    "\n",
    "4. best_gain = -∞\n",
    "5. G_left = 0, H_left = 0\n",
    "6. G_total = SUM(sorted_gradients)\n",
    "7. H_total = SUM(sorted_hessians)\n",
    "\n",
    "8. FOR i = 0 to n-2:\n",
    "   a) G_left += sorted_gradients[i]\n",
    "   b) H_left += sorted_hessians[i]\n",
    "   c) IF sorted_values[i] == sorted_values[i+1]: CONTINUE\n",
    "   \n",
    "   d) G_right = G_total - G_left\n",
    "   e) H_right = H_total - H_left\n",
    "   \n",
    "   f) gain = CALCULATE_GAIN(G_left, H_left, G_right, H_right)\n",
    "   g) IF gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_threshold = (sorted_values[i] + sorted_values[i+1]) / 2\n",
    "\n",
    "9. RETURN best_threshold, best_gain\n",
    "```\n",
    "\n",
    "### Histogram Method (Binning Algorithm)\n",
    "\n",
    "**Core Idea:** Pre-discretize continuous features into bins, only evaluate splits at bin boundaries.\n",
    "\n",
    "**Mathematical Framework:**\n",
    "Instead of $n-1$ possible splits, we evaluate only $B-1$ splits where $B$ is the number of bins (typically 255).\n",
    "\n",
    "**Binning Strategies:**\n",
    "1. **Equal-width binning:** $\\text{bin}_k = \\left[\\frac{k(x_{\\max} - x_{\\min})}{B}, \\frac{(k+1)(x_{\\max} - x_{\\min})}{B}\\right]$\n",
    "2. **Quantile binning:** Bins contain equal number of samples\n",
    "3. **Feature-specific binning:** Different strategies per feature type\n",
    "\n",
    "**Histogram Construction:**\n",
    "For each bin $b$, maintain accumulated statistics:\n",
    "\n",
    "$$H_b^{(g)} = \\sum_{i: x_i \\in \\text{bin}_b} g_i, \\quad H_b^{(h)} = \\sum_{i: x_i \\in \\text{bin}_b} h_i$$\n",
    "\n",
    "**Time Complexity:** $\\mathcal{O}(B \\times d)$ per tree level\n",
    "**Space Complexity:** $\\mathcal{O}(B \\times d)$ for histograms\n",
    "\n",
    "```\n",
    "ALGORITHM: Histogram-Based Split Finding\n",
    "INPUT: Feature values X[:, j], gradients g, hessians h, bin_boundaries\n",
    "\n",
    "1. n_bins = LENGTH(bin_boundaries) + 1\n",
    "2. hist_gradients = ZEROS(n_bins)\n",
    "3. hist_hessians = ZEROS(n_bins)\n",
    "\n",
    "4. FOR each sample i:\n",
    "   a) bin_idx = FIND_BIN(X[i, j], bin_boundaries)\n",
    "   b) hist_gradients[bin_idx] += g[i]\n",
    "   c) hist_hessians[bin_idx] += h[i]\n",
    "\n",
    "5. best_gain = -∞\n",
    "6. G_left = 0, H_left = 0\n",
    "7. G_total = SUM(hist_gradients)\n",
    "8. H_total = SUM(hist_hessians)\n",
    "\n",
    "9. FOR k = 0 to n_bins-2:\n",
    "   a) G_left += hist_gradients[k]\n",
    "   b) H_left += hist_hessians[k]\n",
    "   c) G_right = G_total - G_left\n",
    "   d) H_right = H_total - H_left\n",
    "   \n",
    "   e) gain = CALCULATE_GAIN(G_left, H_left, G_right, H_right)\n",
    "   f) IF gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_threshold = bin_boundaries[k]\n",
    "\n",
    "10. RETURN best_threshold, best_gain\n",
    "```\n",
    "\n",
    "### Performance Trade-offs\n",
    "\n",
    "| Method | Accuracy | Speed | Memory | Best Use Case |\n",
    "|--------|----------|-------|---------|---------------|\n",
    "| **Exact** | Optimal | Slow | High | Small datasets (<100K), maximum accuracy |\n",
    "| **Histogram** | ~99% of optimal | Fast | Low | Large datasets (>100K), production systems |\n",
    "\n",
    "---\n",
    "\n",
    "## Split Families: Axis-Aligned, Oblique, Hybrid, and More\n",
    "\n",
    "Real-world boosting systems differ as much by **how they split** as by their loss and sampling. This section catalogs the main split families, their math, algorithms, and trade-offs—so you can choose intentionally (or mix them).\n",
    "\n",
    "### Taxonomy at a Glance\n",
    "\n",
    "| Family                                | Predicate form                                                         | Typical training          | Strengths                            | Caveats                                     |\n",
    "| ------------------------------------- | ---------------------------------------------------------------------- | ------------------------- | ------------------------------------ | ------------------------------------------- |\n",
    "| **Axis-Aligned (Axial)**              | $x_j \\le \\theta$                                                       | exact / histogram         | Fast, simple, stable                 | Needs more depth to capture interactions    |\n",
    "| **Oblique (Linear)**                  | $\\mathbf{w}^\\top \\mathbf{x} \\le \\theta$                                | greedy local optimization | Shallow trees, captures interactions | Costly; needs regularization                |\n",
    "| **Sparse Oblique**                    | $\\mathbf{w}$ sparse                                                    | L1/feature gating         | Better interpretability; speed       | Tuning sparsity                             |\n",
    "| **Random Projection / Rotated**       | $\\tilde{\\mathbf{x}} = R\\mathbf{x}$, then axial                         | random / PCA / LDA        | Cheap interaction capture            | Adds randomness; weaker control             |\n",
    "| **Quadratic / Kernelized**            | $\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{w}^\\top \\mathbf{x} \\le \\theta$ | local second-order fit    | Very expressive                      | Overfits; expensive                         |\n",
    "| **Categorical (Multiway / 1-v-Rest)** | $x_j \\in S$                                                            | exact / greedy set search | Native categorical handling          | Set search is exponential; needs heuristics |\n",
    "| **Constrained (Monotone/Rule)**       | split + constraint filter                                              | feasibility checks        | Enforces domain structure            | May skip “best” split                       |\n",
    "\n",
    "### Axis-Aligned (Axial) Splits\n",
    "\n",
    "Same framework as in your **Exact** and **Histogram** methods. For second-order boosting, the **gain** for a split $(j,\\theta)$ is:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\tfrac{1}{2}\\!\\left[\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H+\\lambda}\\right] - \\gamma\n",
    "$$\n",
    "\n",
    "with the usual accumulated gradient $G_\\bullet$ and Hessian $H_\\bullet$, and regularizers $(\\lambda,\\gamma)$. See your existing pseudocode.\n",
    "\n",
    "**When to use:** default for large-scale, high-dimensional data; pairs perfectly with **histogram binning** and **GOSS**.\n",
    "\n",
    "### Oblique (Linear) Splits\n",
    "\n",
    "Replace a single-feature threshold by a **hyperplane**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\top \\mathbf{x} \\le \\theta \\quad \\Rightarrow \\quad \\text{left},\\ \\text{else right}.\n",
    "$$\n",
    "\n",
    "If you **fix** $(\\mathbf{w},\\theta)$, you can reuse the same gain formula by treating $z_i=\\mathbf{w}^\\top \\mathbf{x}_i$ as a pseudo-feature and splitting at $\\theta$. The challenge is to **learn $(\\mathbf{w},\\theta)$** at each node efficiently.\n",
    "\n",
    "#### Common Training Recipes\n",
    "\n",
    "1. **Gradient-Weighted Logistic Split (fast and robust)**\n",
    "   Fit a linear classifier to route samples left/right using the **signed first-order signal** (e.g., residuals for first-order GBDT or $\\operatorname{sign}(g_i)$ for second-order) as soft labels.\n",
    "\n",
    "   * Objective (at node $\\mathcal{I}$):\n",
    "\n",
    "     $$\n",
    "     \\min_{\\mathbf{w},b}\\ \\sum_{i\\in \\mathcal{I}} \\underbrace{\\alpha_i}_{\\text{|grad| or Hessian-weight}} \\cdot \\log\\!\\big(1+\\exp(-y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b))\\big) + \\lambda_1\\|\\mathbf{w}\\|_1+\\tfrac{\\lambda_2}{2}\\|\\mathbf{w}\\|_2^2\n",
    "     $$\n",
    "\n",
    "     with $y_i\\in\\{-1,+1\\}$ indicating preferred side (e.g., sign of gradient), and $\\alpha_i$ the per-sample weight (e.g., $|g_i|$ or $h_i$).\n",
    "   * After optimizing $(\\mathbf{w},b)$, set $\\theta=-b$. Evaluate **Newton gain** as usual over the routing induced by $\\mathbf{w}^\\top \\mathbf{x}\\le\\theta$.\n",
    "\n",
    "2. **Gain-Direct Optimization (coordinate/line search)**\n",
    "   Optimize $\\mathbf{w}$ to **maximize** the second-order gain directly. In practice:\n",
    "\n",
    "   * Start with $\\mathbf{w}$ from a quick linear fit (recipe 1).\n",
    "   * Do a few **coordinate updates**: tweak one coefficient, recompute the projected $z_i$, re-bin $z$ (histogram), sweep thresholds, keep the best. Repeat for $K$ passes over a small **feature subset**.\n",
    "\n",
    "3. **Fisher/LDA or SVM Warm-Starts**\n",
    "   Use LDA (if class labels available) or a linear SVM trained to separate high-loss vs low-loss samples (e.g., top-|g| vs others). Then fine-tune with #2.\n",
    "\n",
    "#### Oblique Split (Histogram) – Pseudocode\n",
    "\n",
    "```\n",
    "INPUT: Node indices I, data X, grads g, hess h\n",
    "PARAMS: feature_subsample m, max_iter K, bins B, λ1, λ2\n",
    "\n",
    "1. J = SAMPLE_FEATURES(d, m)                    // feature gating for sparsity/speed\n",
    "2. w = 0; b = 0                                  // initialize hyperplane\n",
    "3. // Warm-start via gradient-weighted logistic\n",
    "4. (w, b) = FIT_LOGISTIC(X[I, J], y=sign(g[I]), weights=abs(g[I]),\n",
    "                         l1=λ1, l2=λ2, iters=small)\n",
    "\n",
    "5. FOR iter = 1..K:\n",
    "      // Project and histogram once per iter\n",
    "      z = X[I, J] @ w + b\n",
    "      (hist_g, hist_h, edges) = BUILD_HISTOGRAM(z, g[I], h[I], B)\n",
    "      (θ_idx, gain) = SWEEP_BINS_FOR_BEST_GAIN(hist_g, hist_h, λ=λ2)\n",
    "      // Optional: small coordinate refinement\n",
    "      FOR j in J (few coords only):\n",
    "           Δ = LINE_SEARCH_ON_COORD(j)          // tweak w_j, reuse hist where possible\n",
    "           if Δ improves gain: update w_j, z, gain accordingly\n",
    "\n",
    "6. θ = edges[θ_idx]\n",
    "7. RETURN (w, θ, gain)\n",
    "```\n",
    "\n",
    "**Complexity Tips**\n",
    "\n",
    "* Use **binning on z** (the 1-D projection) to keep the sweep at $O(B)$.\n",
    "* Subsample features per node (e.g., $m=\\min(32,\\sqrt{d})$).\n",
    "* Enforce **sparsity** with $L_1$ or a hard $k$-nonzero cap for $\\mathbf{w}$.\n",
    "\n",
    "**Why/When**\n",
    "\n",
    "* Great on **tabular** with interactions and moderate $d$.\n",
    "* Often reduces **depth** and **tree count**.\n",
    "\n",
    "### Sparse Oblique (L1 / Gated)\n",
    "\n",
    "Same as oblique, but explicitly **limit nonzeros in $\\mathbf{w}$**:\n",
    "\n",
    "* Add $\\lambda_1\\|\\mathbf{w}\\|_1$ or constrain $\\|\\mathbf{w}\\|_0 \\le k$.\n",
    "* Or apply a **two-stage gate**: rank features by one-step axial gain; keep top-$k$; fit oblique only on those.\n",
    "\n",
    "**Practical defaults:** $k\\in[4,16]$, $\\lambda_1$ such that 80–90% of nodes produce $\\le k$ nonzeros.\n",
    "\n",
    "### Random Projection / Rotation Forest Splits\n",
    "\n",
    "Form randomized features $\\tilde{\\mathbf{x}} = R \\mathbf{x}$ and do **standard axial** splits in the rotated space.\n",
    "\n",
    "* **R choices:** sparse random matrix, block-PCA at node, or class-aware LDA (if labels).\n",
    "* **Lightweight obliques:** captures interactions **without** iterative $\\mathbf{w}$ training.\n",
    "* Works nicely with **histograms** and **GOSS**.\n",
    "\n",
    "**Pseudo:**\n",
    "\n",
    "```\n",
    "R = SAMPLE_RANDOM_ROTATION(d, m)        // e.g., sparse ±1, normalized\n",
    "Xtilde = X[I] @ R\n",
    "best_axial_on_Xtilde()\n",
    "```\n",
    "\n",
    "### Quadratic / Kernelized Splits (Advanced)\n",
    "\n",
    "Predicate: $\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{w}^\\top \\mathbf{x} \\le \\theta$.\n",
    "Fit locally via ridge/logistic on a **quadratic feature map** $\\phi(\\mathbf{x}) = [x_1,\\dots,x_d, x_1^2, x_1x_2,\\dots]$ with strong regularization. Very expressive, but generally overkill for boosting (prefer putting capacity in **leaf models** instead; see below).\n",
    "\n",
    "### Categorical Splits\n",
    "\n",
    "For a categorical feature with categories $\\mathcal{C}$:\n",
    "\n",
    "* **One-vs-Rest (binary)**: learn a subset $S\\subset \\mathcal{C}$ and split on $x\\in S$.\n",
    "  Heuristics: order categories by **target statistic** (mean residual/gradient), then scan contiguous subsets of that order (LightGBM trick).\n",
    "\n",
    "* **Multiway**: split into $k$ children (one per category or grouped). Often replaced by **binary** for depth control.\n",
    "\n",
    "**Ordered scan (binary)**:\n",
    "\n",
    "1. Compute category score $s(c)=\\frac{\\sum_{i:x_i=c} g_i}{\\sum_{i:x_i=c} h_i+\\epsilon}$.\n",
    "2. Sort categories by $s(c)$.\n",
    "3. Sweep a prefix $S$ to maximize Newton gain.\n",
    "\n",
    "### Constrained Splits (Monotonicity, Custom Rules)\n",
    "\n",
    "When some features must be **monotone** ($+/-$) w\\.r.t. prediction, filter split candidates to **preserve feasibility**:\n",
    "\n",
    "* During candidate evaluation, **reject** splits whose **optimal leaf weights** would violate monotone constraints.\n",
    "* Implement by checking that for any constrained feature, along any path, the **sign** of leaf updates is consistent.\n",
    "\n",
    "### Integrating Split Families into Boosting\n",
    "\n",
    "You can **mix** families per node with a simple, time-budgeted contender set.\n",
    "\n",
    "```\n",
    "ALGORITHM: Mixed Split Search at a Node\n",
    "INPUT: I, time_budget T\n",
    "\n",
    "1. C = []\n",
    "2. // Axial contenders (fast)\n",
    "3. C += TOP_K_AXIAL_SPLITS(k_axial)\n",
    "\n",
    "4. // Oblique contenders (time-bounded)\n",
    "5. if T allows:\n",
    "      C += OBLIQUE_SPLIT(feature_subsample=m, iters=K, bins=B)\n",
    "\n",
    "6. // Random projection contender (very cheap)\n",
    "7. C += AXIAL_ON_RANDOM_ROTATION(m)\n",
    "\n",
    "8. // Categorical specialized\n",
    "9. C += BEST_CATEGORICAL_SPLITS()\n",
    "\n",
    "10. RETURN argmax_{c in C} Gain(c)\n",
    "```\n",
    "\n",
    "**Leaf Models vs Splits:**\n",
    "Before moving to **quadratic/complex splits**, consider **linear/GLM leaves** (a.k.a. **GBDT-LR**). They often yield similar accuracy with simpler splits.\n",
    "\n",
    "### Practical Defaults & Tips\n",
    "\n",
    "* **Start axial**; enable **sparse oblique** only at nodes with $|I|\\ge 2{,}000$ (or depth $\\le 3$).\n",
    "* **Feature subsample for oblique**: $m=\\min(32,\\sqrt{d})$.\n",
    "* **Binning**: reuse your **global histogram bins** on the **projected z** (re-binning z per node; $B=63$ or $127$ is usually enough).\n",
    "* **Regularize** obliques: $\\lambda_2$ tied to parent $H$ (e.g., $\\lambda_2 = c \\cdot \\frac{H}{|I|}$ with $c\\in[0.1,1]$); $\\lambda_1$ tuned to hit desired sparsity.\n",
    "* **Early abort** oblique search if the best oblique gain $< (1+\\epsilon)$ times the best axial gain (e.g., $\\epsilon=0.02$).\n",
    "* **GOSS compatibility**: weight the logistic/linear fit by $\\alpha_i$ (your amplified sample weights).\n",
    "\n",
    "### Evaluation: When Do Obliques Pay Off?\n",
    "\n",
    "* **Low depth budgets** (e.g., max\\_depth $\\le 4$): obliques recover accuracy lost by shallower trees.\n",
    "* **Moderate $d$** (10–200) with interactions and weak linear trends.\n",
    "* **Strong collinearity**: obliques reduce “feature ping-pong” across levels.\n",
    "\n",
    "Expected impact (rules of thumb):\n",
    "\n",
    "* Depth ↓ 25–40% for same validation loss.\n",
    "* Trees ↓ 10–25% for same test error.\n",
    "* Per-node compute ↑ 1.5–5× (mitigated by feature gating and binning).\n",
    "\n",
    "### Minimal API Hooks (Implementation Notes)\n",
    "\n",
    "* **Splitter interface** should accept $(X_I, g_I, h_I, bins)$ and return `(predicate, gain)`, where `predicate` can encode:\n",
    "\n",
    "  * Axial: `(type=\"axial\", j, θ)`\n",
    "  * Oblique: `(type=\"oblique\", w_sparse, θ)`\n",
    "  * Categorical: `(type=\"cat\", j, subset S)`\n",
    "* **Histogram reuse**: axial uses precomputed per-feature hist; oblique builds **1-D hist over z** only.\n",
    "* **Regularization/penalties**: expose $(\\lambda_1,\\lambda_2,\\gamma)$ and a **max\\_nonzeros** for oblique.\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Sampling: GOSS\n",
    "\n",
    "### Motivation and Intuition\n",
    "\n",
    "**Key Insight:** Data instances contribute unequally to the learning process. Instances with larger gradient magnitudes are more \"informative\" because they represent harder-to-predict cases.\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "In the information gain formula:\n",
    "\n",
    "$$\\text{Gain} \\propto \\frac{(\\sum g_i)^2}{\\sum h_i + \\lambda}$$\n",
    "\n",
    "Samples with larger $|g_i|$ contribute more to the numerator, thus having greater impact on split decisions.\n",
    "\n",
    "### GOSS Algorithm\n",
    "\n",
    "**Goal:** Maintain most informative samples while reducing dataset size through intelligent sampling.\n",
    "\n",
    "**Two-Sided Sampling Strategy:**\n",
    "1. **Set A (Top-$a$):** Keep all samples with largest gradient magnitudes\n",
    "2. **Set B (Random-$b$):** Randomly sample from remaining instances\n",
    "3. **Amplification:** Multiply gradients in Set B by factor $\\frac{1-a}{b}$\n",
    "\n",
    "**Theoretical Justification:**\n",
    "To maintain unbiased estimation:\n",
    "\n",
    "$$\\mathbb{E}\\left[\\sum_{i \\in A \\cup B} \\tilde{g}_i\\right] = \\sum_{i=1}^n g_i$$\n",
    "\n",
    "where $\\tilde{g}_i$ are the amplified gradients.\n",
    "\n",
    "**Amplification Factor Derivation:**\n",
    "- Total gradient contribution from non-top samples: $(1-a) \\times \\sum_{i=1}^n |g_i|$\n",
    "- We sample fraction $b$ of these: $b \\times (1-a) \\times \\sum_{i=1}^n |g_i|$\n",
    "- Amplification needed: $\\frac{1-a}{b}$ to restore expected value\n",
    "\n",
    "```\n",
    "ALGORITHM: GOSS (Gradient-based One-Side Sampling)\n",
    "INPUT: Dataset D, gradients {g_i}, parameters a, b\n",
    "OUTPUT: Sampled dataset D_sampled with weights\n",
    "\n",
    "1. n = |D|\n",
    "2. sorted_indices = SORT_BY_GRADIENT_MAGNITUDE(g, descending=True)\n",
    "\n",
    "3. // Select top-a fraction (Set A)\n",
    "4. n_top = ⌊a × n⌋\n",
    "5. A = sorted_indices[0:n_top]\n",
    "\n",
    "6. // Randomly sample b fraction from remainder (Set B)\n",
    "7. remaining = sorted_indices[n_top:]\n",
    "8. n_other = ⌊b × |remaining|⌋\n",
    "9. B = RANDOM_SAMPLE(remaining, n_other)\n",
    "\n",
    "10. // Combine sets\n",
    "11. selected_indices = A ∪ B\n",
    "12. amplification_factor = (1 - a) / b\n",
    "\n",
    "13. // Create weights\n",
    "14. weights = ONES(|selected_indices|)\n",
    "15. weights[|A|:] = amplification_factor  // Amplify Set B\n",
    "\n",
    "16. RETURN D[selected_indices], weights\n",
    "```\n",
    "\n",
    "### Information Gain with GOSS\n",
    "\n",
    "The modified gain calculation becomes:\n",
    "\n",
    "$$\\text{Gain}_{\\text{GOSS}} = \\frac{1}{2}\\left[\\frac{\\left(\\sum_{i \\in A_L} g_i + \\frac{1-a}{b}\\sum_{i \\in B_L} g_i\\right)^2}{\\sum_{i \\in A_L} h_i + \\frac{1-a}{b}\\sum_{i \\in B_L} h_i + \\lambda} + \\text{(right side)} - \\text{(parent)}\\right]$$\n",
    "\n",
    "### GOSS Performance Analysis\n",
    "\n",
    "**Computational Complexity:**\n",
    "- Standard: $\\mathcal{O}(n \\times d)$ per tree\n",
    "- GOSS: $\\mathcal{O}((a + b) \\times n \\times d)$ per tree\n",
    "- Typical speedup: 3-10× with minimal accuracy loss\n",
    "\n",
    "**Parameter Guidelines:**\n",
    "- $a = 0.2$ (keep top 20% of gradients)\n",
    "- $b = 0.1$ (sample 10% of remaining)\n",
    "- Effective dataset size: $0.2 + 0.1 = 0.3$ (70% reduction)\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization: DART\n",
    "\n",
    "### Problem with Sequential Boosting\n",
    "\n",
    "Traditional gradient boosting suffers from **over-specialization**: later trees tend to correct mistakes of specific earlier trees, leading to overfitting.\n",
    "\n",
    "**Visualization of the Problem:**\n",
    "```\n",
    "Standard Boosting Dependencies:\n",
    "Tree1 → Tree2 → Tree3 → Tree4 → Tree5\n",
    "  ↓      ↓      ↓      ↓      ↓\n",
    "Each tree sees ALL previous predictions (deterministic context)\n",
    "```\n",
    "\n",
    "### DART: Dropout Regularization for Trees\n",
    "\n",
    "**Core Idea:** Randomly \"dropout\" some previous trees during each iteration's training, forcing new trees to work with different ensemble contexts.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Standard GBDT prediction:\n",
    "$$F_m(\\mathbf{x}) = \\sum_{k=1}^m \\nu \\cdot T_k(\\mathbf{x})$$\n",
    "\n",
    "DART prediction:\n",
    "$$F_m(\\mathbf{x}) = \\sum_{k \\in \\mathcal{K}_m} T_k(\\mathbf{x}) + \\nu \\cdot T_m(\\mathbf{x})$$\n",
    "\n",
    "where $\\mathcal{K}_m \\subseteq \\{1, 2, ..., m-1\\}$ is the set of **non-dropped** trees.\n",
    "\n",
    "**Normalization Factor:**\n",
    "To maintain consistent prediction magnitude:\n",
    "\n",
    "$$\\text{norm} = \\frac{|\\{1, 2, ..., m-1\\}|}{|\\{1, 2, ..., m-1\\} \\setminus \\mathcal{D}_m|} = \\frac{m-1}{m-1-|\\mathcal{D}_m|}$$\n",
    "\n",
    "where $\\mathcal{D}_m$ is the set of dropped trees.\n",
    "\n",
    "**Normalized Prediction:**\n",
    "$$\\tilde{F}_{m-1}(\\mathbf{x}) = \\text{norm} \\times \\sum_{k \\in \\mathcal{K}_m} \\nu \\cdot T_k(\\mathbf{x})$$\n",
    "\n",
    "### DART Algorithm\n",
    "\n",
    "```\n",
    "ALGORITHM: DART (Dropouts meet Multiple Additive Regression Trees)\n",
    "INPUT: Dataset D, loss L, iterations M, drop_rate p, max_drop K\n",
    "\n",
    "1. trees = []\n",
    "2. F_0(x) = initial_prediction\n",
    "\n",
    "3. FOR m = 1 to M:\n",
    "   a) // Select trees to drop\n",
    "   IF RANDOM() > skip_drop_probability:\n",
    "      dropped_trees = RANDOM_SUBSET(trees, drop_rate, max_drop)\n",
    "   ELSE:\n",
    "      dropped_trees = []\n",
    "   \n",
    "   b) // Compute normalized prediction from active trees\n",
    "   active_trees = trees \\ dropped_trees\n",
    "   norm_factor = |trees| / |active_trees| if |active_trees| > 0 else 1\n",
    "   \n",
    "   F_normalized(x) = norm_factor × Σ_{T ∈ active_trees} T(x)\n",
    "   \n",
    "   c) // Train new tree on normalized residuals\n",
    "   residuals = -∂L(y, F_normalized(x)) / ∂F\n",
    "   T_m = TRAIN_TREE(D, residuals)\n",
    "   \n",
    "   d) // Add to ensemble\n",
    "   trees.APPEND(T_m)\n",
    "\n",
    "4. RETURN trees\n",
    "```\n",
    "\n",
    "### DART Hyperparameters\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "- **drop_rate** $\\in [0.1, 0.5]$: Probability of dropping each tree\n",
    "- **max_drop** $\\in [1, 50]$: Maximum number of trees to drop per iteration\n",
    "- **skip_drop** $\\in [0.3, 0.7]$: Probability of skipping dropout entirely\n",
    "- **normalize_type**: How to handle normalization (tree-wise vs weight-wise)\n",
    "\n",
    "**Parameter Interaction:**\n",
    "The effective regularization strength is:\n",
    "\n",
    "$$\\text{Regularization} \\propto \\text{drop\\_rate} \\times (1 - \\text{skipd\\_rop}) \\times \\frac{\\text{n\\_trees}}{\\text{max\\_drop}}$$\n",
    "\n",
    "### DART vs Standard Boosting\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces overfitting through ensemble diversification\n",
    "- Better generalization on complex datasets\n",
    "- Self-regularizing (less hyperparameter tuning needed)\n",
    "- Typically 2-5% accuracy improvement\n",
    "\n",
    "**Disadvantages:**\n",
    "- Slower training (10-20% overhead)\n",
    "- More hyperparameters to tune\n",
    "- Slightly more complex to implement\n",
    "- Can be less stable during early training\n",
    "\n",
    "```\n",
    "Dependency Comparison:\n",
    "\n",
    "Standard Boosting:\n",
    "T1 → T2 → T3 → T4 → T5 (each sees all previous)\n",
    "\n",
    "DART with Dropout:\n",
    "T1   T2 → T3   T4 → T5 (randomized dependencies)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Post-Tree Pruning\n",
    "\n",
    "### Motivation and Theory\n",
    "\n",
    "**The Overfitting Problem:**\n",
    "Decision trees in gradient boosting can grow too deep, memorizing training data patterns that don't generalize. Post-pruning addresses this by removing branches that provide minimal improvement to validation performance.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For a subtree rooted at node $t$, define the **complexity cost**:\n",
    "\n",
    "$C_\\alpha(T_t) = \\sum_{\\ell \\in \\text{leaves}(T_t)} N_\\ell \\cdot \\text{Error}(\\ell) + \\alpha \\cdot |\\text{leaves}(T_t)|$\n",
    "\n",
    "where:\n",
    "- $N_\\ell$ = number of samples in leaf $\\ell$\n",
    "- $\\text{Error}(\\ell)$ = impurity measure for leaf $\\ell$\n",
    "- $\\alpha$ = complexity parameter (regularization strength)\n",
    "- $|\\text{leaves}(T_t)|$ = number of leaves in subtree $T_t$\n",
    "\n",
    "**Pruning Decision Rule:**\n",
    "Prune subtree $T_t$ if replacing it with a single leaf reduces complexity cost:\n",
    "\n",
    "$C_\\alpha(\\text{leaf}) < C_\\alpha(T_t)$\n",
    "\n",
    "### Types of Pruning\n",
    "\n",
    "#### 1. Minimal Cost-Complexity Pruning (Weakest Link)\n",
    "\n",
    "**Core Idea:** Find the subtree that provides the least improvement per additional leaf.\n",
    "\n",
    "For each internal node $t$, compute the **improvement per leaf**:\n",
    "\n",
    "$g(t) = \\frac{\\text{Error}(\\text{leaf}_t) - \\text{Error}(T_t)}{|\\text{leaves}(T_t)| - 1}$\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "ALGORITHM: Minimal Cost-Complexity Pruning\n",
    "INPUT: Tree T, validation set V\n",
    "OUTPUT: Optimally pruned tree T*\n",
    "\n",
    "1. // Build sequence of nested trees\n",
    "2. tree_sequence = []\n",
    "3. alpha_sequence = []\n",
    "4. current_tree = COPY(T)\n",
    "\n",
    "5. WHILE |leaves(current_tree)| > 1:\n",
    "   a) // Find weakest link (smallest g(t))\n",
    "   min_alpha = ∞\n",
    "   weakest_node = null\n",
    "   \n",
    "   FOR each internal node t in current_tree:\n",
    "      g_t = CALCULATE_IMPROVEMENT_PER_LEAF(t)\n",
    "      IF g_t < min_alpha:\n",
    "         min_alpha = g_t\n",
    "         weakest_node = t\n",
    "   \n",
    "   b) // Store current state\n",
    "   tree_sequence.APPEND(COPY(current_tree))\n",
    "   alpha_sequence.APPEND(min_alpha)\n",
    "   \n",
    "   c) // Prune weakest link\n",
    "   current_tree = PRUNE_SUBTREE(current_tree, weakest_node)\n",
    "\n",
    "6. // Select best tree using validation set\n",
    "7. best_score = -∞\n",
    "8. best_tree = null\n",
    "\n",
    "9. FOR each tree in tree_sequence:\n",
    "   validation_score = EVALUATE(tree, V)\n",
    "   IF validation_score > best_score:\n",
    "      best_score = validation_score\n",
    "      best_tree = tree\n",
    "\n",
    "10. RETURN best_tree\n",
    "```\n",
    "\n",
    "#### 2. Reduced Error Pruning\n",
    "\n",
    "**Core Idea:** Prune nodes where replacing the subtree with a leaf doesn't increase validation error.\n",
    "\n",
    "**Validation-Based Decision:**\n",
    "For each internal node $t$:\n",
    "1. Compute validation accuracy with current subtree: $A_{\\text{subtree}}$\n",
    "2. Compute validation accuracy if replaced by leaf: $A_{\\text{leaf}}$\n",
    "3. If $A_{\\text{leaf}} \\geq A_{\\text{subtree}}$, prune the subtree\n",
    "\n",
    "```\n",
    "ALGORITHM: Reduced Error Pruning\n",
    "INPUT: Tree T, validation set V\n",
    "OUTPUT: Pruned tree T'\n",
    "\n",
    "1. T' = COPY(T)\n",
    "2. changed = True\n",
    "\n",
    "3. WHILE changed:\n",
    "   changed = False\n",
    "   \n",
    "   FOR each internal node t in T' (bottom-up):\n",
    "      // Current subtree performance\n",
    "      pred_subtree = PREDICT_WITH_SUBTREE(V, t)\n",
    "      error_subtree = ERROR(V.labels, pred_subtree)\n",
    "      \n",
    "      // Leaf replacement performance  \n",
    "      leaf_prediction = MAJORITY_CLASS(samples_at_node_t)\n",
    "      pred_leaf = [leaf_prediction] * |samples_at_t|\n",
    "      error_leaf = ERROR(V.labels[samples_at_t], pred_leaf)\n",
    "      \n",
    "      IF error_leaf ≤ error_subtree:\n",
    "         T' = REPLACE_SUBTREE_WITH_LEAF(T', t)\n",
    "         changed = True\n",
    "\n",
    "4. RETURN T'\n",
    "```\n",
    "\n",
    "#### 3. Critical Value Pruning\n",
    "\n",
    "**Core Idea:** Prune nodes where the split provides less improvement than a threshold.\n",
    "\n",
    "**Mathematical Criterion:**\n",
    "For node $t$ with split creating children $t_L$ and $t_R$:\n",
    "\n",
    "$\\Delta_{\\text{impurity}} = I(t) - \\frac{N_{t_L}}{N_t} I(t_L) - \\frac{N_{t_R}}{N_t} I(t_R)$\n",
    "\n",
    "Prune if $\\Delta_{\\text{impurity}} < \\tau$ (critical value threshold).\n",
    "\n",
    "**For Gradient Boosting Trees:**\n",
    "Using the gain formula:\n",
    "\n",
    "$\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda}\\right]$\n",
    "\n",
    "Prune if $\\text{Gain} < \\tau_{\\text{gain}}$.\n",
    "\n",
    "```\n",
    "ALGORITHM: Critical Value Pruning\n",
    "INPUT: Tree T, threshold τ\n",
    "OUTPUT: Pruned tree T'\n",
    "\n",
    "1. T' = COPY(T)\n",
    "\n",
    "2. FOR each internal node t in T' (bottom-up):\n",
    "   \n",
    "   a) gain = CALCULATE_SPLIT_GAIN(t)\n",
    "   \n",
    "   b) IF gain < τ:\n",
    "      T' = REPLACE_SUBTREE_WITH_LEAF(T', t)\n",
    "\n",
    "3. RETURN T'\n",
    "```\n",
    "\n",
    "### Gradient Boosting Specific Considerations\n",
    "\n",
    "#### Optimal Leaf Values After Pruning\n",
    "\n",
    "When a subtree is replaced by a leaf, the optimal leaf value for second-order methods is:\n",
    "\n",
    "$w^* = -\\frac{\\sum_{i \\in \\text{leaf}} g_i}{\\sum_{i \\in \\text{leaf}} h_i + \\lambda}$\n",
    "\n",
    "For first-order methods, use the mean of residuals:\n",
    "\n",
    "$w^* = \\frac{1}{|\\text{leaf}|} \\sum_{i \\in \\text{leaf}} r_i$\n",
    "\n",
    "#### Integration with Boosting Loop\n",
    "\n",
    "**Strategy 1: Post-hoc Pruning**\n",
    "```\n",
    "FOR each boosting iteration m:\n",
    "1. tree = BUILD_FULL_TREE(gradients, hessians)\n",
    "2. pruned_tree = PRUNE_TREE(tree, validation_set)\n",
    "3. ensemble.ADD(pruned_tree)\n",
    "```\n",
    "\n",
    "**Strategy 2: Online Pruning**\n",
    "```\n",
    "FOR each boosting iteration m:\n",
    "1. tree = BUILD_TREE_WITH_EARLY_STOPPING(gradients, hessians)\n",
    "2. // Tree is naturally pruned during construction\n",
    "3. ensemble.ADD(tree)\n",
    "```\n",
    "\n",
    "### Pruning Metrics and Evaluation\n",
    "\n",
    "#### Complexity Measures\n",
    "\n",
    "**Tree Size Metrics:**\n",
    "- Number of leaves: $|\\text{leaves}(T)|$\n",
    "- Tree depth: $\\max_{\\ell \\in \\text{leaves}} \\text{depth}(\\ell)$\n",
    "- Total nodes: $|\\text{nodes}(T)|$\n",
    "\n",
    "**Model Complexity:**\n",
    "For ensemble of $M$ trees:\n",
    "\n",
    "$\\text{Complexity} = \\sum_{m=1}^M |\\text{leaves}(T_m)|$\n",
    "\n",
    "#### Performance Evaluation\n",
    "\n",
    "**Bias-Variance Trade-off:**\n",
    "Pruning typically:\n",
    "- **Increases bias:** Simpler trees make more assumptions\n",
    "- **Decreases variance:** Less sensitive to training data variations\n",
    "- **Reduces overfitting:** Better generalization to unseen data\n",
    "\n",
    "**Validation Strategy:**\n",
    "```\n",
    "ALGORITHM: Pruning Validation\n",
    "INPUT: Ensemble E, validation set V, test set T\n",
    "\n",
    "1. // Measure complexity vs performance\n",
    "2. complexity_levels = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "3. results = []\n",
    "\n",
    "4. FOR each α in complexity_levels:\n",
    "   a) pruned_ensemble = []\n",
    "   \n",
    "   FOR each tree in E:\n",
    "      pruned_tree = PRUNE_WITH_ALPHA(tree, α)\n",
    "      pruned_ensemble.ADD(pruned_tree)\n",
    "   \n",
    "   b) val_score = EVALUATE(pruned_ensemble, V)\n",
    "   c) test_score = EVALUATE(pruned_ensemble, T)\n",
    "   d) complexity = MEASURE_COMPLEXITY(pruned_ensemble)\n",
    "   \n",
    "   e) results.ADD({\n",
    "        alpha: α,\n",
    "        validation_score: val_score,\n",
    "        test_score: test_score,\n",
    "        complexity: complexity\n",
    "      })\n",
    "\n",
    "5. // Select optimal α\n",
    "6. best_alpha = ARGMAX(r.validation_score for r in results)\n",
    "7. RETURN best_alpha\n",
    "```\n",
    "\n",
    "### Advanced Pruning Techniques\n",
    "\n",
    "#### 1. Ensemble-Aware Pruning\n",
    "\n",
    "**Problem:** Individual tree pruning ignores ensemble interactions.\n",
    "\n",
    "**Solution:** Prune considering the entire ensemble's performance.\n",
    "\n",
    "```\n",
    "ALGORITHM: Ensemble-Aware Pruning\n",
    "INPUT: Ensemble E = {T_1, T_2, ..., T_M}, validation set V\n",
    "\n",
    "1. FOR m = 1 to M:\n",
    "   a) ensemble_minus_m = E \\ {T_m}\n",
    "   b) candidates = GENERATE_PRUNED_VERSIONS(T_m)\n",
    "   \n",
    "   c) best_score = -∞\n",
    "   d) best_tree = T_m\n",
    "   \n",
    "   FOR each candidate in candidates:\n",
    "      temp_ensemble = ensemble_minus_m ∪ {candidate}\n",
    "      score = EVALUATE(temp_ensemble, V)\n",
    "      \n",
    "      IF score > best_score:\n",
    "         best_score = score\n",
    "         best_tree = candidate\n",
    "   \n",
    "   e) E[m] = best_tree\n",
    "\n",
    "2. RETURN E\n",
    "```\n",
    "\n",
    "#### 2. Progressive Pruning\n",
    "\n",
    "**Core Idea:** Gradually increase pruning strength as ensemble grows.\n",
    "\n",
    "**Rationale:** Early trees need more complexity to capture main patterns; later trees can be simpler for fine-tuning.\n",
    "\n",
    "```\n",
    "ALGORITHM: Progressive Pruning\n",
    "INPUT: Training data D, iterations M\n",
    "\n",
    "1. ensemble = []\n",
    "2. base_threshold = 0.01\n",
    "\n",
    "3. FOR m = 1 to M:\n",
    "   a) // Adaptive pruning threshold\n",
    "   threshold = base_threshold * (1 + m/M)\n",
    "   \n",
    "   b) tree = BUILD_TREE(D, gradients, hessians)\n",
    "   c) pruned_tree = PRUNE_WITH_THRESHOLD(tree, threshold)\n",
    "   \n",
    "   d) ensemble.ADD(pruned_tree)\n",
    "   e) UPDATE_PREDICTIONS(D, pruned_tree)\n",
    "\n",
    "4. RETURN ensemble\n",
    "```\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "#### Hyperparameter Selection\n",
    "\n",
    "**Pruning Strength:**\n",
    "- **Light pruning:** $\\alpha \\in [0.001, 0.01]$ - minimal complexity reduction\n",
    "- **Moderate pruning:** $\\alpha \\in [0.01, 0.1]$ - balanced approach\n",
    "- **Heavy pruning:** $\\alpha \\in [0.1, 1.0]$ - aggressive simplification\n",
    "\n",
    "**Validation Strategy:**\n",
    "- Use separate validation set (not training or test)\n",
    "- Cross-validate pruning parameters\n",
    "- Monitor both accuracy and model size\n",
    "\n",
    "#### When to Apply Pruning\n",
    "\n",
    "**Indicators for Pruning:**\n",
    "- Large gap between training and validation performance\n",
    "- Trees with many leaves but poor validation scores\n",
    "- Memory or inference speed constraints\n",
    "- Need for model interpretability\n",
    "\n",
    "**Pruning Decision Framework:**\n",
    "```\n",
    "Dataset Size?\n",
    "├─ Small (<1K): Light pruning (preserve capacity)\n",
    "├─ Medium (1K-100K): Moderate pruning\n",
    "└─ Large (>100K): Heavy pruning acceptable\n",
    "\n",
    "Overfitting Severity?\n",
    "├─ High: Aggressive pruning + early stopping\n",
    "├─ Medium: Standard cost-complexity pruning\n",
    "└─ Low: Minimal or no pruning\n",
    "\n",
    "Deployment Constraints?\n",
    "├─ Mobile/Edge: Heavy pruning for efficiency\n",
    "├─ Server: Balance accuracy vs complexity\n",
    "└─ Research: Minimal pruning for max performance\n",
    "```\n",
    "\n",
    "#### Performance Impact\n",
    "\n",
    "**Expected Improvements:**\n",
    "- **Model size:** 30-70% reduction in tree complexity\n",
    "- **Inference speed:** 20-50% faster predictions\n",
    "- **Generalization:** 1-5% improvement in test accuracy (when overfitting)\n",
    "- **Memory usage:** Proportional to complexity reduction\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Importance in Gradient Boosting\n",
    "\n",
    "### Motivation and Theory\n",
    "\n",
    "**Why Feature Importance Matters:**\n",
    "Understanding which features contribute most to predictions is crucial for:\n",
    "- Model interpretability and debugging\n",
    "- Feature selection and dimensionality reduction\n",
    "- Business insights and decision-making\n",
    "- Regulatory compliance and model auditing\n",
    "\n",
    "**Challenge in Ensemble Methods:**\n",
    "Unlike linear models where coefficients directly indicate importance, tree ensembles require aggregating importance across multiple trees and splits.\n",
    "\n",
    "### Types of Feature Importance\n",
    "\n",
    "#### 1. Split-Based Importance (Gain/Impurity)\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For each split in tree $T_m$ at node $t$ using feature $j$, the importance contribution is:\n",
    "\n",
    "$I_{j,t}^{(m)} = p_t \\cdot \\Delta I_t$\n",
    "\n",
    "where:\n",
    "- $p_t = \\frac{N_t}{N}$ is the proportion of samples reaching node $t$\n",
    "- $\\Delta I_t$ is the impurity reduction from the split\n",
    "\n",
    "**For Gradient Boosting (Second-Order):**\n",
    "The gain-based importance for feature $j$ in tree $m$ is:\n",
    "\n",
    "$I_j^{(m)} = \\sum_{t: \\text{split on } j} \\frac{N_t}{N} \\cdot \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda}\\right]$\n",
    "\n",
    "**Ensemble Aggregation:**\n",
    "Total importance for feature $j$ across all trees:\n",
    "\n",
    "$I_j^{\\text{total}} = \\sum_{m=1}^M I_j^{(m)}$\n",
    "\n",
    "**Normalized Importance:**\n",
    "$I_j^{\\text{norm}} = \\frac{I_j^{\\text{total}}}{\\sum_{k=1}^d I_k^{\\text{total}}}$\n",
    "\n",
    "```\n",
    "ALGORITHM: Split-Based Feature Importance\n",
    "INPUT: Ensemble E = {T_1, T_2, ..., T_M}, features d\n",
    "OUTPUT: Feature importance vector I\n",
    "\n",
    "1. importance = ZEROS(d)\n",
    "\n",
    "2. FOR m = 1 to M:\n",
    "   FOR each internal node t in T_m:\n",
    "      j = FEATURE_USED_AT_SPLIT(t)\n",
    "      sample_weight = NUMBER_OF_SAMPLES(t) / TOTAL_SAMPLES\n",
    "      gain = SPLIT_GAIN(t)  // From gradient boosting gain formula\n",
    "      \n",
    "      importance[j] += sample_weight * gain\n",
    "\n",
    "3. // Normalize to sum to 1\n",
    "4. total_importance = SUM(importance)\n",
    "5. IF total_importance > 0:\n",
    "   importance = importance / total_importance\n",
    "\n",
    "6. RETURN importance\n",
    "```\n",
    "\n",
    "#### 2. Frequency-Based Importance (Split Count)\n",
    "\n",
    "**Core Idea:** Count how often each feature is used for splits across all trees.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "$I_j^{\\text{freq}} = \\frac{1}{M} \\sum_{m=1}^M C_j^{(m)}$\n",
    "\n",
    "where $C_j^{(m)}$ is the count of splits using feature $j$ in tree $m$.\n",
    "\n",
    "**Weighted by Tree Depth:**\n",
    "More sophisticated version weights splits by their depth (earlier splits are more important):\n",
    "\n",
    "$I_j^{\\text{weighted-freq}} = \\sum_{m=1}^M \\sum_{t: \\text{split on } j} \\frac{1}{2^{\\text{depth}(t)}}$\n",
    "\n",
    "```\n",
    "ALGORITHM: Frequency-Based Feature Importance\n",
    "INPUT: Ensemble E, optional depth_weighting\n",
    "\n",
    "1. frequency = ZEROS(d)\n",
    "2. total_splits = 0\n",
    "\n",
    "3. FOR m = 1 to M:\n",
    "   FOR each internal node t in T_m:\n",
    "      j = FEATURE_USED_AT_SPLIT(t)\n",
    "      \n",
    "      IF depth_weighting:\n",
    "         weight = 1.0 / (2^DEPTH(t))\n",
    "      ELSE:\n",
    "         weight = 1.0\n",
    "      \n",
    "      frequency[j] += weight\n",
    "      total_splits += weight\n",
    "\n",
    "4. // Normalize\n",
    "5. IF total_splits > 0:\n",
    "   frequency = frequency / total_splits\n",
    "\n",
    "6. RETURN frequency\n",
    "```\n",
    "\n",
    "#### 3. Permutation Importance\n",
    "\n",
    "**Core Principle:** Measure the increase in prediction error when a feature's values are randomly permuted.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "For feature $j$, permutation importance is:\n",
    "\n",
    "$I_j^{\\text{perm}} = \\frac{1}{K} \\sum_{k=1}^K \\left[\\text{Error}(\\mathbf{y}, \\hat{\\mathbf{y}}^{(k)}_{\\pi_j}) - \\text{Error}(\\mathbf{y}, \\hat{\\mathbf{y}})\\right]$\n",
    "\n",
    "where:\n",
    "- $\\hat{\\mathbf{y}}^{(k)}_{\\pi_j}$ are predictions with feature $j$ permuted in the $k$-th trial\n",
    "- $K$ is the number of permutation trials (typically 5-10)\n",
    "\n",
    "**Advantages:**\n",
    "- Model-agnostic (works with any ML algorithm)\n",
    "- Captures feature interactions\n",
    "- Based on actual prediction performance\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive (requires recomputing predictions)\n",
    "- Can be unreliable with highly correlated features\n",
    "\n",
    "```\n",
    "ALGORITHM: Permutation Feature Importance\n",
    "INPUT: Model M, dataset (X, y), metric ERROR, trials K\n",
    "OUTPUT: Permutation importance vector\n",
    "\n",
    "1. baseline_predictions = M.PREDICT(X)\n",
    "2. baseline_error = ERROR(y, baseline_predictions)\n",
    "3. importance = ZEROS(d)\n",
    "\n",
    "4. FOR j = 1 to d:  // Each feature\n",
    "   trial_errors = []\n",
    "   \n",
    "   FOR k = 1 to K:  // Multiple trials\n",
    "      X_permuted = COPY(X)\n",
    "      X_permuted[:, j] = RANDOM_PERMUTATION(X[:, j])\n",
    "      \n",
    "      permuted_predictions = M.PREDICT(X_permuted)\n",
    "      permuted_error = ERROR(y, permuted_predictions)\n",
    "      \n",
    "      trial_errors.APPEND(permuted_error)\n",
    "   \n",
    "   // Average importance across trials\n",
    "   importance[j] = MEAN(trial_errors) - baseline_error\n",
    "\n",
    "5. RETURN importance\n",
    "```\n",
    "\n",
    "#### 4. SHAP (SHapley Additive exPlanations) Values\n",
    "\n",
    "**Theoretical Foundation:**\n",
    "SHAP values are based on cooperative game theory, satisfying four axioms:\n",
    "- **Efficiency:** $\\sum_{j=1}^d \\phi_j = f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})]$\n",
    "- **Symmetry:** If features contribute equally, they have equal SHAP values\n",
    "- **Dummy:** Features that don't affect output have zero SHAP value\n",
    "- **Additivity:** For ensemble $f = f_1 + f_2$, $\\phi_j^f = \\phi_j^{f_1} + \\phi_j^{f_2}$\n",
    "\n",
    "**Mathematical Definition:**\n",
    "The SHAP value for feature $j$ is:\n",
    "\n",
    "$\\phi_j = \\sum_{S \\subseteq \\mathcal{F} \\setminus \\{j\\}} \\frac{|S|!(d-|S|-1)!}{d!} [f(S \\cup \\{j\\}) - f(S)]$\n",
    "\n",
    "where $\\mathcal{F}$ is the set of all features and $S$ represents feature subsets.\n",
    "\n",
    "**TreeSHAP for Gradient Boosting:**\n",
    "For tree ensembles, TreeSHAP provides an efficient algorithm with polynomial complexity:\n",
    "\n",
    "$\\phi_j = \\sum_{m=1}^M \\phi_j^{(m)}$\n",
    "\n",
    "where $\\phi_j^{(m)}$ is the SHAP value for feature $j$ in tree $m$.\n",
    "\n",
    "**Tree Path Calculation:**\n",
    "For a tree with path $P$ from root to leaf:\n",
    "\n",
    "$\\phi_j^{\\text{tree}} = \\sum_{t \\in P: \\text{split on } j} \\frac{\\text{one\\_hot}(j, t)}{\\text{zero\\_frac}(t)} \\cdot [\\text{leaf\\_value}(\\text{hot\\_path}) - \\text{leaf\\_value}(\\text{cold\\_path})]$\n",
    "\n",
    "```\n",
    "ALGORITHM: TreeSHAP Feature Importance\n",
    "INPUT: Tree T, instance x, background dataset D\n",
    "OUTPUT: SHAP values φ\n",
    "\n",
    "1. // Initialize path tracking\n",
    "2. path = []  // Stores (feature, threshold, zero_fraction)\n",
    "3. shap_values = ZEROS(d)\n",
    "\n",
    "4. FUNCTION RECURSIVE_SHAP(node, path, zero_frac):\n",
    "   IF node is leaf:\n",
    "      // Distribute leaf value across path features\n",
    "      FOR each (feature_j, threshold, z_frac) in path:\n",
    "         weight = zero_frac / z_frac\n",
    "         shap_values[feature_j] += weight * node.value\n",
    "      RETURN\n",
    "   \n",
    "   // Get feature split and zero fraction\n",
    "   feature_j = node.split_feature\n",
    "   threshold = node.split_threshold\n",
    "   \n",
    "   // Calculate zero fraction (how often background data goes left)\n",
    "   zero_frac_left = COUNT(D[:, feature_j] <= threshold) / |D|\n",
    "   \n",
    "   // Add to path\n",
    "   path.APPEND((feature_j, threshold, zero_frac_left))\n",
    "   \n",
    "   // Recurse on children\n",
    "   IF x[feature_j] <= threshold:\n",
    "      RECURSIVE_SHAP(node.left, path, zero_frac * zero_frac_left)\n",
    "   ELSE:\n",
    "      RECURSIVE_SHAP(node.right, path, zero_frac * (1 - zero_frac_left))\n",
    "   \n",
    "   // Remove from path\n",
    "   path.POP()\n",
    "\n",
    "5. // Start recursion\n",
    "6. RECURSIVE_SHAP(T.root, [], 1.0)\n",
    "\n",
    "7. RETURN shap_values\n",
    "```\n",
    "\n",
    "### Gradient Boosting Specific Considerations\n",
    "\n",
    "#### Learning Rate Effects\n",
    "\n",
    "**Issue:** Learning rate affects the magnitude of tree contributions but not relative importance.\n",
    "\n",
    "**Solution:** When aggregating across trees, importance should be scale-invariant:\n",
    "\n",
    "$I_j^{\\text{adjusted}} = \\sum_{m=1}^M \\frac{I_j^{(m)}}{\\max_k I_k^{(m)}}$\n",
    "\n",
    "#### Tree Depth and Importance\n",
    "\n",
    "**Shallow Trees:** Feature importance tends to be more concentrated on fewer features.\n",
    "**Deep Trees:** Importance is more distributed, capturing complex interactions.\n",
    "\n",
    "**Depth-Adjusted Importance:**\n",
    "Weight importance by the depth at which features appear:\n",
    "\n",
    "$I_j^{\\text{depth-adj}} = \\sum_{m=1}^M \\sum_{t: \\text{split on } j} w_{\\text{depth}(t)} \\cdot \\text{gain}(t)$\n",
    "\n",
    "where $w_d = \\frac{1}{\\sqrt{d + 1}}$ reduces importance for deeper splits.\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "**Approach 1:** Exclude samples with missing values from importance calculation.\n",
    "**Approach 2:** Use default direction importance:\n",
    "\n",
    "For missing value handling in XGBoost-style algorithms:\n",
    "\n",
    "$I_j^{\\text{missing}} = \\sum_{\\text{splits on } j} p_{\\text{missing}} \\cdot \\text{gain} \\cdot \\mathbf{1}[\\text{default direction chosen}]$\n",
    "\n",
    "### Ensemble-Level Importance Aggregation\n",
    "\n",
    "#### Simple Average\n",
    "$I_j = \\frac{1}{M} \\sum_{m=1}^M I_j^{(m)}$\n",
    "\n",
    "#### Weighted by Tree Performance\n",
    "Weight trees by their individual contribution to ensemble performance:\n",
    "\n",
    "$I_j = \\frac{\\sum_{m=1}^M w_m \\cdot I_j^{(m)}}{\\sum_{m=1}^M w_m}$\n",
    "\n",
    "where $w_m$ could be:\n",
    "- Tree's contribution to overall loss reduction\n",
    "- Tree's validation performance\n",
    "- Inverse of tree's training error\n",
    "\n",
    "#### Time-Decay Weighting (for DART)\n",
    "In DART, later trees may be more representative of final model:\n",
    "\n",
    "$I_j = \\sum_{m=1}^M e^{-\\alpha(M-m)} \\cdot I_j^{(m)}$\n",
    "\n",
    "where $\\alpha$ controls decay rate.\n",
    "\n",
    "### Comparative Analysis of Methods\n",
    "\n",
    "| Method | Computational Cost | Interpretability | Captures Interactions | Robust to Correlation |\n",
    "|--------|-------------------|------------------|---------------------|----------------------|\n",
    "| **Split-based** | Low | High | Limited | No |\n",
    "| **Frequency** | Low | High | No | No |\n",
    "| **Permutation** | High | Medium | Yes | Partially |\n",
    "| **SHAP** | Medium | Very High | Yes | Yes |\n",
    "\n",
    "### Practical Implementation Guidelines\n",
    "\n",
    "#### Method Selection Framework\n",
    "\n",
    "```\n",
    "Use Case Decision Tree:\n",
    "\n",
    "Model Debugging?\n",
    "├─ Split-based importance (fast, intuitive)\n",
    "\n",
    "Feature Selection?\n",
    "├─ Permutation importance (performance-based)\n",
    "└─ SHAP with feature clustering\n",
    "\n",
    "Regulatory/Explanation?\n",
    "├─ SHAP values (theoretically grounded)\n",
    "\n",
    "Quick Overview?\n",
    "├─ Frequency-based (fastest)\n",
    "```\n",
    "\n",
    "#### Stability and Reliability\n",
    "\n",
    "**Cross-Validation Importance:**\n",
    "```\n",
    "ALGORITHM: Stable Feature Importance\n",
    "INPUT: Dataset D, CV folds K\n",
    "\n",
    "1. importance_matrix = ZEROS(K, d)\n",
    "\n",
    "2. FOR fold k = 1 to K:\n",
    "   train_k, val_k = CV_SPLIT(D, k)\n",
    "   model_k = TRAIN(train_k)\n",
    "   importance_matrix[k, :] = CALCULATE_IMPORTANCE(model_k, val_k)\n",
    "\n",
    "3. // Aggregate statistics\n",
    "4. mean_importance = MEAN(importance_matrix, axis=0)\n",
    "5. std_importance = STD(importance_matrix, axis=0)\n",
    "6. ci_lower = PERCENTILE(importance_matrix, 5, axis=0)\n",
    "7. ci_upper = PERCENTILE(importance_matrix, 95, axis=0)\n",
    "\n",
    "8. RETURN mean_importance, std_importance, ci_lower, ci_upper\n",
    "```\n",
    "\n",
    "#### Visualization and Interpretation\n",
    "\n",
    "**Importance Ranking Plot:**\n",
    "- Bar chart with error bars for uncertainty\n",
    "- Separate plots for different importance types\n",
    "- Include feature names and importance values\n",
    "\n",
    "**Feature Interaction Analysis:**\n",
    "For SHAP values, analyze feature interactions:\n",
    "\n",
    "$\\phi_{ij} = \\frac{1}{2} \\sum_{S \\subseteq \\mathcal{F} \\setminus \\{i,j\\}} \\frac{|S|!(d-|S|-2)!}{d!} \\cdot \\Delta_{ij}(S)$\n",
    "\n",
    "where $\\Delta_{ij}(S) = f(S \\cup \\{i,j\\}) - f(S \\cup \\{i\\}) - f(S \\cup \\{j\\}) + f(S)$\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "#### 1. Correlation Bias\n",
    "**Problem:** Correlated features may have artificially split importance.\n",
    "**Solution:** Use feature clustering or permutation importance with groups:\n",
    "\n",
    "```\n",
    "ALGORITHM: Group Permutation Importance\n",
    "INPUT: Correlated feature groups G = {G_1, G_2, ..., G_k}\n",
    "\n",
    "FOR each group G_i:\n",
    "   permuted_error = 0\n",
    "   FOR trial = 1 to K:\n",
    "      X_perm = COPY(X)\n",
    "      FOR feature j in G_i:\n",
    "         X_perm[:, j] = RANDOM_PERMUTATION(X[:, j])\n",
    "      \n",
    "      error = EVALUATE(model, X_perm, y)\n",
    "      permuted_error += error\n",
    "   \n",
    "   group_importance[G_i] = permuted_error / K - baseline_error\n",
    "```\n",
    "\n",
    "#### 2. Scale Sensitivity\n",
    "**Problem:** Features with different scales may have biased importance.\n",
    "**Solution:** Use relative importance or standardize features before training.\n",
    "\n",
    "#### 3. Sample Size Effects\n",
    "**Problem:** Importance can be unstable with small datasets.\n",
    "**Solution:** Use bootstrap confidence intervals:\n",
    "\n",
    "```\n",
    "ALGORITHM: Bootstrap Importance Confidence\n",
    "INPUT: Model M, data (X, y), bootstrap samples B\n",
    "\n",
    "FOR b = 1 to B:\n",
    "   X_boot, y_boot = BOOTSTRAP_SAMPLE(X, y)\n",
    "   importance_boot[b] = CALCULATE_IMPORTANCE(M, X_boot, y_boot)\n",
    "\n",
    "confidence_interval = PERCENTILE(importance_boot, [2.5, 97.5], axis=0)\n",
    "```\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "#### Dynamic Importance Over Training\n",
    "Track how feature importance evolves during boosting:\n",
    "\n",
    "$I_j^{(1:m)} = \\sum_{k=1}^m I_j^{(k)}$\n",
    "\n",
    "This reveals:\n",
    "- Which features are learned first\n",
    "- When diminishing returns occur\n",
    "- Optimal stopping points for feature subsets\n",
    "\n",
    "#### Conditional Importance\n",
    "Importance of feature $j$ given feature $i$ is already in the model:\n",
    "\n",
    "$I_{j|i} = I(\\text{model with } i,j) - I(\\text{model with } i)$\n",
    "\n",
    "This helps understand feature redundancy and complementarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### Algorithm Selection Framework\n",
    "\n",
    "```\n",
    "DECISION TREE: Choosing the Right Configuration\n",
    "\n",
    "Dataset Size?\n",
    "├─ Small (<10K): Use exact split finding\n",
    "└─ Large (>10K): Use histogram method\n",
    "\n",
    "Overfitting Risk?\n",
    "├─ High: Enable DART regularization\n",
    "└─ Low: Standard gradient boosting\n",
    "\n",
    "Training Speed Priority?\n",
    "├─ High: Enable GOSS sampling\n",
    "└─ Low: Use full dataset\n",
    "\n",
    "Accuracy Requirements?\n",
    "├─ Maximum: Second-order + exact splits\n",
    "└─ Balanced: Second-order + histogram + GOSS\n",
    "```\n",
    "\n",
    "### Hyperparameter Guidelines\n",
    "\n",
    "**Core Parameters:**\n",
    "```\n",
    "learning_rate ∈ [0.01, 0.3]\n",
    "├─ 0.01-0.05: Conservative, needs more trees\n",
    "├─ 0.1: Good default\n",
    "└─ 0.2-0.3: Aggressive, fewer trees needed\n",
    "\n",
    "n_estimators ∈ [50, 3000]\n",
    "├─ Depends inversely on learning_rate\n",
    "└─ Use early stopping for optimal count\n",
    "\n",
    "max_depth ∈ [3, 10]\n",
    "├─ 3-6: Good for most problems\n",
    "└─ 7-10: Deep trees, risk overfitting\n",
    "```\n",
    "\n",
    "**Advanced Parameters:**\n",
    "```\n",
    "GOSS Configuration:\n",
    "├─ top_rate: 0.2 (keep 20% top gradients)\n",
    "├─ other_rate: 0.1 (sample 10% others)\n",
    "└─ Effective dataset: 30% of original\n",
    "\n",
    "DART Configuration:\n",
    "├─ drop_rate: 0.1 (moderate regularization)\n",
    "├─ max_drop: min(50, n_trees/4)\n",
    "└─ skip_drop: 0.5 (skip dropout 50% of time)\n",
    "\n",
    "Histogram Configuration:\n",
    "├─ max_bins: 255 (LightGBM default)\n",
    "└─ binning_method: quantile (balanced bins)\n",
    "```\n",
    "\n",
    "### Performance Optimization Strategies\n",
    "\n",
    "**Memory Optimization:**\n",
    "```\n",
    "Data Layout:\n",
    "├─ Use float32 instead of float64 (halves memory)\n",
    "├─ Ensure C-contiguous arrays\n",
    "└─ Consider memory-mapped files for huge datasets\n",
    "\n",
    "Tree Storage:\n",
    "├─ Compress leaf values\n",
    "├─ Use compact tree representations\n",
    "└─ Implement tree pruning for minimal accuracy loss\n",
    "```\n",
    "\n",
    "**Computational Optimization:**\n",
    "```\n",
    "Parallelization:\n",
    "├─ Feature-level: Evaluate splits in parallel\n",
    "├─ Data-level: Batch processing for large datasets\n",
    "└─ NUMA-aware: Pin threads to CPU cores\n",
    "\n",
    "Algorithmic:\n",
    "├─ Early stopping: Monitor validation loss\n",
    "├─ Feature selection: Remove irrelevant features\n",
    "└─ Smart initialization: Better than naive mean\n",
    "```\n",
    "\n",
    "### Convergence and Stopping Criteria\n",
    "\n",
    "**Early Stopping Implementation:**\n",
    "```\n",
    "ALGORITHM: Early Stopping\n",
    "patience = 50\n",
    "best_score = -∞\n",
    "wait_count = 0\n",
    "\n",
    "FOR each iteration:\n",
    "    validation_score = EVALUATE(validation_set)\n",
    "    \n",
    "    IF validation_score > best_score:\n",
    "        best_score = validation_score\n",
    "        wait_count = 0\n",
    "        SAVE_MODEL()\n",
    "    ELSE:\n",
    "        wait_count += 1\n",
    "        \n",
    "    IF wait_count >= patience:\n",
    "        BREAK  // Stop training\n",
    "        \n",
    "LOAD_BEST_MODEL()\n",
    "```\n",
    "\n",
    "### Debugging and Monitoring\n",
    "\n",
    "**Key Metrics to Track:**\n",
    "- Training vs validation loss curves\n",
    "- Feature importance evolution\n",
    "- Gradient magnitude distribution (for GOSS)\n",
    "- Tree depth and leaf statistics\n",
    "- Memory usage and training time per iteration\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "```\n",
    "Problem: Overfitting\n",
    "├─ Increase regularization (DART, L1/L2)\n",
    "├─ Reduce learning_rate\n",
    "├─ Limit max_depth\n",
    "└─ Enable early stopping\n",
    "\n",
    "Problem: Slow convergence\n",
    "├─ Increase learning_rate\n",
    "├─ Use second-order methods\n",
    "├─ Check feature preprocessing\n",
    "└─ Verify loss function choice\n",
    "\n",
    "Problem: Memory issues\n",
    "├─ Enable GOSS sampling\n",
    "├─ Use histogram method\n",
    "├─ Reduce max_bins\n",
    "└─ Implement batch processing\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
