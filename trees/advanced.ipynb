{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62259a0b",
   "metadata": {},
   "source": [
    "# Advanced Tree Learning Concepts: Mathematical Foundations\n",
    "\n",
    "## Table of Contents\n",
    "1. [Gradient-based One-Side Sampling (GOSS)](#goss)\n",
    "2. [Dropouts meet Multiple Additive Regression Trees (DART)](#dart)\n",
    "3. [Leaf-wise vs Level-wise Growth](#leaf-vs-level)\n",
    "4. [Feature Bundling and Optimization](#feature-bundling)\n",
    "5. [Mathematical Foundations](#mathematical-foundations)\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient-based One-Side Sampling (GOSS) {#goss}\n",
    "\n",
    "### Intuitive Concept\n",
    "GOSS is a sampling technique that intelligently selects training samples based on their gradients. The key insight is that **samples with larger gradients contribute more to learning**, while samples with small gradients are already well-fitted.\n",
    "\n",
    "Think of it like a teacher focusing more attention on students who are struggling (large gradients) rather than those who already understand the material (small gradients).\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let $\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)\\}$ be our training dataset.\n",
    "\n",
    "For each sample $i$, we have gradient $g_i$ and Hessian $h_i$:\n",
    "- $g_i = \\frac{\\partial L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}_i)}$\n",
    "- $h_i = \\frac{\\partial^2 L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}_i)^2}$\n",
    "\n",
    "**GOSS Algorithm:**\n",
    "\n",
    "1. **Sort samples by gradient magnitude**: $|g_1| \\geq |g_2| \\geq \\ldots \\geq |g_n|$\n",
    "\n",
    "2. **Select top-a samples**: $A = \\{i : i \\leq a \\cdot n\\}$ (large gradients)\n",
    "\n",
    "3. **Randomly sample from remaining**: $B \\sim \\text{Random}(\\{a \\cdot n + 1, \\ldots, n\\}, b \\cdot n)$ (small gradients)\n",
    "\n",
    "4. **Compute weighted information gain**:\n",
    "   $$\\text{Gain}_{GOSS} = \\frac{1}{|A| + \\frac{1-a}{b}|B|} \\left[ \\frac{(\\sum_{i \\in A} g_i + \\frac{1-a}{b}\\sum_{i \\in B} g_i)^2}{\\sum_{i \\in A} h_i + \\frac{1-a}{b}\\sum_{i \\in B} h_i} \\right]$$\n",
    "\n",
    "The factor $\\frac{1-a}{b}$ **amplifies** the contribution of small-gradient samples to compensate for under-sampling.\n",
    "\n",
    "### Why GOSS Works\n",
    "\n",
    "**Theorem (GOSS Approximation):** Under mild conditions, GOSS provides an unbiased estimate of the full-data information gain:\n",
    "\n",
    "$$\\mathbb{E}[\\text{Gain}_{GOSS}] \\approx \\text{Gain}_{Full}$$\n",
    "\n",
    "with variance reduction of approximately $\\frac{a + b(1-a)^2}{1}$ compared to random sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## Dropouts meet Multiple Additive Regression Trees (DART) {#dart}\n",
    "\n",
    "### Intuitive Concept\n",
    "DART addresses **overfitting in boosting** by applying dropout techniques to trees. Instead of always adding new trees to all previous trees, DART randomly \"drops out\" some previous trees during training.\n",
    "\n",
    "This is like studying for an exam by sometimes ignoring your previous notes to avoid over-relying on them.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "Standard gradient boosting:\n",
    "$$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\gamma_m h_m(\\mathbf{x})$$\n",
    "\n",
    "**DART modification:**\n",
    "$$F_m(\\mathbf{x}) = \\sum_{k \\in \\mathcal{K}_m} \\tilde{\\gamma}_k h_k(\\mathbf{x}) + \\gamma_m h_m(\\mathbf{x})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{K}_m \\subset \\{1, 2, \\ldots, m-1\\}$ is the set of **non-dropped trees**\n",
    "- $\\tilde{\\gamma}_k$ are **normalized weights** for non-dropped trees\n",
    "\n",
    "### DART Algorithm\n",
    "\n",
    "1. **Dropout Selection**: For each iteration $m$, randomly select trees to drop:\n",
    "   $$\\mathcal{D}_m \\sim \\text{Binomial}(\\{1, \\ldots, m-1\\}, p_{drop})$$\n",
    "   \n",
    "2. **Weight Normalization**: Normalize weights of remaining trees:\n",
    "   $$\\tilde{\\gamma}_k = \\gamma_k \\cdot \\frac{|\\mathcal{K}_m|}{m-1} \\quad \\text{for } k \\in \\mathcal{K}_m$$\n",
    "\n",
    "3. **New Tree Training**: Train $h_m$ on residuals from the reduced ensemble:\n",
    "   $$r_i^{(m)} = y_i - \\sum_{k \\in \\mathcal{K}_m} \\tilde{\\gamma}_k h_k(\\mathbf{x}_i)$$\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "**Regularization Effect**: DART introduces implicit regularization through:\n",
    "\n",
    "$$\\mathbb{E}[\\text{Model Complexity}] = (1-p_{drop}) \\cdot \\text{Standard Complexity}$$\n",
    "\n",
    "**Variance Reduction**: The expected prediction variance:\n",
    "$$\\text{Var}[F_{DART}(\\mathbf{x})] \\leq (1-p_{drop}^2) \\cdot \\text{Var}[F_{GBM}(\\mathbf{x})]$$\n",
    "\n",
    "---\n",
    "\n",
    "## Leaf-wise vs Level-wise Growth {#leaf-vs-level}\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Level-wise (Breadth-First):\n",
    "       Root\n",
    "      /    \\\n",
    "    N1      N2      ← Level 1: Split both nodes\n",
    "   /  \\    /  \\\n",
    "  N3  N4  N5  N6    ← Level 2: Split all nodes\n",
    "\n",
    "Leaf-wise (Best-First):\n",
    "       Root\n",
    "      /    \\\n",
    "    N1      N2\n",
    "   /  \\      \n",
    "  N3  N4     \n",
    " /  \\        \n",
    "N5  N6       ← Only split the most beneficial leaf\n",
    "```\n",
    "\n",
    "### Mathematical Analysis\n",
    "\n",
    "**Information Gain Comparison:**\n",
    "\n",
    "For level-wise splitting, we maximize:\n",
    "$$\\Delta_{\\text{level}} = \\sum_{i \\in \\text{Level } L} \\max_{\\text{split } s_i} \\text{Gain}(s_i)$$\n",
    "\n",
    "For leaf-wise splitting:\n",
    "$$\\Delta_{\\text{leaf}} = \\max_{\\text{leaf } j} \\max_{\\text{split } s_j} \\text{Gain}(s_j)$$\n",
    "\n",
    "**Theorem**: $\\Delta_{\\text{leaf}} \\geq \\Delta_{\\text{level}}$ (leaf-wise is locally optimal)\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "| Method | Time per Level | Memory | Overfitting Risk |\n",
    "|--------|----------------|---------|------------------|\n",
    "| Level-wise | $O(2^L \\cdot n \\cdot d)$ | $O(2^L)$ | Lower |\n",
    "| Leaf-wise | $O(k \\cdot n \\cdot d)$ | $O(k)$ | Higher |\n",
    "\n",
    "Where $L$ is depth, $k$ is number of leaves, $n$ is samples, $d$ is features.\n",
    "\n",
    "### Regularization for Leaf-wise Growth\n",
    "\n",
    "To prevent overfitting, leaf-wise methods use:\n",
    "\n",
    "1. **Minimum samples per leaf**: $\\text{samples}_{\\text{leaf}} \\geq \\lambda_1$\n",
    "2. **Maximum depth**: $\\text{depth} \\leq \\lambda_2$  \n",
    "3. **Minimum gain threshold**: $\\text{Gain} \\geq \\lambda_3$\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Bundling and Optimization {#feature-bundling}\n",
    "\n",
    "### Exclusive Feature Bundling (EFB)\n",
    "\n",
    "**Core Insight**: Many features are mutually exclusive (sparse), so they can be bundled together.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Features $f_i$ and $f_j$ are bundled if:\n",
    "$$\\text{Conflict}(f_i, f_j) = \\frac{|\\{k : f_i(\\mathbf{x}_k) \\neq 0 \\text{ and } f_j(\\mathbf{x}_k) \\neq 0\\}|}{n} < \\theta$$\n",
    "\n",
    "**Bundle Construction Algorithm:**\n",
    "1. Build conflict graph $G = (V, E)$ where $V$ are features, $E$ are conflicts\n",
    "2. Find graph coloring (NP-hard, use greedy approximation)\n",
    "3. Each color class becomes a bundle\n",
    "\n",
    "**Bundle Merging:**\n",
    "For features $f_1, f_2, \\ldots, f_k$ in a bundle:\n",
    "$$f_{\\text{bundle}}(\\mathbf{x}) = \\sum_{i=1}^k \\text{offset}_i \\cdot f_i(\\mathbf{x})$$\n",
    "\n",
    "Where $\\text{offset}_i$ ensures no value collision.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundations {#mathematical-foundations}\n",
    "\n",
    "### Gradient Boosting Framework\n",
    "\n",
    "The general objective function:\n",
    "$$\\mathcal{L}(\\Theta) = \\sum_{i=1}^n L(y_i, F(\\mathbf{x}_i)) + \\sum_{m=1}^M \\Omega(h_m)$$\n",
    "\n",
    "Where:\n",
    "- $L(\\cdot, \\cdot)$ is the loss function\n",
    "- $\\Omega(\\cdot)$ is regularization\n",
    "- $F(\\mathbf{x}) = \\sum_{m=1}^M \\gamma_m h_m(\\mathbf{x})$\n",
    "\n",
    "### Second-order Approximation\n",
    "\n",
    "Taylor expansion of loss around current prediction:\n",
    "$$L(y_i, F_{m-1}(\\mathbf{x}_i) + h_m(\\mathbf{x}_i)) \\approx L(y_i, F_{m-1}(\\mathbf{x}_i)) + g_i h_m(\\mathbf{x}_i) + \\frac{1}{2}h_i h_m^2(\\mathbf{x}_i)$$\n",
    "\n",
    "This leads to the optimal leaf weight:\n",
    "$$w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$$\n",
    "\n",
    "And information gain:\n",
    "$$\\text{Gain} = \\frac{1}{2}\\left[\\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i + \\lambda}\\right] - \\gamma$$\n",
    "\n",
    "### Convergence Analysis\n",
    "\n",
    "**Theorem (Boosting Convergence)**: Under conditions:\n",
    "1. $\\|h_m\\|_\\infty \\leq M$ (bounded weak learners)\n",
    "2. Edge condition: $\\gamma_m \\geq \\gamma > 0$\n",
    "\n",
    "The training error decreases exponentially:\n",
    "$$\\text{Error}_m \\leq \\text{Error}_0 \\exp(-2\\gamma^2 m)$$\n",
    "\n",
    "### Bias-Variance Decomposition\n",
    "\n",
    "For ensemble $F(\\mathbf{x}) = \\sum_{m=1}^M \\alpha_m h_m(\\mathbf{x})$:\n",
    "\n",
    "$$\\mathbb{E}[(y - F(\\mathbf{x}))^2] = \\text{Bias}^2[F(\\mathbf{x})] + \\text{Var}[F(\\mathbf{x})] + \\sigma^2$$\n",
    "\n",
    "Where:\n",
    "- **Bias**: $\\mathbb{E}[F(\\mathbf{x})] - f^*(\\mathbf{x})$\n",
    "- **Variance**: $\\mathbb{E}[(F(\\mathbf{x}) - \\mathbb{E}[F(\\mathbf{x})])^2]$\n",
    "\n",
    "**Key Insights:**\n",
    "- More trees → Lower bias, Higher variance\n",
    "- DART and regularization → Control variance\n",
    "- GOSS → Maintain bias while reducing computation\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Considerations\n",
    "\n",
    "### Memory Optimization\n",
    "- **Histogram-based splitting**: $O(k)$ instead of $O(n)$ for split finding\n",
    "- **Feature bundling**: Reduces feature space dimensionality\n",
    "- **Gradient quantization**: Compress gradients to save memory\n",
    "\n",
    "### Parallel Optimization\n",
    "- **Data parallelism**: Split data across workers\n",
    "- **Feature parallelism**: Split features across workers  \n",
    "- **Hybrid approach**: Combine both for optimal scaling\n",
    "\n",
    "### Hyperparameter Sensitivity\n",
    "\n",
    "| Parameter | Effect | Typical Range |\n",
    "|-----------|--------|---------------|\n",
    "| `learning_rate` | Bias-variance tradeoff | [0.01, 0.3] |\n",
    "| `max_depth` | Model complexity | [3, 10] |\n",
    "| `min_samples_leaf` | Overfitting control | [1, 100] |\n",
    "| `goss_top_rate` | Sample efficiency | [0.1, 0.5] |\n",
    "| `dart_drop_rate` | Regularization strength | [0.01, 0.2] |\n",
    "\n",
    "These advanced techniques work synergistically to create highly efficient and accurate tree-based models while maintaining interpretability and preventing overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
