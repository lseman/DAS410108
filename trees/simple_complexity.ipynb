{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e2ecd5",
   "metadata": {},
   "source": [
    "# Decision Tree Time Complexity\n",
    "\n",
    "## 0. Notation and Setup\n",
    "\n",
    "* Data: $\\{(x_i, y_i)\\}_{i=1}^n$, $x_i \\in \\mathbb{R}^p$.\n",
    "* Tree built by recursive binary splitting.\n",
    "* “Balanced” means each split roughly halves the samples: $n_L \\approx n_R \\approx n/2$.\n",
    "* “Worst case” means maximally unbalanced: $n_L=1,\\; n_R=n-1$.\n",
    "* For classification, let $K$ be the number of classes (treat $K$ as a small constant unless otherwise noted).\n",
    "\n",
    "**Split core:**\n",
    "At a node with $n$ samples, searching for the best split across all $p$ features.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Naïve CART (per-node sort)\n",
    "\n",
    "### 1.1 Per-Node Complexity\n",
    "\n",
    "**Theorem 1 (Naïve per-node cost).**\n",
    "Finding the best split at one node costs $T_{\\text{split}}(n,p) = O(p\\,n\\log n)$.\n",
    "\n",
    "**Proof (step by step).**\n",
    "\n",
    "1. For each feature $j \\in \\{1,\\dots,p\\}$, sort the $n$ values: $O(n \\log n)$.\n",
    "2. Sweep through the sorted list once, maintaining running (left/right) statistics:\n",
    "\n",
    "   * Regression: constant-time updates per candidate threshold $\\Rightarrow O(n)$.\n",
    "   * Classification: $O(K)$ per threshold with running counts $\\Rightarrow O(Kn)$ (take $K$ as constant).\n",
    "3. Per feature: $O(n\\log n) + O(n) = O(n\\log n)$.\n",
    "4. Over $p$ features: $O(p\\,n\\log n)$. ∎\n",
    "\n",
    "**Remark (why not $O(p\\,n^2)$?).**\n",
    "The running-sum trick avoids recomputing impurities from scratch; we only do $O(1)$ (or $O(K)$) work per adjacent threshold in sorted order.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Whole-Tree Complexity (Naïve)\n",
    "\n",
    "Let $T(n,p)$ be total time to build a tree on $n$ samples.\n",
    "\n",
    "**Recurrence (any split):**\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; O(p\\,n\\log n) \\;+\\; T(n_L,p) \\;+\\; T(n_R,p),\\quad n_L+n_R=n.\n",
    "$$\n",
    "\n",
    "#### Worst case (maximally unbalanced)\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; O(p\\,n\\log n) \\;+\\; T(n-1,p).\n",
    "$$\n",
    "\n",
    "**Unroll (explicit):**\n",
    "\n",
    "$$\n",
    "T(n,p) = \\sum_{k=1}^{n} O(p\\,k\\log k) \n",
    "= O\\!\\Big(p\\sum_{k=1}^{n} k\\log k\\Big).\n",
    "$$\n",
    "\n",
    "**Bound the sum (integral comparison):**\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{n} k\\log k \n",
    "\\;\\;=\\;\\; \\Theta(n^2\\log n).\n",
    "$$\n",
    "\n",
    "Therefore $T(n,p)=O(p\\,n^2\\log n)$.\n",
    "\n",
    "**ASCII view (worst case):**\n",
    "\n",
    "```\n",
    "Level 0:  n samples  → cost ~ c p n log n\n",
    "Level 1:  n-1        → cost ~ c p (n-1) log(n-1)\n",
    "Level 2:  n-2        → cost ~ c p (n-2) log(n-2)\n",
    "...\n",
    "Level n-1: 1         → cost ~ c p (1) log 1 ≈ 0\n",
    "Sum ~ c p * [n log n + (n-1)log(n-1) + ...] = O(p n^2 log n)\n",
    "```\n",
    "\n",
    "#### Balanced case\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; 2T\\!\\left(\\frac{n}{2},p\\right) + O(p\\,n\\log n).\n",
    "$$\n",
    "\n",
    "**Recursion tree (work per level):**\n",
    "\n",
    "* Level $\\ell$: $2^\\ell$ nodes, each of size $\\approx n/2^\\ell$.\n",
    "* Per node cost: $c\\,p\\,(n/2^\\ell)\\log(n/2^\\ell)$.\n",
    "* Level $\\ell$ total:\n",
    "\n",
    "  $$\n",
    "  2^\\ell \\cdot c\\,p\\,(n/2^\\ell)\\big(\\log n - \\ell\\big)\n",
    "  \\;=\\; c\\,p\\,n\\big(\\log n - \\ell\\big).\n",
    "  $$\n",
    "* Depth: $\\log_2 n$.\n",
    "\n",
    "**Sum over levels $\\ell=0,\\dots,\\log n - 1$:**\n",
    "\n",
    "$$\n",
    "\\sum_{\\ell=0}^{\\log n - 1} c\\,p\\,n(\\log n - \\ell)\n",
    "= c\\,p\\,n \\underbrace{\\sum_{t=1}^{\\log n} t}_{=\\;\\Theta((\\log n)^2)}\n",
    "= \\Theta(p\\,n\\,(\\log n)^2).\n",
    "$$\n",
    "\n",
    "**ASCII recursion tree (balanced, naïve):**\n",
    "\n",
    "```\n",
    "Level 0: 1 node of size n        → cost ≈ c p n (log n)\n",
    "Level 1: 2 nodes of size n/2     → cost ≈ 2 * c p (n/2) (log(n/2)) = c p n (log n - 1)\n",
    "Level 2: 4 nodes of size n/4     → cost ≈ c p n (log n - 2)\n",
    "...\n",
    "Level L: 2^L nodes size n/2^L    → cost ≈ c p n (log n - L)\n",
    "\n",
    "Sum L=0..log n-1 of [c p n (log n - L)] = c p n [log n + (log n - 1) + ... + 1]\n",
    "= Θ(p n (log n)^2)\n",
    "```\n",
    "\n",
    "**Conclusion (naïve CART):**\n",
    "\n",
    "* Worst case: $O(p\\,n^2\\log n)$\n",
    "* Balanced: $O(p\\,n\\log^2 n)$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Presorted CART (classic optimization)\n",
    "\n",
    "**Idea.** Presort each feature **once** at the root and maintain **sorted index lists** down the tree. Then each node no longer pays $O(n\\log n)$ per feature—only a **linear sweep** and an $O(n)$ partition of indices per child.\n",
    "\n",
    "### 2.1 Per-Node Complexity (Presorted)\n",
    "\n",
    "**Theorem 2 (Presorted per-node cost).**\n",
    "With per-feature presorting carried once and index lists maintained, the per-node split search costs $O(p\\,n)$ (plus $O(n)$ to partition indices), not $O(p\\,n\\log n)$.\n",
    "\n",
    "**Proof (sketch).**\n",
    "\n",
    "* At a node, for each feature, we already have that node’s samples in **sorted order** (by inheriting/partitioning the parent’s sorted lists).\n",
    "* We sweep once per feature, maintaining cumulative statistics $\\Rightarrow O(n)$ per feature $\\Rightarrow O(p\\,n)$.\n",
    "* Partitioning the sorted index lists into left/right for the chosen split is $O(n)$ per feature **in total**, but with careful bookkeeping you only physically partition **once** for the chosen split; the other features’ index orders are **filtered** to children via stable partition or pointer views. Practical implementations make this linear in $n$. ∎\n",
    "\n",
    "### 2.2 Whole-Tree Complexity (Presorted)\n",
    "\n",
    "Balanced recurrence:\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; 2T\\Big(\\frac{n}{2},p\\Big) + O(p\\,n).\n",
    "$$\n",
    "\n",
    "By Master Theorem ($a=2, b=2, f(n)=\\Theta(n)$):\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; O(p\\,n\\log n).\n",
    "$$\n",
    "\n",
    "Worst case (unbalanced):\n",
    "\n",
    "$$\n",
    "T(n,p) \\;=\\; O(p\\,n) + T(n-1,p) \\;\\Rightarrow\\; O(p\\,n^2).\n",
    "$$\n",
    "\n",
    "**ASCII recursion tree (balanced, presorted):**\n",
    "\n",
    "```\n",
    "Level 0:  cost ≈ c p n\n",
    "Level 1:  cost ≈ c p n\n",
    "...\n",
    "Level log n - 1: cost ≈ c p n\n",
    "#levels ≈ log n → total ≈ c p n log n\n",
    "```\n",
    "\n",
    "**Takeaway:** Presorting removes one $\\log n$ factor vs naïve CART.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Histogram CART (binning)\n",
    "\n",
    "**Key idea.** Discretize each feature into $B \\ll n$ bins, and evaluate splits only at **bin boundaries**.\n",
    "\n",
    "### 3.1 Two variants you can teach\n",
    "\n",
    "* **Variant A (simple):** Per node, reassign all $n$ samples into bins for all $p$ features $\\Rightarrow O(p\\,n)$ per node; then evaluate $B-1$ thresholds $\\Rightarrow O(p\\,B)$.\n",
    "* **Variant B (incremental/prefix sums):** Maintain **prefix sums** of bin statistics and update **incrementally** when splitting, so the dominant node cost is $O(p\\,B)$ (nearly independent of $n$).\n",
    "\n",
    "### 3.2 Preprocessing\n",
    "\n",
    "* Build bin edges per feature (quantiles or uniform): $O(p\\,n\\log n)$ if using quantiles; $O(p\\,n)$ if using single pass with approximate quantiles / sketches.\n",
    "* Initialize per-feature, per-bin statistics.\n",
    "\n",
    "### 3.3 Per-Node Costs (detailed)\n",
    "\n",
    "**Theorem 3 (Histogram per-node cost).**\n",
    "\n",
    "* Variant A: $O(p\\,n) + O(p\\,B) = O(p(n+B))$.\n",
    "* Variant B (incremental): $O(p\\,B)$.\n",
    "\n",
    "**Proof (step by step).**\n",
    "\n",
    "1. **Assignment / stats.**\n",
    "\n",
    "   * Variant A: sweep samples $\\Rightarrow O(p\\,n)$.\n",
    "   * Variant B: children’s histograms = parent’s prefix sums “cut” at the chosen threshold; updates are $O(p\\,B)$.\n",
    "2. **Split search.**\n",
    "   For each feature, try $B-1$ bin cuts; with running prefix/suffix sums the scan is $O(B)$ per feature $\\Rightarrow O(p\\,B)$.\n",
    "3. Sum both parts. ∎\n",
    "\n",
    "### 3.4 Whole-Tree Complexity (balanced)\n",
    "\n",
    "* **Variant A (simple, with re-binning):**\n",
    "\n",
    "  $$\n",
    "  T(n,p) = 2T(n/2,p) + O(p\\,n) \\;\\Rightarrow\\; O(p\\,n\\log n).\n",
    "  $$\n",
    "* **Variant B (incremental, $O(pB)$ per node):**\n",
    "  There are $O(n)$ total nodes across all levels (bounded by $2n\\!-\\!1$ in a binary tree with $n$ leaves).\n",
    "  Total $= O\\big((\\#\\text{nodes}) \\cdot pB\\big) = O(p\\,B\\,n)$.\n",
    "  Add preprocessing $O(p\\,n\\log n)$ (quantile bins).\n",
    "  **Total:** $O(p\\,n\\log n + p\\,B\\,n)$.\n",
    "  For fixed $B$ (e.g., $B\\le 255$), this is $O(p\\,n\\log n)$.\n",
    "\n",
    "**ASCII recursion tree (balanced, Variant A):**\n",
    "\n",
    "```\n",
    "Level 0:  cost ≈ c p n\n",
    "Level 1:  cost ≈ c p n\n",
    "...\n",
    "Level log n - 1: cost ≈ c p n\n",
    "Total ≈ c p n log n\n",
    "```\n",
    "\n",
    "**Worst case (unbalanced).**\n",
    "\n",
    "* Variant A: $\\sum_{k=1}^{n} O(p\\,k) = O(p\\,n^2)$.\n",
    "* Variant B: $\\sum_{k=1}^{n} O(p\\,B) = O(p\\,B\\,n)$ (plus preprocessing).\n",
    "\n",
    "**Practical note.** Modern GBM libraries approximate quantiles with sketches to reduce preprocessing to near-linear time and memory.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Worked Mini-Examples\n",
    "\n",
    "### 4.1 Balanced, naïve CART with $n=8$, $p$ fixed\n",
    "\n",
    "Costs per level:\n",
    "\n",
    "* Level 0: $c\\,p\\,8\\log 8 = c\\,p\\,8\\cdot 3 = 24cp$\n",
    "* Level 1: $2 \\times c\\,p\\,(4\\log 4) = 2 \\times c\\,p\\,4\\cdot 2 = 16cp$\n",
    "* Level 2: $4 \\times c\\,p\\,(2\\log 2) = 4 \\times c\\,p\\,2\\cdot 1 = 8cp$\n",
    "\n",
    "Total $= (24+16+8)cp = 48cp = c\\,p\\,8\\cdot (3+2+1) = c\\,p\\,n \\sum_{t=1}^{\\log n} t$.\n",
    "\n",
    "### 4.2 Balanced, presorted CART with $n=8$\n",
    "\n",
    "Each level costs $\\approx c\\,p\\,n = 8cp$, and there are $\\log_2 8=3$ levels.\n",
    "Total $= 3 \\cdot 8cp = 24cp = O(p\\,n\\log n)$.\n",
    "\n",
    "### 4.3 Balanced, histogram (Variant A) with $n=8$\n",
    "\n",
    "Each level costs $\\approx c\\,p\\,n = 8cp$, three levels $\\Rightarrow 24cp$, same asymptotics as presorted: $O(p\\,n\\log n)$.\n",
    "If Variant B is used, each node costs $\\approx c\\,p\\,B$, total nodes scale $O(n)$ $\\Rightarrow O(p\\,B\\,n)$ plus preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Memory & Cache, Step-by-Step\n",
    "\n",
    "### 5.1 Memory\n",
    "\n",
    "* **Naïve / Presorted CART**\n",
    "\n",
    "  * Data matrix: $O(n\\,p)$.\n",
    "  * Presorted indices: $O(n\\,p)$ integers (can be kept per feature, shared across nodes via views).\n",
    "  * Tree: $O(\\#\\text{nodes}) \\le O(n)$.\n",
    "\n",
    "* **Histogram CART**\n",
    "\n",
    "  * Bin edges: $O(p\\,B)$.\n",
    "  * Bin statistics: classification $O(p\\,B\\,K)$, regression $O(p\\,B)$.\n",
    "  * Tree: $O(n)$.\n",
    "\n",
    "### 5.2 Cache\n",
    "\n",
    "* **Naïve (per-feature scans over $n$ values)** → many cache misses for large $n$.\n",
    "* **Histogram (operate on $B$ counters)** → $B\\ll n$ fits in L2/L3; sequential memory patterns; drastically fewer misses.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Parallel Complexity (outline)\n",
    "\n",
    "* **Naïve / Presorted CART**\n",
    "\n",
    "  * Per-feature split scans can parallelize across features: up to $p$-way parallelism (memory-bandwidth limited).\n",
    "  * Across nodes at the same depth: also parallelizable.\n",
    "\n",
    "* **Histogram CART**\n",
    "\n",
    "  * Building/merging histograms across features and nodes is **embarrassingly parallel**.\n",
    "  * Variant B (prefix sums) benefits from SIMD-friendly scans over small $B$.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Ensemble Costs (per tree contrasted)\n",
    "\n",
    "| Method             | Per-node cost   | Balanced total per tree      | Worst-case total per tree |\n",
    "| ------------------ | --------------- | ---------------------------- | ------------------------- |\n",
    "| Naïve CART         | $O(p\\,n\\log n)$ | $O(p\\,n\\log^2 n)$            | $O(p\\,n^2\\log n)$         |\n",
    "| Presorted CART     | $O(p\\,n)$       | $O(p\\,n\\log n)$              | $O(p\\,n^2)$               |\n",
    "| Histogram (Var. A) | $O(p(n+B))$     | $O(p\\,n\\log n)$              | $O(p\\,n^2)$               |\n",
    "| Histogram (Var. B) | $O(p\\,B)$       | $O(p\\,n\\log n) + O(p\\,B\\,n)$ | $O(p\\,B\\,n)$              |\n",
    "\n",
    "**Random Forest (B trees).**\n",
    "\n",
    "* Using presorted or histogram trees:\n",
    "\n",
    "  * Presorted, balanced: $O(B\\,p\\,n\\log n)$.\n",
    "  * Histogram (Var. B), balanced: $O(B\\,(p\\,n\\log n + p\\,B\\,n))$.\n",
    "* **Parallelism:** nearly linear across trees.\n",
    "\n",
    "**Gradient Boosting (M iterations).**\n",
    "\n",
    "* Each iteration fits one tree to residuals (same per-tree cost as above).\n",
    "* Total:\n",
    "\n",
    "  * Presorted: $O(M\\,p\\,n\\log n)$ (balanced).\n",
    "  * Histogram (Var. B): $O(M\\,(p\\,n\\log n + p\\,B\\,n))$.\n",
    "* **Sequential bottleneck** across iterations persists.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Final Comparison (intuitive summary)\n",
    "\n",
    "1. **Naïve CART** pays $\\log n$ inside each level → $O(p\\,n\\log^2 n)$ balanced.\n",
    "2. **Presorted CART** removes that inner $\\log n$ → $O(p\\,n\\log n)$ balanced.\n",
    "3. **Histogram CART** trades exact thresholds for $B$ bins:\n",
    "\n",
    "   * Simple Variant A re-bins: $O(p\\,n\\log n)$ balanced.\n",
    "   * Incremental Variant B makes per-node work \\~$O(p\\,B)$ → total $O(p\\,n\\log n + p\\,B\\,n)$; with fixed small $B$, this is effectively $O(p\\,n\\log n)$ and very cache-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. ASCII Recursion Trees — Ready for Slides\n",
    "\n",
    "### 9.1 Naïve CART (balanced)\n",
    "\n",
    "```\n",
    "                   n (cost ~ c p n log n)\n",
    "                   /                     \\\n",
    "        n/2 (c p (n/2) log(n/2))   n/2 (c p (n/2) log(n/2))\n",
    "           /           \\               /           \\\n",
    "      n/4 (...)    n/4 (...)      n/4 (...)    n/4 (...)\n",
    "        .                                 .\n",
    "        .                                 .\n",
    "Depth ~ log2 n\n",
    "\n",
    "Work per level:\n",
    "  L = 0:  c p n (log n)\n",
    "  L = 1:  c p n (log n - 1)\n",
    "  ...\n",
    "  L = log n - 1: c p n (1)\n",
    "\n",
    "Total ≈ c p n * (1 + 2 + ... + log n) = Θ(p n (log n)^2)\n",
    "```\n",
    "\n",
    "### 9.2 Presorted CART (balanced)\n",
    "\n",
    "```\n",
    "                   n (cost ~ c p n)\n",
    "                   /               \\\n",
    "            n/2 (c p n/2)      n/2 (c p n/2)\n",
    "               /      \\           /      \\\n",
    "           n/4        n/4     n/4        n/4\n",
    "               ...                 ...\n",
    "\n",
    "Work per level:\n",
    "  Always ≈ c p n\n",
    "#levels ≈ log n → total ≈ c p n log n\n",
    "```\n",
    "\n",
    "### 9.3 Histogram CART (Variant A; re-bin each node)\n",
    "\n",
    "```\n",
    "Same shape as presorted.\n",
    "Work per level still ≈ c p n (for the re-binning)\n",
    "Total ≈ c p n log n\n",
    "```\n",
    "\n",
    "### 9.4 Histogram CART (Variant B; incremental O(pB) per node)\n",
    "\n",
    "```\n",
    "Every node ~ O(p B), independent of its n_i\n",
    "Total nodes in a binary tree with n leaves: O(n)\n",
    "Total ≈ O(p B n)  (+ preprocessing O(p n log n) for quantile bins)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Pedagogical Takeaways (for students)\n",
    "\n",
    "* **One optimization removes one log.** Naïve vs presorted is the difference between $\\log^2 n$ and $\\log n$.\n",
    "* **Histograms buy speed via approximation.** With small $B$, per-node work is tiny and cache-friendly.\n",
    "* **Balanced vs worst-case matters.** Always mention both: $O(p\\,n\\log n)$ vs $O(p\\,n^2)$ (presorted/hist).\n",
    "* **Ensembles scale differently.** Random forests parallelize across trees; boosting remains sequential across rounds.\n",
    "* **Asymptotics vs constants.** For small $n$, constants and cache dominate; for large $n$, these complexity differences decide feasibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e426b0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350944c5",
   "metadata": {},
   "source": [
    "# CumSum Trick\n",
    "\n",
    "## Setup\n",
    "\n",
    "Suppose at a node we consider feature $j$. After sorting the samples by $x_{\\cdot,j}$, the paired (feature, target) order is:\n",
    "\n",
    "$$\n",
    "\\big((x_{(1),j},y_{(1)}),\\dots,(x_{(6),j},y_{(6)})\\big)\n",
    "\\quad\\text{with}\\quad\n",
    "y_{(1..6)} = [2,\\;3,\\;5,\\;1,\\;4,\\;2].\n",
    "$$\n",
    "\n",
    "Totals over all 6 samples:\n",
    "\n",
    "$$\n",
    "n=6,\\quad\n",
    "S_1^{\\text{tot}}=\\sum y_i=17,\\quad\n",
    "S_2^{\\text{tot}}=\\sum y_i^2=59.\n",
    "$$\n",
    "\n",
    "For a subset $S$ with size $n_S$, sum $S_1=\\sum_{i\\in S} y_i$, sum of squares $S_2=\\sum_{i\\in S} y_i^2$,\n",
    "the **SSE** is\n",
    "\n",
    "$$\n",
    "\\mathrm{SSE}(S)=S_2-\\frac{S_1^2}{n_S}.\n",
    "$$\n",
    "\n",
    "(Weighted MSE impurity is just $(\\mathrm{SSE}_L+\\mathrm{SSE}_R)/n$; minimizing total SSE or weighted MSE is equivalent for a fixed $n$.)\n",
    "\n",
    "We consider thresholds **between** consecutive sorted points: after 1st, 2nd, …, 5th sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Running-sum sweep (one pass)\n",
    "\n",
    "Initialize:\n",
    "\n",
    "* Left (L): $n_L=0,\\; S_{1L}=0,\\; S_{2L}=0$.\n",
    "* Right (R): $n_R=6,\\; S_{1R}=17,\\; S_{2R}=59$.\n",
    "\n",
    "At step $k$ we move $y_{(k)}$ from R to L and evaluate the split **between** $x_{(k),j}$ and $x_{(k+1),j}$.\n",
    "\n",
    "Update rules (O(1) each step):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&n_L \\leftarrow n_L+1,\\quad S_{1L} \\leftarrow S_{1L}+y_{(k)},\\quad S_{2L} \\leftarrow S_{2L}+y_{(k)}^2,\\\\\n",
    "&n_R \\leftarrow n_R-1,\\quad S_{1R} \\leftarrow S_{1R}-y_{(k)},\\quad S_{2R} \\leftarrow S_{2R}-y_{(k)}^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then compute\n",
    "\n",
    "$$\n",
    "\\mathrm{SSE}_L=S_{2L}-\\frac{S_{1L}^2}{n_L},\\qquad\n",
    "\\mathrm{SSE}_R=S_{2R}-\\frac{S_{1R}^2}{n_R},\\qquad\n",
    "\\mathrm{SSE}_{\\text{total}}=\\mathrm{SSE}_L+\\mathrm{SSE}_R.\n",
    "$$\n",
    "\n",
    "We’ll tabulate all 5 candidate splits:\n",
    "\n",
    "|  k | move $y_{(k)}$ | $n_L,S_{1L},S_{2L}$ | $n_R,S_{1R},S_{2R}$ |     $\\mathrm{SSE}_L$    |    $\\mathrm{SSE}_R$    | $\\mathrm{SSE}_{\\text{total}}$ |\n",
    "| -: | :------------- | :------------------ | :------------------ | :---------------------: | :--------------------: | :---------------------------: |\n",
    "|  1 | 2              | $1,2,4$             | $5,15,55$           |        $4-4/1=0$        |  $55-15^2/5=55-45=10$  |           **10.000**          |\n",
    "|  2 | 3              | $2,5,13$            | $4,12,46$           |      $13-25/2=0.5$      |  $46-12^2/4=46-36=10$  |           **10.500**          |\n",
    "|  3 | 5              | $3,10,38$           | $3,7,21$            | $38-100/3\\approx4.6667$ | $21-49/3\\approx4.6667$ |           **9.3333**          |\n",
    "|  4 | 1              | $4,11,39$           | $2,6,20$            |     $39-121/4=8.75$     |       $20-36/2=2$      |           **10.750**          |\n",
    "|  5 | 4              | $5,15,55$           | $1,2,4$             |      $55-225/5=10$      |        $4-4/1=0$       |           **10.000**          |\n",
    "\n",
    "* **Best split** is after $k=3$ (between the 3rd and 4th sorted samples), with\n",
    "  $\\mathrm{SSE}_{\\text{total}}\\approx 9.3333$.\n",
    "\n",
    "If you prefer **MSE** impurity (weighted by child sizes), divide the total SSE by $n=6$; the argmin is unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this is fast\n",
    "\n",
    "* We sorted the feature **once**: $O(n\\log n)$.\n",
    "* Then we evaluated all $n-1$ thresholds in **one linear sweep** with **O(1)** work per threshold.\n",
    "* **Per feature cost:** $O(n\\log n) + O(n) = O(n\\log n)$, instead of $O(n^2)$ if we recomputed sums from scratch at each threshold."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
