{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd31875b",
   "metadata": {},
   "source": [
    "# Guide to Bagging Algorithms\n",
    "\n",
    "## Theoretical Foundations\n",
    "\n",
    "### The Bootstrap Principle\n",
    "\n",
    "**Core Idea:** Given a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, create diverse training sets through resampling to reduce prediction variance.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For an unstable base learner $h$, bagging constructs an ensemble:\n",
    "\n",
    "$$F(x) = \\frac{1}{M} \\sum_{m=1}^M h^{(m)}(x)$$\n",
    "\n",
    "where each $h^{(m)}$ is trained on bootstrap sample $\\mathcal{D}^{(m)}$.\n",
    "\n",
    "### Bias-Variance Decomposition in Bagging\n",
    "\n",
    "**Expected Test Error:** For regression with squared loss:\n",
    "\n",
    "$$\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "**Bagging's Effect:**\n",
    "- **Bias:** $\\text{Bias}[F(x)] \\approx \\text{Bias}[h(x)]$ (nearly unchanged)\n",
    "- **Variance:** $\\text{Var}[F(x)] = \\rho \\text{Var}[h(x)] + \\frac{1-\\rho}{M}\\text{Var}[h(x)]$\n",
    "\n",
    "where $\\rho$ is the correlation between base predictors.\n",
    "\n",
    "**Key Insight:** Bagging is most effective when:\n",
    "1. Base learner has high variance (e.g., deep trees)\n",
    "2. Base predictors have low correlation ($\\rho \\to 0$)\n",
    "\n",
    "---\n",
    "\n",
    "## Bootstrap Sampling Theory\n",
    "\n",
    "### Classical Bootstrap\n",
    "\n",
    "**Sampling Process:**\n",
    "```\n",
    "ALGORITHM: Bootstrap Sampling\n",
    "INPUT: Dataset D = {(x_i, y_i)}_{i=1}^n, number of samples M\n",
    "OUTPUT: M bootstrap datasets\n",
    "\n",
    "FOR m = 1 to M:\n",
    "    D^(m) = {}\n",
    "    FOR i = 1 to n:\n",
    "        idx = RANDOM_INTEGER(1, n)  // With replacement\n",
    "        D^(m).ADD(D[idx])\n",
    "    \n",
    "RETURN {D^(1), D^(2), ..., D^(M)}\n",
    "```\n",
    "\n",
    "**Statistical Properties:**\n",
    "- Probability a sample appears in bootstrap: $1 - (1 - 1/n)^n \\approx 1 - e^{-1} \\approx 63.2\\%$\n",
    "- Expected unique samples per bootstrap: $n(1 - e^{-1}) \\approx 0.632n$\n",
    "- Out-of-bag samples: $\\approx 0.368n$\n",
    "\n",
    "### Advanced Bootstrap Variants\n",
    "\n",
    "#### 1. Balanced Bootstrap\n",
    "Ensures each original sample appears exactly once across all bootstraps:\n",
    "\n",
    "$$\\sum_{m=1}^M \\mathbf{1}[i \\in \\mathcal{D}^{(m)}] = M \\quad \\forall i$$\n",
    "\n",
    "```\n",
    "ALGORITHM: Balanced Bootstrap\n",
    "INPUT: Dataset D, M (must divide n evenly)\n",
    "\n",
    "1. Create M copies of indices [1,2,...,n]\n",
    "2. Concatenate into single array of size M×n\n",
    "3. Randomly shuffle the array\n",
    "4. Split into M chunks of size n\n",
    "5. Each chunk defines one bootstrap sample\n",
    "```\n",
    "\n",
    "#### 2. Subsampling (Subbagging)\n",
    "Sample $s < n$ points **without replacement**:\n",
    "\n",
    "**Advantages:**\n",
    "- Lower computational cost\n",
    "- Reduced correlation between base learners\n",
    "- Better for very large datasets\n",
    "\n",
    "**Variance Analysis:**\n",
    "For subsample fraction $r = s/n$:\n",
    "\n",
    "$$\\text{Var}[\\text{Subbag}] = \\left[\\rho + \\frac{(1-\\rho)}{M}\\right] \\cdot \\frac{1}{r} \\cdot \\text{Var}[h]$$\n",
    "\n",
    "**Optimal Subsample Rate:**\n",
    "Theoretical optimum around $r^* \\approx 0.632$ (same as bootstrap coverage), but empirically $r \\in [0.5, 0.8]$ often works well.\n",
    "\n",
    "```\n",
    "ALGORITHM: Subsampling\n",
    "INPUT: Dataset D, subsample size s, M\n",
    "\n",
    "FOR m = 1 to M:\n",
    "    indices = RANDOM_SAMPLE_WITHOUT_REPLACEMENT(n, s)\n",
    "    D^(m) = D[indices]\n",
    "```\n",
    "\n",
    "#### 3. Poisson Bootstrap\n",
    "Each sample $i$ appears $k_i$ times where $k_i \\sim \\text{Poisson}(\\lambda)$:\n",
    "\n",
    "**Mathematical Framework:**\n",
    "- With $\\lambda = 1$: $\\mathbb{E}[|D^{(m)}|] = n$, $\\text{Var}[|D^{(m)}|] = n$\n",
    "- Limiting distribution matches classical bootstrap\n",
    "- Easily vectorizable and parallelizable\n",
    "\n",
    "```\n",
    "ALGORITHM: Poisson Bootstrap\n",
    "INPUT: Dataset D, intensity λ = 1, M\n",
    "\n",
    "FOR m = 1 to M:\n",
    "    FOR i = 1 to n:\n",
    "        k_i = POISSON_RANDOM(λ)\n",
    "        Add (x_i, y_i) to D^(m) exactly k_i times\n",
    "```\n",
    "\n",
    "#### 4. m-out-of-n Bootstrap\n",
    "Bootstrap samples of size $m \\neq n$:\n",
    "\n",
    "**Theory:** Useful when $m = o(n)$ (e.g., $m = \\sqrt{n}$ or $m = n^{2/3}$) for:\n",
    "- Reducing computational cost\n",
    "- Improving finite-sample properties\n",
    "- Handling heavy-tailed distributions\n",
    "\n",
    "**When to Use:**\n",
    "- Very large datasets ($n > 10^6$)\n",
    "- Noisy labels with outliers\n",
    "- Streaming/online learning scenarios\n",
    "\n",
    "### Bootstrap Sample Complexity\n",
    "\n",
    "**Effective Sample Size:** The number of unique samples in a bootstrap follows:\n",
    "\n",
    "$$\\mathbb{E}[\\text{Unique samples}] = n\\left(1 - \\left(1 - \\frac{1}{n}\\right)^n\\right)$$\n",
    "\n",
    "**Asymptotic Distribution:**\n",
    "As $n \\to \\infty$, the number of unique samples is approximately:\n",
    "\n",
    "$$\\text{Unique} \\sim \\mathcal{N}(n(1-e^{-1}), n e^{-1}(1-e^{-1}))$$\n",
    "\n",
    "---\n",
    "\n",
    "## Variance Reduction Mathematics\n",
    "\n",
    "### Correlation-Variance Trade-off\n",
    "\n",
    "**Ensemble Variance Formula:**\n",
    "For $M$ predictors with individual variance $\\sigma^2$ and pairwise correlation $\\rho$:\n",
    "\n",
    "$$\\text{Var}[\\bar{h}] = \\rho \\sigma^2 + \\frac{1-\\rho}{M} \\sigma^2$$\n",
    "\n",
    "**Decomposition:**\n",
    "- **Irreducible component:** $\\rho \\sigma^2$ (correlation-induced floor)\n",
    "- **Reducible component:** $\\frac{1-\\rho}{M} \\sigma^2$ (decreases with $M$)\n",
    "\n",
    "### Correlation Sources and Mitigation\n",
    "\n",
    "#### 1. Data Correlation\n",
    "Bootstrap samples overlap → correlated predictions\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "- Subsampling with lower overlap\n",
    "- Feature bagging (random subspaces)\n",
    "- Different training algorithms per tree\n",
    "\n",
    "#### 2. Model Correlation\n",
    "Similar model architectures learn similar patterns\n",
    "\n",
    "**Quantifying Correlation:**\n",
    "For predictions $\\hat{y}_m(x)$ from different models:\n",
    "\n",
    "$$\\rho(x) = \\frac{\\text{Cov}[\\hat{y}_i(x), \\hat{y}_j(x)]}{\\sqrt{\\text{Var}[\\hat{y}_i(x)] \\text{Var}[\\hat{y}_j(x)]}}$$\n",
    "\n",
    "**Empirical Correlation Estimation:**\n",
    "```\n",
    "ALGORITHM: Estimate Prediction Correlation\n",
    "INPUT: Ensemble {h_m}, test set T\n",
    "\n",
    "correlations = []\n",
    "FOR each pair (i,j) where i < j:\n",
    "    pred_i = [h_i(x) for x in T]\n",
    "    pred_j = [h_j(x) for x in T]\n",
    "    corr_ij = PEARSON_CORRELATION(pred_i, pred_j)\n",
    "    correlations.ADD(corr_ij)\n",
    "\n",
    "average_correlation = MEAN(correlations)\n",
    "```\n",
    "\n",
    "### Optimal Ensemble Size\n",
    "\n",
    "**Theoretical Minimum Variance:**\n",
    "As $M \\to \\infty$:\n",
    "\n",
    "$$\\lim_{M \\to \\infty} \\text{Var}[\\bar{h}] = \\rho \\sigma^2$$\n",
    "\n",
    "**Practical Considerations:**\n",
    "- **Plateau effect:** Diminishing returns after variance reduction plateaus\n",
    "- **Computational budget:** Training time scales linearly with $M$\n",
    "- **Memory constraints:** Model storage requirements\n",
    "\n",
    "**Adaptive Stopping Rule:**\n",
    "```\n",
    "ALGORITHM: Adaptive Ensemble Size\n",
    "tolerance = 0.001\n",
    "window_size = 10\n",
    "recent_improvements = []\n",
    "\n",
    "FOR m = window_size+1 to MAX_M:\n",
    "    current_error = OOB_ERROR(ensemble[:m])\n",
    "    prev_error = OOB_ERROR(ensemble[:m-1])\n",
    "    improvement = prev_error - current_error\n",
    "    \n",
    "    recent_improvements.ADD(improvement)\n",
    "    IF LENGTH(recent_improvements) > window_size:\n",
    "        recent_improvements.POP_FRONT()\n",
    "    \n",
    "    avg_improvement = MEAN(recent_improvements)\n",
    "    IF avg_improvement < tolerance:\n",
    "        RETURN m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Core Bagging Variants\n",
    "\n",
    "### Algorithm Comparison Matrix\n",
    "\n",
    "| Variant | Sample Size | Replacement | Overlap | Best Use Case |\n",
    "|---------|-------------|-------------|---------|---------------|\n",
    "| **Bootstrap** | $n$ | Yes | ~63% | Standard bagging, moderate $n$ |\n",
    "| **Subbagging** | $s < n$ | No | $\\frac{s}{n}$ | Large datasets, speed priority |\n",
    "| **Poisson** | $\\sim n$ | Weighted | ~63% | Streaming, parallel processing |\n",
    "| **m-out-of-n** | $m \\neq n$ | Yes | Variable | Theory-driven, heavy tails |\n",
    "| **Balanced** | $n$ | Constrained | Exactly 1 | Minimize sampling variance |\n",
    "\n",
    "### Subbagging Deep Dive\n",
    "\n",
    "**Theoretical Analysis:**\n",
    "For subsample fraction $r = s/n$, the prediction variance becomes:\n",
    "\n",
    "$$\\text{Var}[\\text{Subbag}] = \\sigma^2 \\left[\\rho + \\frac{1-\\rho}{M}\\right] \\cdot \\frac{1}{r}$$\n",
    "\n",
    "**Bias Introduction:**\n",
    "Unlike bootstrap, subsampling introduces bias:\n",
    "\n",
    "$$\\text{Bias}[\\text{Subbag}] = (1-r) \\cdot \\text{Bias}_{\\text{finite-sample}}$$\n",
    "\n",
    "**Optimal Trade-off:**\n",
    "Minimize MSE = Bias² + Variance:\n",
    "\n",
    "$$r^* = \\arg\\min_r \\left\\{ (1-r)^2 \\text{Bias}^2 + \\frac{\\sigma^2[\\rho + (1-\\rho)/M]}{r} \\right\\}$$\n",
    "\n",
    "### Poisson Bootstrap Implementation\n",
    "\n",
    "**Efficient Vectorized Version:**\n",
    "```\n",
    "ALGORITHM: Vectorized Poisson Bootstrap\n",
    "INPUT: Dataset D, M ensembles\n",
    "\n",
    "# Generate all Poisson counts at once\n",
    "counts = POISSON_RANDOM(λ=1, shape=(M, n))\n",
    "\n",
    "FOR m = 1 to M:\n",
    "    # Create weighted dataset\n",
    "    sample_weights = counts[m, :]\n",
    "    D_weighted = CREATE_WEIGHTED_DATASET(D, sample_weights)\n",
    "    h_m = TRAIN_WITH_WEIGHTS(D_weighted)\n",
    "```\n",
    "\n",
    "**Online/Streaming Adaptation:**\n",
    "```\n",
    "ALGORITHM: Online Poisson Bagging\n",
    "STATE: M online models {h_m}\n",
    "\n",
    "ON_NEW_SAMPLE (x, y):\n",
    "    FOR m = 1 to M:\n",
    "        k = POISSON_RANDOM(λ=1)\n",
    "        FOR _ in 1 to k:\n",
    "            h_m.UPDATE(x, y)  # Online learning update\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forests Deep Dive\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Enhanced Variance Reduction:**\n",
    "Random Forests combine two decorrelation mechanisms:\n",
    "1. **Bootstrap sampling** (data randomness)\n",
    "2. **Feature bagging** (feature randomness)\n",
    "\n",
    "**Combined Variance Formula:**\n",
    "$$\\text{Var}[RF] = \\left(\\rho_{\\text{data}} \\cdot \\rho_{\\text{feature}} + \\frac{1-\\rho_{\\text{data}}}{M_{\\text{trees}}} + \\frac{1-\\rho_{\\text{feature}}}{M_{\\text{try}}}\\right) \\sigma^2$$\n",
    "\n",
    "### Feature Selection Strategy\n",
    "\n",
    "**Theoretical Optimal $m_{try}$:**\n",
    "For $d$ features, theoretical analysis suggests:\n",
    "\n",
    "- **Classification:** $m_{try} = \\sqrt{d}$\n",
    "- **Regression:** $m_{try} = d/3$\n",
    "\n",
    "**Empirical Refinements:**\n",
    "Based on extensive experiments, better defaults are often:\n",
    "\n",
    "```\n",
    "Classification:\n",
    "├─ Small d (<20): m_try = max(1, √d)  \n",
    "├─ Medium d (20-1000): m_try = max(2, √d)\n",
    "└─ Large d (>1000): m_try = max(10, √d/2)\n",
    "\n",
    "Regression:  \n",
    "├─ Small d (<20): m_try = max(1, d/3)\n",
    "├─ Medium d (20-1000): m_try = max(5, d/3)  \n",
    "└─ Large d (>1000): m_try = max(20, d/5)\n",
    "```\n",
    "\n",
    "**Adaptive Feature Selection:**\n",
    "```\n",
    "ALGORITHM: Adaptive m_try Selection\n",
    "INPUT: Dataset D, validation set V\n",
    "\n",
    "m_try_candidates = [√d/2, √d, 2√d, d/5, d/3, d/2]\n",
    "best_score = -∞\n",
    "best_m_try = √d\n",
    "\n",
    "FOR m_try in m_try_candidates:\n",
    "    rf = TRAIN_RANDOM_FOREST(D, m_try=m_try, n_trees=100)\n",
    "    score = EVALUATE(rf, V)\n",
    "    \n",
    "    IF score > best_score:\n",
    "        best_score = score\n",
    "        best_m_try = m_try\n",
    "\n",
    "RETURN best_m_try\n",
    "```\n",
    "\n",
    "### Split Finding in Random Forests\n",
    "\n",
    "**Standard Approach:**\n",
    "```\n",
    "ALGORITHM: RF Split Finding (Node-level)\n",
    "INPUT: Node samples I, features F, m_try\n",
    "\n",
    "1. feature_subset = RANDOM_SAMPLE(F, m_try)\n",
    "2. best_gain = -∞\n",
    "3. best_split = None\n",
    "\n",
    "4. FOR each feature j in feature_subset:\n",
    "   FOR each threshold θ in CANDIDATE_THRESHOLDS(j):\n",
    "       gain = CALCULATE_IMPURITY_REDUCTION(j, θ, I)\n",
    "       IF gain > best_gain:\n",
    "           best_gain = gain\n",
    "           best_split = (j, θ)\n",
    "\n",
    "5. RETURN best_split\n",
    "```\n",
    "\n",
    "**Histogram-Based Optimization:**\n",
    "For large datasets, use binning:\n",
    "\n",
    "```\n",
    "ALGORITHM: Histogram-Based RF Split\n",
    "PARAMS: max_bins B (typically 32-255)\n",
    "\n",
    "1. # Pre-compute histograms for sampled features\n",
    "2. FOR each feature j in feature_subset:\n",
    "   hist_j = BUILD_HISTOGRAM(samples[I, j], targets[I], B)\n",
    "   \n",
    "3. # Fast sweep over bins\n",
    "4. FOR each feature j:\n",
    "   FOR each bin_boundary θ in hist_j:\n",
    "       gain = FAST_GAIN_FROM_HISTOGRAM(hist_j, θ)\n",
    "       UPDATE_BEST_IF_BETTER(gain, (j, θ))\n",
    "```\n",
    "\n",
    "### Tree Growing Strategies\n",
    "\n",
    "#### 1. Depth-First Growing (Standard)\n",
    "```\n",
    "ALGORITHM: Depth-First Tree Growing\n",
    "FUNCTION GROW_NODE(samples, depth):\n",
    "    IF STOPPING_CRITERION(samples, depth):\n",
    "        RETURN LEAF(samples)\n",
    "    \n",
    "    split = FIND_BEST_SPLIT(samples)\n",
    "    IF split is None:\n",
    "        RETURN LEAF(samples)\n",
    "    \n",
    "    left_samples, right_samples = PARTITION(samples, split)\n",
    "    left_child = GROW_NODE(left_samples, depth+1)\n",
    "    right_child = GROW_NODE(right_samples, depth+1)\n",
    "    \n",
    "    RETURN INTERNAL_NODE(split, left_child, right_child)\n",
    "```\n",
    "\n",
    "#### 2. Breadth-First Growing\n",
    "Better memory locality, easier parallelization:\n",
    "\n",
    "```\n",
    "ALGORITHM: Breadth-First Tree Growing\n",
    "queue = [ROOT_NODE(all_samples)]\n",
    "\n",
    "WHILE queue is not empty:\n",
    "    current_level = queue\n",
    "    next_level = []\n",
    "    \n",
    "    FOR node in current_level:\n",
    "        IF not SHOULD_SPLIT(node):\n",
    "            CONTINUE\n",
    "        \n",
    "        split = FIND_BEST_SPLIT(node.samples)\n",
    "        left, right = PARTITION(node.samples, split)\n",
    "        \n",
    "        node.left = CREATE_NODE(left)\n",
    "        node.right = CREATE_NODE(right)\n",
    "        \n",
    "        next_level.ADD(node.left, node.right)\n",
    "    \n",
    "    queue = next_level\n",
    "```\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "**Multi-criteria Stopping:**\n",
    "```\n",
    "FUNCTION STOPPING_CRITERION(samples, depth):\n",
    "    RETURN (\n",
    "        LENGTH(samples) < min_samples_split OR\n",
    "        depth >= max_depth OR\n",
    "        PURITY(samples) > purity_threshold OR\n",
    "        BEST_GAIN(samples) < min_impurity_decrease\n",
    "    )\n",
    "```\n",
    "\n",
    "**Advanced Criteria:**\n",
    "\n",
    "1. **Statistical Significance Testing:**\n",
    "   ```\n",
    "   chi2_stat = CALCULATE_CHI_SQUARE(left_dist, right_dist)\n",
    "   p_value = CHI_SQUARE_TEST(chi2_stat, df=classes-1)\n",
    "   IF p_value > significance_threshold:\n",
    "       STOP_SPLITTING()\n",
    "   ```\n",
    "\n",
    "2. **Minimum Description Length (MDL):**\n",
    "   ```\n",
    "   current_cost = ENCODING_COST(samples)\n",
    "   split_cost = ENCODING_COST(left) + ENCODING_COST(right) + SPLIT_OVERHEAD\n",
    "   IF split_cost >= current_cost:\n",
    "       STOP_SPLITTING()\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Extremely Randomized Trees\n",
    "\n",
    "### Mathematical Motivation\n",
    "\n",
    "**Bias-Variance Analysis:**\n",
    "ExtraTrees introduces more bias but dramatically reduces variance:\n",
    "\n",
    "- **Extra Bias:** From suboptimal splits\n",
    "- **Variance Reduction:** From maximally decorrelated trees\n",
    "\n",
    "**Expected Performance:**\n",
    "$$\\text{MSE}[\\text{ExtraTrees}] = \\text{Bias}^2[\\text{RF}] + \\Delta\\text{Bias}^2 + \\frac{\\text{Var}[\\text{RF}]}{C}$$\n",
    "\n",
    "where $C > 1$ is the decorrelation factor and $\\Delta\\text{Bias}^2$ is additional bias from random splits.\n",
    "\n",
    "### Split Generation Algorithms\n",
    "\n",
    "#### 1. Uniform Random Splits\n",
    "```\n",
    "ALGORITHM: Uniform Random Split Generation\n",
    "INPUT: Feature j, samples at node, K candidates\n",
    "\n",
    "min_val = MIN(samples[:, j])\n",
    "max_val = MAX(samples[:, j])\n",
    "\n",
    "best_gain = -∞\n",
    "best_threshold = None\n",
    "\n",
    "FOR k = 1 to K:\n",
    "    θ = UNIFORM_RANDOM(min_val, max_val)\n",
    "    gain = CALCULATE_GAIN(j, θ, samples)\n",
    "    IF gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_threshold = θ\n",
    "\n",
    "RETURN (j, best_threshold, best_gain)\n",
    "```\n",
    "\n",
    "#### 2. Quantile-Based Random Splits\n",
    "More principled than uniform sampling:\n",
    "\n",
    "```\n",
    "ALGORITHM: Quantile Random Split Generation\n",
    "INPUT: Feature j, samples at node, K candidates\n",
    "\n",
    "sorted_values = SORT(samples[:, j])\n",
    "n_unique = LENGTH(UNIQUE(sorted_values))\n",
    "\n",
    "# Generate K random quantiles\n",
    "quantiles = UNIFORM_RANDOM(0, 1, K)\n",
    "thresholds = [QUANTILE(sorted_values, q) for q in quantiles]\n",
    "\n",
    "# Evaluate each threshold\n",
    "FOR θ in thresholds:\n",
    "    gain = CALCULATE_GAIN(j, θ, samples)\n",
    "    UPDATE_BEST_IF_BETTER(gain, (j, θ))\n",
    "```\n",
    "\n",
    "#### 3. Histogram-Based Random Splits\n",
    "For pre-binned features:\n",
    "\n",
    "```\n",
    "ALGORITHM: Histogram Random Split Generation\n",
    "INPUT: Pre-computed histogram bins, K candidates\n",
    "\n",
    "available_bins = NON_EMPTY_BINS(histogram)\n",
    "selected_bins = RANDOM_SAMPLE(available_bins, min(K, len(available_bins)))\n",
    "\n",
    "FOR bin_idx in selected_bins:\n",
    "    θ = bin_boundaries[bin_idx]\n",
    "    gain = FAST_GAIN_FROM_HISTOGRAM(bin_idx)\n",
    "    UPDATE_BEST_IF_BETTER(gain, (feature_j, θ))\n",
    "```\n",
    "\n",
    "### ExtraTrees Hyperparameters\n",
    "\n",
    "**Key Parameters:**\n",
    "- **K (n_random_cuts):** Number of random thresholds per feature\n",
    "  - Low K: More bias, less variance\n",
    "  - High K: Approaches optimal splits\n",
    "  - Typical: K = 1 (maximally random) to K = 10\n",
    "\n",
    "- **Bootstrap:** Usually False (use full dataset)\n",
    "  - Randomness comes from splits, not sampling\n",
    "  - Can enable for very large datasets\n",
    "\n",
    "- **Feature Selection:** Often use more features than RF\n",
    "  - Since splits are suboptimal, need more features to compensate\n",
    "\n",
    "**Hyperparameter Guidelines:**\n",
    "```\n",
    "ExtraTrees Configuration:\n",
    "├─ K = 1: Maximum randomness, lowest correlation\n",
    "├─ K = 3-5: Good balance for most problems  \n",
    "├─ K = 10+: Approaches Random Forest behavior\n",
    "└─ K = all: Equivalent to Random Forest\n",
    "\n",
    "m_try (ExtraTrees):\n",
    "├─ Often higher than RF: 2×√d to d\n",
    "├─ Since splits are random, need more feature diversity\n",
    "```\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "**Computational Complexity:**\n",
    "- **Split Finding:** O(K × m_try) vs O(n × m_try) for RF\n",
    "- **Memory:** Lower (no need to store/sort all split candidates)\n",
    "- **Parallelization:** Excellent (less coordination needed)\n",
    "\n",
    "**Statistical Properties:**\n",
    "- **Consistency:** Still consistent under regularity conditions\n",
    "- **Rate of Convergence:** Potentially slower than optimal methods\n",
    "- **Robustness:** Very robust to outliers and noise\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Split Families\n",
    "\n",
    "### Axis-Aligned Splits (Enhanced)\n",
    "\n",
    "**Optimal Threshold Selection:**\n",
    "For continuous features, consider multiple threshold selection strategies:\n",
    "\n",
    "```\n",
    "ALGORITHM: Multi-Strategy Threshold Selection\n",
    "strategies = [\n",
    "    \"midpoint_between_classes\",\n",
    "    \"entropy_weighted_mean\", \n",
    "    \"gini_optimal\",\n",
    "    \"quantile_based\"\n",
    "]\n",
    "\n",
    "FOR each strategy:\n",
    "    thresholds = GENERATE_THRESHOLDS(feature, strategy)\n",
    "    FOR θ in thresholds:\n",
    "        gain = EVALUATE_SPLIT(feature, θ)\n",
    "        UPDATE_GLOBAL_BEST()\n",
    "```\n",
    "\n",
    "**Missing Value Handling:**\n",
    "```\n",
    "ALGORITHM: Missing Value Split Evaluation  \n",
    "# Three-way split: left, right, missing\n",
    "FOR each threshold θ:\n",
    "    left_samples = samples[feature <= θ]\n",
    "    right_samples = samples[feature > θ]  \n",
    "    missing_samples = samples[IS_MISSING(feature)]\n",
    "    \n",
    "    # Evaluate all possible assignments of missing values\n",
    "    gain_left = IMPURITY_GAIN(left + missing, right, missing_penalty=0.1)\n",
    "    gain_right = IMPURITY_GAIN(left, right + missing, missing_penalty=0.1)\n",
    "    gain_separate = IMPURITY_GAIN(left, right, missing, missing_penalty=0.2)\n",
    "    \n",
    "    best_assignment = ARGMAX(gain_left, gain_right, gain_separate)\n",
    "```\n",
    "\n",
    "### Oblique Splits in Bagging\n",
    "\n",
    "**Challenge:** Oblique splits can increase correlation between trees\n",
    "**Solution:** Careful regularization and feature subsampling\n",
    "\n",
    "```\n",
    "ALGORITHM: Regularized Oblique Splits for Bagging\n",
    "INPUT: Node samples, feature_subset, regularization λ\n",
    "\n",
    "# Step 1: Feature selection for oblique split\n",
    "IF LENGTH(feature_subset) > max_oblique_features:\n",
    "    feature_subset = SELECT_TOP_K(feature_subset, max_oblique_features)\n",
    "\n",
    "# Step 2: Fit regularized linear classifier\n",
    "X_node = samples[:, feature_subset]\n",
    "y_binary = BINARIZE_TARGETS(samples.targets)  # For split direction\n",
    "\n",
    "# Ridge regression with strong regularization  \n",
    "weights = RIDGE_REGRESSION(X_node, y_binary, alpha=λ)\n",
    "\n",
    "# Step 3: Sparsify weights (feature selection)\n",
    "weights = SOFT_THRESHOLD(weights, threshold=0.1 * MAX(ABS(weights)))\n",
    "\n",
    "# Step 4: Find optimal threshold\n",
    "projections = X_node @ weights\n",
    "optimal_threshold = FIND_BEST_THRESHOLD(projections, y_binary)\n",
    "\n",
    "RETURN ObliqueCondition(feature_subset, weights, optimal_threshold)\n",
    "```\n",
    "\n",
    "**Oblique Forest Hyperparameters:**\n",
    "```\n",
    "Oblique Random Forest:\n",
    "├─ lambda: 0.1 - 1.0 (regularization strength)\n",
    "├─ max_oblique_features: min(10, m_try) \n",
    "├─ sparsity_threshold: 0.05 - 0.2\n",
    "└─ oblique_probability: 0.3 (fraction of oblique vs axial splits)\n",
    "```\n",
    "\n",
    "### Categorical Splits\n",
    "\n",
    "**Multi-class Categorical Features:**\n",
    "```\n",
    "ALGORITHM: Optimal Categorical Split\n",
    "INPUT: Categorical feature with levels {c_1, c_2, ..., c_K}\n",
    "\n",
    "IF K <= 10:  # Small cardinality - exact search\n",
    "    # Enumerate all 2^(K-1) possible binary partitions\n",
    "    FOR each subset S ⊆ {c_1, ..., c_K}:\n",
    "        gain = EVALUATE_BINARY_SPLIT(feature ∈ S vs feature ∉ S)\n",
    "        UPDATE_BEST()\n",
    "\n",
    "ELSE:  # Large cardinality - heuristic search\n",
    "    # Order categories by target statistic\n",
    "    stats = [MEAN_TARGET(samples[feature == c]) for c in categories]\n",
    "    ordered_categories = SORT_BY(categories, stats)\n",
    "    \n",
    "    # Search contiguous prefixes (reduces search space)\n",
    "    FOR i = 1 to K-1:\n",
    "        S = ordered_categories[:i]\n",
    "        gain = EVALUATE_BINARY_SPLIT(feature ∈ S vs feature ∉ S)\n",
    "        UPDATE_BEST()\n",
    "```\n",
    "\n",
    "**High-Cardinality Categorical Features:**\n",
    "```\n",
    "ALGORITHM: Hash-Based Categorical Splits\n",
    "INPUT: High-cardinality categorical feature\n",
    "\n",
    "# Strategy 1: Feature hashing\n",
    "hash_buckets = HASH_CATEGORIES_TO_BUCKETS(categories, n_buckets=32)\n",
    "FOR each bucket_subset:\n",
    "    EVALUATE_SPLIT(feature_hash ∈ bucket_subset)\n",
    "\n",
    "# Strategy 2: Frequency-based grouping  \n",
    "rare_categories = CATEGORIES_WITH_FREQUENCY_BELOW(threshold=0.01)\n",
    "frequent_categories = COMPLEMENT(rare_categories)\n",
    "splits = [\n",
    "    frequent_categories vs rare_categories,\n",
    "    TOP_K_FREQUENT vs REST,\n",
    "    RANDOM_BINARY_PARTITION(frequent_categories)\n",
    "]\n",
    "\n",
    "# Strategy 3: Target-based clustering\n",
    "category_embeddings = TARGET_STATISTIC_EMBEDDINGS(categories)\n",
    "clusters = KMEANS(category_embeddings, k=4)  \n",
    "FOR each cluster_subset:\n",
    "    EVALUATE_SPLIT(feature ∈ cluster_subset)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Out-of-Bag Estimation\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**OOB Error as Cross-Validation:**\n",
    "For bootstrap sample $m$, let $\\mathcal{O}_m$ be the OOB set. The OOB prediction for sample $i$ is:\n",
    "\n",
    "$$\\hat{y}_i^{\\text{OOB}} = \\frac{1}{|\\{m : i \\in \\mathcal{O}_m\\}|} \\sum_{m: i \\in \\mathcal{O}_m} h^{(m)}(x_i)$$\n",
    "\n",
    "**Theoretical Properties:**\n",
    "- **Unbiased estimator** of generalization error under mild conditions\n",
    "- **Variance:** Higher than k-fold CV due to uneven OOB set sizes\n",
    "- **Computational cost:** Free (computed during training)\n",
    "\n",
    "### Advanced OOB Techniques\n",
    "\n",
    "#### 1. Weighted OOB Estimation\n",
    "Account for varying OOB set sizes:\n",
    "\n",
    "```\n",
    "ALGORITHM: Weighted OOB Estimation\n",
    "oob_predictions = {}\n",
    "oob_weights = {}\n",
    "\n",
    "FOR m = 1 to M:\n",
    "    oob_indices = COMPLEMENT(bootstrap_indices[m])\n",
    "    \n",
    "    FOR i in oob_indices:\n",
    "        IF i not in oob_predictions:\n",
    "            oob_predictions[i] = 0\n",
    "            oob_weights[i] = 0\n",
    "            \n",
    "        weight = 1.0 / SQRT(LENGTH(oob_indices))  # Weight by OOB set size\n",
    "        oob_predictions[i] += weight * h_m(x_i)  \n",
    "        oob_weights[i] += weight\n",
    "\n",
    "# Normalize predictions\n",
    "FOR i in oob_predictions:\n",
    "    oob_predictions[i] /= oob_weights[i]\n",
    "\n",
    "oob_error = CALCULATE_ERROR(y[oob_predictions.keys()], oob_predictions.values())\n",
    "```\n",
    "\n",
    "#### 2. Progressive OOB Monitoring\n",
    "Track OOB error evolution during training:\n",
    "\n",
    "```\n",
    "ALGORITHM: Progressive OOB Monitoring\n",
    "oob_scores = []\n",
    "early_stopping_patience = 50\n",
    "best_score = -∞\n",
    "patience_counter = 0\n",
    "\n",
    "FOR m = 1 to MAX_M:\n",
    "    # Train next tree\n",
    "    h_m = TRAIN_TREE(bootstrap_sample[m])\n",
    "    ensemble.ADD(h_m)\n",
    "    \n",
    "    # Update OOB predictions\n",
    "    UPDATE_OOB_PREDICTIONS(h_m, oob_indices[m])\n",
    "    \n",
    "    # Calculate current OOB score  \n",
    "    current_oob = CALCULATE_OOB_ERROR()\n",
    "    oob_scores.ADD(current_oob)\n",
    "    \n",
    "    # Early stopping logic\n",
    "    IF current_oob > best_score:\n",
    "        best_score = current_oob\n",
    "        patience_counter = 0\n",
    "    ELSE:\n",
    "        patience_counter += 1\n",
    "        IF patience_counter >= early_stopping_patience:\n",
    "            BREAK\n",
    "```\n",
    "\n",
    "### OOB-Based Model Selection\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "```\n",
    "ALGORITHM: OOB Hyperparameter Selection\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'm_try': [√d, d/3, d/2, d], \n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "best_score = -∞\n",
    "best_params = None\n",
    "\n",
    "FOR param_combination in CARTESIAN_PRODUCT(param_grid):\n",
    "    rf = RandomForest(**param_combination)\n",
    "    rf.FIT(training_data)\n",
    "    oob_score = rf.OOB_SCORE()\n",
    "    \n",
    "    IF oob_score > best_score:\n",
    "        best_score = oob_score\n",
    "        best_params = param_combination\n",
    "\n",
    "final_model = RandomForest(**best_params)\n",
    "```\n",
    "\n",
    "**Feature Selection via OOB:**\n",
    "```\n",
    "ALGORITHM: OOB-Based Feature Selection  \n",
    "current_features = ALL_FEATURES\n",
    "best_oob_score = INITIAL_OOB_SCORE(current_features)\n",
    "\n",
    "WHILE True:\n",
    "    feature_importance = OOB_PERMUTATION_IMPORTANCE(current_features)\n",
    "    \n",
    "    # Remove least important feature\n",
    "    least_important = ARGMIN(feature_importance)\n",
    "    candidate_features = current_features - {least_important}\n",
    "    \n",
    "    candidate_oob_score = OOB_SCORE(candidate_features)\n",
    "    \n",
    "    IF candidate_oob_score >= best_oob_score - tolerance:\n",
    "        current_features = candidate_features\n",
    "        best_oob_score = candidate_oob_score\n",
    "    ELSE:\n",
    "        BREAK  # No improvement, stop selection\n",
    "\n",
    "RETURN current_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Uncertainty Quantification\n",
    "\n",
    "### Theoretical Framework\n",
    "\n",
    "**Sources of Uncertainty in Bagging:**\n",
    "1. **Epistemic (Model) Uncertainty:** Due to limited training data\n",
    "2. **Aleatoric (Data) Uncertainty:** Inherent noise in observations\n",
    "3. **Ensemble Disagreement:** Variation across ensemble members\n",
    "\n",
    "### Bootstrap Confidence Intervals\n",
    "\n",
    "#### 1. Percentile Method\n",
    "For prediction $\\hat{y}(x)$, collect bootstrap predictions:\n",
    "\n",
    "$\\{\\hat{y}^{(1)}(x), \\hat{y}^{(2)}(x), \\ldots, \\hat{y}^{(M)}(x)\\}$\n",
    "\n",
    "**Confidence Interval:**\n",
    "$CI_{1-\\alpha}(x) = [Q_{\\alpha/2}(\\{\\hat{y}^{(m)}(x)\\}), Q_{1-\\alpha/2}(\\{\\hat{y}^{(m)}(x)\\})]$\n",
    "\n",
    "```\n",
    "ALGORITHM: Bootstrap Percentile CI\n",
    "INPUT: Test point x, ensemble {h_m}, confidence level α\n",
    "\n",
    "predictions = [h_m(x) for m in 1..M]\n",
    "lower_bound = QUANTILE(predictions, α/2)\n",
    "upper_bound = QUANTILE(predictions, 1-α/2) \n",
    "\n",
    "RETURN [lower_bound, upper_bound]\n",
    "```\n",
    "\n",
    "#### 2. Bias-Corrected and Accelerated (BCa) Bootstrap\n",
    "Adjusts for bias and skewness in the bootstrap distribution:\n",
    "\n",
    "$CI_{BCa} = [Q_{\\alpha_1}, Q_{\\alpha_2}]$\n",
    "\n",
    "where:\n",
    "$\\alpha_1 = \\Phi\\left(\\hat{z}_0 + \\frac{\\hat{z}_0 + z_{\\alpha/2}}{1 - \\hat{a}(\\hat{z}_0 + z_{\\alpha/2})}\\right)$\n",
    "\n",
    "$\\alpha_2 = \\Phi\\left(\\hat{z}_0 + \\frac{\\hat{z}_0 + z_{1-\\alpha/2}}{1 - \\hat{a}(\\hat{z}_0 + z_{1-\\alpha/2})}\\right)$\n",
    "\n",
    "**Bias Correction Factor:**\n",
    "$\\hat{z}_0 = \\Phi^{-1}\\left(\\frac{\\#\\{\\hat{y}^{(m)}(x) < \\hat{y}(x)\\}}{M}\\right)$\n",
    "\n",
    "**Acceleration Parameter:**\n",
    "$\\hat{a} = \\frac{\\sum_{i}(\\bar{u} - u_i)^3}{6[\\sum_{i}(\\bar{u} - u_i)^2]^{3/2}}$\n",
    "\n",
    "where $u_i$ are jackknife estimates.\n",
    "\n",
    "### Infinitesimal Jackknife (IJ)\n",
    "\n",
    "**Theoretical Foundation:**\n",
    "For each training sample $i$, compute its influence on the final prediction:\n",
    "\n",
    "$\\text{IJ}_i(x) = \\frac{\\partial \\hat{y}(x)}{\\partial w_i}$\n",
    "\n",
    "where $w_i$ is the weight of sample $i$.\n",
    "\n",
    "**Variance Estimation:**\n",
    "$\\hat{\\text{Var}}[\\hat{y}(x)] = \\sum_{i=1}^n \\text{IJ}_i(x)^2$\n",
    "\n",
    "```\n",
    "ALGORITHM: Infinitesimal Jackknife for Random Forest\n",
    "INPUT: Random Forest ensemble, test point x\n",
    "\n",
    "total_variance = 0\n",
    "\n",
    "FOR each training sample i:\n",
    "    influence_i = 0\n",
    "    \n",
    "    FOR each tree m:\n",
    "        IF i was in bootstrap sample m:\n",
    "            # Compute influence of sample i on tree m's prediction\n",
    "            influence_m = COMPUTE_TREE_INFLUENCE(tree_m, sample_i, x)\n",
    "            influence_i += influence_m / M\n",
    "    \n",
    "    total_variance += influence_i^2\n",
    "\n",
    "prediction_std = SQRT(total_variance)\n",
    "RETURN prediction_std\n",
    "```\n",
    "\n",
    "### Quantile Regression Forests\n",
    "\n",
    "**Core Principle:** Instead of predicting point estimates, maintain full conditional distributions at leaves.\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "ALGORITHM: Quantile Regression Forest Training\n",
    "INPUT: Training data D, quantiles Q = {q_1, q_2, ..., q_k}\n",
    "\n",
    "FOR each tree m:\n",
    "    D_m = BOOTSTRAP_SAMPLE(D)\n",
    "    tree_m = BUILD_TREE(D_m)\n",
    "    \n",
    "    FOR each leaf L in tree_m:\n",
    "        # Store all target values that reach this leaf\n",
    "        leaf_targets[L] = [y_i for (x_i, y_i) in D_m if REACHES_LEAF(x_i, L)]\n",
    "\n",
    "FUNCTION PREDICT_QUANTILES(x, quantiles):\n",
    "    all_targets = []\n",
    "    \n",
    "    FOR each tree m:\n",
    "        leaf = FIND_LEAF(tree_m, x)\n",
    "        all_targets.EXTEND(leaf_targets[leaf])\n",
    "    \n",
    "    # Return empirical quantiles\n",
    "    RETURN [QUANTILE(all_targets, q) for q in quantiles]\n",
    "```\n",
    "\n",
    "**Prediction Intervals:**\n",
    "For confidence level $1-\\alpha$:\n",
    "\n",
    "$PI_{1-\\alpha}(x) = [Q_{\\alpha/2}(Y|X=x), Q_{1-\\alpha/2}(Y|X=x)]$\n",
    "\n",
    "### Conformal Prediction with Random Forests\n",
    "\n",
    "**Split Conformal Framework:**\n",
    "1. Split data: $\\mathcal{D} = \\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{cal}}$\n",
    "2. Train ensemble on $\\mathcal{D}_{\\text{train}}$\n",
    "3. Compute nonconformity scores on $\\mathcal{D}_{\\text{cal}}$\n",
    "4. Use quantiles of scores for prediction intervals\n",
    "\n",
    "```\n",
    "ALGORITHM: Conformal Prediction Intervals\n",
    "INPUT: Trained RF, calibration set D_cal, confidence α\n",
    "\n",
    "# Step 1: Compute nonconformity scores\n",
    "nonconformity_scores = []\n",
    "FOR (x_i, y_i) in D_cal:\n",
    "    pred_i = RF.PREDICT(x_i)\n",
    "    score_i = ABS(y_i - pred_i)  # Or other score function\n",
    "    nonconformity_scores.ADD(score_i)\n",
    "\n",
    "# Step 2: Find quantile threshold\n",
    "n_cal = LENGTH(D_cal)\n",
    "q_level = (n_cal + 1) * (1 - α) / n_cal\n",
    "threshold = QUANTILE(nonconformity_scores, q_level)\n",
    "\n",
    "# Step 3: Prediction intervals for new points\n",
    "FUNCTION PREDICT_INTERVAL(x_new):\n",
    "    pred = RF.PREDICT(x_new)\n",
    "    RETURN [pred - threshold, pred + threshold]\n",
    "```\n",
    "\n",
    "**Adaptive Conformal Prediction:**\n",
    "Adjust interval width based on local prediction difficulty:\n",
    "\n",
    "```\n",
    "ALGORITHM: Locally Adaptive Conformal Prediction\n",
    "# Use ensemble disagreement as difficulty measure\n",
    "FOR (x_i, y_i) in D_cal:\n",
    "    individual_preds = [tree.PREDICT(x_i) for tree in RF.trees]\n",
    "    disagreement_i = STANDARD_DEVIATION(individual_preds)\n",
    "    \n",
    "    # Normalized nonconformity score\n",
    "    score_i = ABS(y_i - MEAN(individual_preds)) / (disagreement_i + ε)\n",
    "    adaptive_scores.ADD(score_i)\n",
    "\n",
    "# Use adaptive scores for threshold\n",
    "adaptive_threshold = QUANTILE(adaptive_scores, q_level)\n",
    "\n",
    "FUNCTION ADAPTIVE_PREDICT_INTERVAL(x_new):\n",
    "    individual_preds = [tree.PREDICT(x_new) for tree in RF.trees]\n",
    "    pred_mean = MEAN(individual_preds)\n",
    "    disagreement = STANDARD_DEVIATION(individual_preds)\n",
    "    \n",
    "    interval_width = adaptive_threshold * (disagreement + ε)\n",
    "    RETURN [pred_mean - interval_width, pred_mean + interval_width]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Importance Methods\n",
    "\n",
    "### Split-Based Importance (Mean Decrease Impurity)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "For feature $j$, the importance is:\n",
    "\n",
    "$I_j = \\frac{1}{M} \\sum_{m=1}^M \\sum_{t \\in T_m: v(t)=j} p(t) \\cdot \\Delta I(t)$\n",
    "\n",
    "where:\n",
    "- $v(t)$ is the feature used at node $t$\n",
    "- $p(t) = \\frac{N_t}{N}$ is the fraction of samples reaching node $t$\n",
    "- $\\Delta I(t)$ is the impurity decrease from splitting node $t$\n",
    "\n",
    "```\n",
    "ALGORITHM: MDI Feature Importance\n",
    "INPUT: Random Forest ensemble\n",
    "\n",
    "importance = ZEROS(n_features)\n",
    "\n",
    "FOR each tree T in ensemble:\n",
    "    FOR each internal node t in T:\n",
    "        feature_j = t.split_feature\n",
    "        n_samples_t = t.n_samples\n",
    "        impurity_decrease = t.impurity_decrease\n",
    "        \n",
    "        # Weight by fraction of samples\n",
    "        weight = n_samples_t / total_training_samples\n",
    "        importance[feature_j] += weight * impurity_decrease\n",
    "\n",
    "# Normalize to sum to 1\n",
    "importance = importance / SUM(importance)\n",
    "RETURN importance\n",
    "```\n",
    "\n",
    "**Limitations:**\n",
    "- Biased toward high-cardinality categorical features\n",
    "- Doesn't account for feature interactions\n",
    "- Can assign importance to irrelevant correlated features\n",
    "\n",
    "### Permutation Importance\n",
    "\n",
    "**Core Principle:** Measure performance degradation when feature values are randomly shuffled.\n",
    "\n",
    "```\n",
    "ALGORITHM: Permutation Importance (OOB)\n",
    "INPUT: Trained Random Forest, OOB samples, metric, n_repeats\n",
    "\n",
    "baseline_score = OOB_SCORE(RF)\n",
    "importances = ZEROS(n_features)\n",
    "\n",
    "FOR feature_j in 1..n_features:\n",
    "    scores = []\n",
    "    \n",
    "    FOR repeat in 1..n_repeats:\n",
    "        # Create permuted OOB dataset\n",
    "        X_oob_permuted = COPY(X_oob)\n",
    "        X_oob_permuted[:, feature_j] = SHUFFLE(X_oob[:, feature_j])\n",
    "        \n",
    "        # Compute score with permuted feature\n",
    "        permuted_score = SCORE(RF, X_oob_permuted, y_oob, metric)\n",
    "        scores.ADD(permuted_score)\n",
    "    \n",
    "    # Importance = average performance drop\n",
    "    importances[feature_j] = baseline_score - MEAN(scores)\n",
    "\n",
    "RETURN importances\n",
    "```\n",
    "\n",
    "**Advanced Permutation Strategies:**\n",
    "\n",
    "#### 1. Conditional Permutation\n",
    "Preserves feature correlations:\n",
    "\n",
    "```\n",
    "ALGORITHM: Conditional Permutation Importance\n",
    "INPUT: Feature j to permute, conditioning features C\n",
    "\n",
    "FOR each unique combination of values in features C:\n",
    "    subset_indices = INDICES_WHERE(X[:, C] == combination)\n",
    "    \n",
    "    # Permute only within this subset\n",
    "    X_permuted[subset_indices, j] = SHUFFLE(X[subset_indices, j])\n",
    "```\n",
    "\n",
    "#### 2. Group Permutation\n",
    "For correlated feature groups:\n",
    "\n",
    "```\n",
    "ALGORITHM: Group Permutation Importance\n",
    "INPUT: Feature groups G = {G_1, G_2, ..., G_k}\n",
    "\n",
    "FOR each group G_i:\n",
    "    # Permute all features in group together\n",
    "    permutation_order = SHUFFLE(RANGE(n_samples))\n",
    "    FOR feature_j in G_i:\n",
    "        X_permuted[:, feature_j] = X[permutation_order, feature_j]\n",
    "    \n",
    "    group_importance[G_i] = baseline_score - SCORE(X_permuted, y)\n",
    "```\n",
    "\n",
    "### SHAP Values for Tree Ensembles\n",
    "\n",
    "**TreeSHAP Algorithm:**\n",
    "Efficiently computes SHAP values for tree ensembles using:\n",
    "\n",
    "$\\phi_j = \\sum_{S \\subseteq \\mathcal{F} \\setminus \\{j\\}} \\frac{|S|!(|\\mathcal{F}|-|S|-1)!}{|\\mathcal{F}|!} [f_x(S \\cup \\{j\\}) - f_x(S)]$\n",
    "\n",
    "```\n",
    "ALGORITHM: TreeSHAP for Random Forest\n",
    "INPUT: Tree ensemble, instance x, background dataset\n",
    "\n",
    "shap_values = ZEROS(n_features)\n",
    "\n",
    "FOR each tree T in ensemble:\n",
    "    tree_shap = COMPUTE_TREE_SHAP(T, x, background)\n",
    "    shap_values += tree_shap / n_trees\n",
    "\n",
    "FUNCTION COMPUTE_TREE_SHAP(tree, x, background):\n",
    "    # Recursive computation using tree structure\n",
    "    RETURN RECURSIVE_SHAP(tree.root, x, background, path=[], \n",
    "                         zero_fraction=1.0, one_fraction=1.0)\n",
    "```\n",
    "\n",
    "**SHAP Interaction Values:**\n",
    "Capture pairwise feature interactions:\n",
    "\n",
    "$\\phi_{i,j} = \\sum_{S \\subseteq \\mathcal{F} \\setminus \\{i,j\\}} \\frac{|S|!(|\\mathcal{F}|-|S|-2)!}{2(|\\mathcal{F}|-1)!} \\Delta_{ij}(S)$\n",
    "\n",
    "where $\\Delta_{ij}(S) = f(S \\cup \\{i,j\\}) - f(S \\cup \\{i\\}) - f(S \\cup \\{j\\}) + f(S)$\n",
    "\n",
    "### Feature Selection Based on Importance\n",
    "\n",
    "**Recursive Feature Elimination with Cross-Validation:**\n",
    "```\n",
    "ALGORITHM: RFE-CV with Random Forest\n",
    "INPUT: Features F, target y, cv_folds, min_features\n",
    "\n",
    "feature_ranking = []\n",
    "current_features = F\n",
    "\n",
    "WHILE LENGTH(current_features) > min_features:\n",
    "    # Train RF and compute importance\n",
    "    rf = TRAIN_RF(current_features, y)\n",
    "    importance = COMPUTE_IMPORTANCE(rf)\n",
    "    \n",
    "    # Cross-validation score with current features\n",
    "    cv_score = CROSS_VAL_SCORE(rf, current_features, y, cv_folds)\n",
    "    feature_ranking.ADD((current_features, cv_score))\n",
    "    \n",
    "    # Remove least important feature\n",
    "    least_important = ARGMIN(importance)\n",
    "    current_features.REMOVE(least_important)\n",
    "\n",
    "# Select feature set with best CV score\n",
    "best_features = ARGMAX(feature_ranking, key=lambda x: x[1])\n",
    "RETURN best_features\n",
    "```\n",
    "\n",
    "**Stability-Based Selection:**\n",
    "```\n",
    "ALGORITHM: Stability-Based Feature Selection\n",
    "INPUT: Dataset D, bootstrap_samples B, stability_threshold\n",
    "\n",
    "stability_scores = ZEROS(n_features)\n",
    "\n",
    "FOR b in 1..B:\n",
    "    D_boot = BOOTSTRAP_SAMPLE(D)\n",
    "    rf_boot = TRAIN_RF(D_boot)\n",
    "    importance_boot = COMPUTE_IMPORTANCE(rf_boot)\n",
    "    \n",
    "    # Binarize importance (top-k selection)\n",
    "    top_k_features = TOP_K_INDICES(importance_boot, k)\n",
    "    binary_selection = ZEROS(n_features)\n",
    "    binary_selection[top_k_features] = 1\n",
    "    \n",
    "    stability_scores += binary_selection\n",
    "\n",
    "# Stability = frequency of selection across bootstraps\n",
    "stability_scores /= B\n",
    "stable_features = INDICES_WHERE(stability_scores >= stability_threshold)\n",
    "\n",
    "RETURN stable_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Special Cases\n",
    "\n",
    "### Imbalanced Classification\n",
    "\n",
    "#### 1. Balanced Random Forest\n",
    "```\n",
    "ALGORITHM: Balanced Random Forest\n",
    "INPUT: Imbalanced dataset D, minority_class_ratio\n",
    "\n",
    "FOR each tree m:\n",
    "    # Balanced bootstrap sampling\n",
    "    D_m = BALANCED_BOOTSTRAP_SAMPLE(D, minority_class_ratio)\n",
    "    tree_m = TRAIN_TREE(D_m)\n",
    "    ensemble.ADD(tree_m)\n",
    "\n",
    "FUNCTION BALANCED_BOOTSTRAP_SAMPLE(D, target_ratio):\n",
    "    minority_samples = D[y == minority_class]\n",
    "    majority_samples = D[y == majority_class]\n",
    "    \n",
    "    n_minority = LENGTH(minority_samples)\n",
    "    n_majority_target = n_minority / target_ratio\n",
    "    \n",
    "    # Oversample minority, undersample majority\n",
    "    D_minority_boot = BOOTSTRAP_SAMPLE(minority_samples, n_minority)\n",
    "    D_majority_boot = SAMPLE_WITHOUT_REPLACEMENT(majority_samples, n_majority_target)\n",
    "    \n",
    "    RETURN CONCATENATE(D_minority_boot, D_majority_boot)\n",
    "```\n",
    "\n",
    "#### 2. Cost-Sensitive Splitting\n",
    "Modify impurity measures to account for misclassification costs:\n",
    "\n",
    "```\n",
    "ALGORITHM: Cost-Sensitive Gini Impurity\n",
    "INPUT: Node samples, cost matrix C\n",
    "\n",
    "weighted_gini = 0\n",
    "total_cost_weighted_samples = 0\n",
    "\n",
    "FOR each class i:\n",
    "    n_i = COUNT(samples, class=i)\n",
    "    cost_weight_i = SUM(C[i, :])  # Cost of misclassifying class i\n",
    "    \n",
    "    weighted_gini += cost_weight_i * n_i * (N - n_i) / N\n",
    "    total_cost_weighted_samples += cost_weight_i * n_i\n",
    "\n",
    "RETURN weighted_gini / total_cost_weighted_samples\n",
    "```\n",
    "\n",
    "#### 3. Threshold Optimization\n",
    "Use OOB predictions to find optimal classification threshold:\n",
    "\n",
    "```\n",
    "ALGORITHM: OOB Threshold Optimization\n",
    "INPUT: RF with OOB predictions, optimization metric\n",
    "\n",
    "oob_probabilities = RF.OOB_PREDICT_PROBA()\n",
    "true_labels = y[oob_indices]\n",
    "\n",
    "# Grid search over thresholds\n",
    "thresholds = LINSPACE(0.1, 0.9, 50)\n",
    "best_threshold = 0.5\n",
    "best_score = -inf\n",
    "\n",
    "FOR threshold in thresholds:\n",
    "    predictions = oob_probabilities[:, 1] >= threshold\n",
    "    score = CALCULATE_METRIC(true_labels, predictions)\n",
    "    \n",
    "    IF score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "RF.decision_threshold = best_threshold\n",
    "```\n",
    "\n",
    "### Multi-Output Problems\n",
    "\n",
    "#### 1. Multi-Output Regression\n",
    "```\n",
    "ALGORITHM: Multi-Output Random Forest\n",
    "INPUT: X, Y where Y has k targets\n",
    "\n",
    "FOR each tree m:\n",
    "    D_m = BOOTSTRAP_SAMPLE(X, Y)\n",
    "    tree_m = BUILD_MULTI_OUTPUT_TREE(D_m)\n",
    "\n",
    "FUNCTION BUILD_MULTI_OUTPUT_TREE(D):\n",
    "    # Use vector impurity measures\n",
    "    FUNCTION VECTOR_MSE_IMPURITY(node):\n",
    "        predictions = MEAN(node.targets, axis=0)  # k-dimensional mean\n",
    "        mse = MEAN(SUM((node.targets - predictions)^2, axis=1))\n",
    "        RETURN mse\n",
    "    \n",
    "    # Split selection remains the same\n",
    "    best_split = FIND_BEST_SPLIT(node, VECTOR_MSE_IMPURITY)\n",
    "```\n",
    "\n",
    "#### 2. Multi-Label Classification\n",
    "```\n",
    "ALGORITHM: Multi-Label Random Forest\n",
    "approaches = [\n",
    "    \"binary_relevance\",    # Independent RF for each label\n",
    "    \"label_powerset\",      # Treat unique label combinations as classes  \n",
    "    \"classifier_chains\"    # Chain predictions (label dependencies)\n",
    "]\n",
    "\n",
    "FUNCTION BINARY_RELEVANCE(X, Y):\n",
    "    forests = {}\n",
    "    FOR each label l:\n",
    "        forests[l] = TRAIN_RF(X, Y[:, l])\n",
    "    \n",
    "    FUNCTION PREDICT(x):\n",
    "        predictions = {}\n",
    "        FOR each label l:\n",
    "            predictions[l] = forests[l].PREDICT_PROBA(x)[1]\n",
    "        RETURN predictions\n",
    "\n",
    "FUNCTION CLASSIFIER_CHAINS(X, Y):\n",
    "    label_order = RANDOM_PERMUTATION(n_labels)\n",
    "    chain_forests = {}\n",
    "    \n",
    "    FOR i, label_l in ENUMERATE(label_order):\n",
    "        # Features = original + predictions of previous labels in chain\n",
    "        if i == 0:\n",
    "            X_augmented = X\n",
    "        else:\n",
    "            prev_predictions = [chain_forests[prev_l].PREDICT(X) \n",
    "                              for prev_l in label_order[:i]]\n",
    "            X_augmented = CONCATENATE(X, prev_predictions)\n",
    "        \n",
    "        chain_forests[label_l] = TRAIN_RF(X_augmented, Y[:, label_l])\n",
    "```\n",
    "\n",
    "### Missing Value Handling\n",
    "\n",
    "#### 1. Surrogate Splits\n",
    "When primary split feature is missing, use correlated features:\n",
    "\n",
    "```\n",
    "ALGORITHM: Surrogate Split Selection\n",
    "INPUT: Primary split (feature j, threshold θ), node samples\n",
    "\n",
    "# Find best surrogate splits\n",
    "surrogates = []\n",
    "primary_direction = [LEFT if x[i,j] <= θ else RIGHT for i in node_samples]\n",
    "\n",
    "FOR each other feature k != j:\n",
    "    FOR each threshold φ:\n",
    "        surrogate_direction = [LEFT if x[i,k] <= φ else RIGHT for i in node_samples]\n",
    "        agreement = PROPORTION_AGREEMENT(primary_direction, surrogate_direction)\n",
    "        \n",
    "        IF agreement > min_surrogate_agreement:\n",
    "            surrogates.ADD((feature=k, threshold=φ, agreement=agreement))\n",
    "\n",
    "# Sort by agreement, keep top surrogates\n",
    "surrogates = SORT_BY(surrogates, agreement, descending=True)\n",
    "RETURN surrogates[:max_surrogates]\n",
    "```\n",
    "\n",
    "#### 2. Probabilistic Routing\n",
    "Route missing values probabilistically based on training data:\n",
    "\n",
    "```\n",
    "ALGORITHM: Probabilistic Missing Value Routing\n",
    "INPUT: Node with missing values for split feature\n",
    "\n",
    "training_samples_at_node = node.training_samples\n",
    "left_probability = COUNT(training_samples_at_node, went_left) / COUNT(training_samples_at_node)\n",
    "right_probability = 1 - left_probability\n",
    "\n",
    "FUNCTION PREDICT_WITH_MISSING(x):\n",
    "    IF x[split_feature] is not MISSING:\n",
    "        RETURN STANDARD_ROUTING(x)\n",
    "    ELSE:\n",
    "        # Probabilistic prediction\n",
    "        left_pred = left_probability * LEFT_SUBTREE.PREDICT(x)\n",
    "        right_pred = right_probability * RIGHT_SUBTREE.PREDICT(x)\n",
    "        RETURN left_pred + right_pred\n",
    "```\n",
    "\n",
    "### High-Dimensional Data\n",
    "\n",
    "#### 1. Feature Hashing\n",
    "For extremely high-dimensional sparse features:\n",
    "\n",
    "```\n",
    "ALGORITHM: Feature Hashing for RF\n",
    "INPUT: Sparse feature matrix X, hash_size H\n",
    "\n",
    "X_hashed = ZEROS(n_samples, H)\n",
    "\n",
    "FOR sample i:\n",
    "    FOR each nonzero feature (j, value):\n",
    "        hash_index = HASH(j) % H\n",
    "        X_hashed[i, hash_index] += value  # Handle collisions by addition\n",
    "\n",
    "rf = TRAIN_RF(X_hashed, y)\n",
    "```\n",
    "\n",
    "#### 2. Random Projections\n",
    "Dimensionality reduction while preserving distances:\n",
    "\n",
    "```\n",
    "ALGORITHM: Random Projection RF\n",
    "INPUT: High-dimensional X, target dimensionality d_target\n",
    "\n",
    "# Generate random projection matrix\n",
    "R = GAUSSIAN_RANDOM_MATRIX(d_original, d_target)\n",
    "R = R / NORM(R, axis=0)  # Normalize columns\n",
    "\n",
    "X_projected = X @ R\n",
    "rf = TRAIN_RF(X_projected, y)\n",
    "\n",
    "FUNCTION PREDICT(x_new):\n",
    "    x_projected = x_new @ R\n",
    "    RETURN rf.PREDICT(x_projected)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Optimization\n",
    "\n",
    "### Memory Optimization\n",
    "\n",
    "#### 1. Compact Tree Representation\n",
    "```\n",
    "ALGORITHM: Memory-Efficient Tree Storage\n",
    "# Instead of storing full tree objects, use arrays\n",
    "\n",
    "# Tree structure arrays (for tree t)\n",
    "left_children[t] = array of left child indices\n",
    "right_children[t] = array of right child indices  \n",
    "split_features[t] = array of split feature indices\n",
    "split_thresholds[t] = array of split threshold values\n",
    "leaf_values[t] = array of leaf prediction values\n",
    "is_leaf[t] = boolean array indicating leaf nodes\n",
    "\n",
    "FUNCTION PREDICT_COMPACT(x, tree_index):\n",
    "    node = 0  # Start at root\n",
    "    \n",
    "    WHILE not is_leaf[tree_index][node]:\n",
    "        feature = split_features[tree_index][node]\n",
    "        threshold = split_thresholds[tree_index][node]\n",
    "        \n",
    "        IF x[feature] <= threshold:\n",
    "            node = left_children[tree_index][node]\n",
    "        ELSE:\n",
    "            node = right_children[tree_index][node]\n",
    "    \n",
    "    RETURN leaf_values[tree_index][node]\n",
    "```\n",
    "\n",
    "#### 2. Feature Binning and Quantization\n",
    "```\n",
    "ALGORITHM: Memory-Efficient Feature Storage\n",
    "# Pre-bin continuous features\n",
    "FOR each feature j:\n",
    "    IF IS_CONTINUOUS(feature_j):\n",
    "        bins_j = QUANTILE_BINS(X[:, j], n_bins=255)\n",
    "        X_binned[:, j] = DIGITIZE(X[:, j], bins_j)\n",
    "        \n",
    "        # Store bins with 8-bit integers instead of 64-bit floats\n",
    "        X_quantized[:, j] = X_binned[:, j].astype(uint8)\n",
    "```\n",
    "\n",
    "### Computational Optimization\n",
    "\n",
    "#### 1. Vectorized Prediction\n",
    "```\n",
    "ALGORITHM: Batch Prediction Optimization\n",
    "INPUT: Test batch X_test, ensemble\n",
    "\n",
    "# Vectorized tree traversal\n",
    "predictions = ZEROS(n_test_samples, n_trees)\n",
    "\n",
    "FOR tree_idx in 1..n_trees:\n",
    "    node_indices = ZEROS(n_test_samples, dtype=int)  # Start at root (0) for all\n",
    "    \n",
    "    # Traverse all samples simultaneously\n",
    "    WHILE any(not is_leaf[tree_idx][node_indices]):\n",
    "        active_mask = not is_leaf[tree_idx][node_indices]\n",
    "        \n",
    "        features = split_features[tree_idx][node_indices[active_mask]]\n",
    "        thresholds = split_thresholds[tree_idx][node_indices[active_mask]]\n",
    "        \n",
    "        # Vectorized comparison\n",
    "        go_left = X_test[active_mask, features] <= thresholds\n",
    "        \n",
    "        # Update node indices\n",
    "        node_indices[active_mask & go_left] = left_children[tree_idx][node_indices[active_mask & go_left]]\n",
    "        node_indices[active_mask & ~go_left] = right_children[tree_idx][node_indices[active_mask & ~go_left]]\n",
    "    \n",
    "    # Collect leaf predictions\n",
    "    predictions[:, tree_idx] = leaf_values[tree_idx][node_indices]\n",
    "\n",
    "# Final ensemble prediction\n",
    "RETURN MEAN(predictions, axis=1)\n",
    "```\n",
    "\n",
    "#### 2. Parallel Tree Construction\n",
    "```\n",
    "ALGORITHM: Parallel Random Forest Training\n",
    "INPUT: Data D, n_trees M, n_threads T\n",
    "\n",
    "# Divide trees across threads\n",
    "trees_per_thread = M // T\n",
    "thread_assignments = [RANGE(i*trees_per_thread, (i+1)*trees_per_thread) \n",
    "                     for i in RANGE(T)]\n",
    "\n",
    "PARALLEL FOR thread_id in 1..T:\n",
    "    local_trees = []\n",
    "    \n",
    "    FOR tree_idx in thread_assignments[thread_id]:\n",
    "        bootstrap_sample = BOOTSTRAP_SAMPLE(D)\n",
    "        tree = BUILD_TREE(bootstrap_sample)\n",
    "        local_trees.ADD(tree)\n",
    "    \n",
    "    thread_results[thread_id] = local_trees\n",
    "\n",
    "# Combine results\n",
    "ensemble = CONCATENATE([thread_results[i] for i in 1..T])\n",
    "```\n",
    "\n",
    "#### 3. NUMA-Aware Processing\n",
    "```\n",
    "ALGORITHM: NUMA-Aware Random Forest\n",
    "# Bind threads to NUMA nodes for better memory locality\n",
    "\n",
    "FUNCTION NUMA_AWARE_TRAINING():\n",
    "    numa_nodes = GET_NUMA_TOPOLOGY()\n",
    "    \n",
    "    FOR numa_node in numa_nodes:\n",
    "        # Bind threads to this NUMA node\n",
    "        SET_THREAD_AFFINITY(numa_node.cpu_cores)\n",
    "        \n",
    "        # Allocate data on local memory\n",
    "        local_data = COPY_TO_NUMA_NODE(training_data, numa_node)\n",
    "        \n",
    "        # Train subset of trees\n",
    "        trees_for_node = n_trees // n_numa_nodes\n",
    "        local_ensemble = TRAIN_TREES(local_data, trees_for_node)\n",
    "        \n",
    "        global_ensemble.EXTEND(local_ensemble)\n",
    "```\n",
    "\n",
    "### Scalability Optimizations\n",
    "\n",
    "#### 1. Online/Streaming Random Forest\n",
    "```\n",
    "ALGORITHM: Online Random Forest\n",
    "STATE: Ensemble of online trees {T_1, T_2, ..., T_M}\n",
    "\n",
    "FUNCTION UPDATE(new_sample):\n",
    "    FOR each tree T_m:\n",
    "        # Each tree sees sample with Poisson(λ=1) frequency\n",
    "        k = POISSON_RANDOM(1)\n",
    "        FOR _ in 1..k:\n",
    "            T_m.UPDATE(new_sample)\n",
    "\n",
    "FUNCTION ONLINE_TREE_UPDATE(tree, sample):\n",
    "    # Traverse to leaf\n",
    "    leaf = FIND_LEAF(tree, sample.x)\n",
    "    \n",
    "    # Update leaf statistics\n",
    "    leaf.sum_targets += sample.y\n",
    "    leaf.sum_squared_targets += sample.y^2\n",
    "    leaf.n_samples += 1\n",
    "    \n",
    "    # Potentially split leaf if it has grown large enough\n",
    "    IF leaf.n_samples > min_samples_to_split:\n",
    "        ATTEMPT_SPLIT(leaf)\n",
    "```\n",
    "\n",
    "#### 2. Distributed Random Forest\n",
    "```\n",
    "ALGORITHM: Map-Reduce Random Forest\n",
    "FUNCTION MAP_PHASE(data_partition):\n",
    "    # Each worker trains subset of trees\n",
    "    local_trees = []\n",
    "    FOR i in 1..trees_per_worker:\n",
    "        bootstrap_sample = BOOTSTRAP_SAMPLE(data_partition)\n",
    "        tree = BUILD_TREE(bootstrap_sample)\n",
    "        local_trees.ADD(tree)\n",
    "    \n",
    "    RETURN local_trees\n",
    "\n",
    "FUNCTION REDUCE_PHASE(all_tree_collections):\n",
    "    # Combine trees from all workers\n",
    "    global_ensemble = []\n",
    "    FOR tree_collection in all_tree_collections:\n",
    "        global_ensemble.EXTEND(tree_collection)\n",
    "    \n",
    "    RETURN RandomForestEnsemble(global_ensemble)\n",
    "\n",
    "# Usage\n",
    "partitioned_data = PARTITION_DATA(training_data, n_workers)\n",
    "tree_collections = PARALLEL_MAP(MAP_PHASE, partitioned_data)\n",
    "final_model = REDUCE_PHASE(tree_collections)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Topics\n",
    "\n",
    "### Mondrian Forests\n",
    "\n",
    "**Time-Structured Splits:** Instead of axis-aligned splits, use \"Mondrian\" cuts that respect temporal structure.\n",
    "\n",
    "```\n",
    "ALGORITHM: Mondrian Forest\n",
    "INPUT: Spatio-temporal data with timestamps\n",
    "\n",
    "FUNCTION MONDRIAN_SPLIT(node_data):\n",
    "    # Sample split time exponentially\n",
    "    τ = node.creation_time + EXPONENTIAL_RANDOM(rate=1)\n",
    "    \n",
    "    # Choose dimension proportional to box extent\n",
    "    extents = [MAX(node_data[:, j]) - MIN(node_data[:, j]) for j in features]\n",
    "    total_extent = SUM(extents)\n",
    "    split_dim = CATEGORICAL_SAMPLE(features, weights=extents/total_extent)\n",
    "    \n",
    "    # Uniform split location within dimension\n",
    "    min_val, max_val = MIN_MAX(node_data[:, split_dim])\n",
    "    split_location = UNIFORM(min_val, max_val)\n",
    "    \n",
    "    RETURN Mondriansplit(time=τ, dimension=split_dim, location=split_location)\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Online learning with temporal consistency\n",
    "- Spatial data with geographical structure\n",
    "- Streaming data with concept drift\n",
    "\n",
    "### Isolation Forests\n",
    "\n",
    "**Anomaly Detection via Random Partitioning:**\n",
    "\n",
    "```\n",
    "ALGORITHM: Isolation Forest\n",
    "INPUT: Dataset X (no labels), contamination_rate\n",
    "\n",
    "FOR tree in 1..n_trees:\n",
    "    # Build isolation tree\n",
    "    tree = BUILD_ISOLATION_TREE(X, max_depth=ceil(log2(n_samples)))\n",
    "\n",
    "FUNCTION BUILD_ISOLATION_TREE(data, current_depth):\n",
    "    IF current_depth >= max_depth OR |data| <= 1:\n",
    "        RETURN LEAF(size=|data|)\n",
    "    \n",
    "    # Random split\n",
    "    split_feature = RANDOM_CHOICE(features)\n",
    "    min_val, max_val = MIN_MAX(data[:, split_feature])\n",
    "    split_point = UNIFORM(min_val, max_val)\n",
    "    \n",
    "    left_data = data[data[:, split_feature] < split_point]\n",
    "    right_data = data[data[:, split_feature] >= split_point]\n",
    "    \n",
    "    left_child = BUILD_ISOLATION_TREE(left_data, current_depth + 1)\n",
    "    right_child = BUILD_ISOLATION_TREE(right_data, current_depth + 1)\n",
    "    \n",
    "    RETURN INTERNAL_NODE(split_feature, split_point, left_child, right_child)\n",
    "\n",
    "FUNCTION ANOMALY_SCORE(x):\n",
    "    path_lengths = []\n",
    "    FOR tree in isolation_forest:\n",
    "        path_length = COMPUTE_PATH_LENGTH(tree, x)\n",
    "        path_lengths.ADD(path_length)\n",
    "    \n",
    "    average_path_length = MEAN(path_lengths)\n",
    "    # Normalize by expected path length in BST\n",
    "    c_n = 2 * (ln(n_samples - 1) + 0.5772156649) - (2 * (n_samples - 1) / n_samples)\n",
    "    anomaly_score = 2^(-average_path_length / c_n)\n",
    "    \n",
    "    RETURN anomaly_score\n",
    "```\n",
    "\n",
    "**Anomaly Threshold Selection:**\n",
    "```\n",
    "# Anomaly scores close to 1 indicate anomalies\n",
    "# Scores close to 0 indicate normal points\n",
    "contamination_threshold = QUANTILE(all_scores, 1 - contamination_rate)\n",
    "anomalies = INDICES_WHERE(scores > contamination_threshold)\n",
    "```\n",
    "\n",
    "### Random Survival Forests\n",
    "\n",
    "**Extension to Censored Data:**\n",
    "\n",
    "```\n",
    "ALGORITHM: Random Survival Forest\n",
    "INPUT: Training data (X, T, δ) where T=event times, δ=censoring indicators\n",
    "\n",
    "FOR each tree m:\n",
    "    bootstrap_sample = BOOTSTRAP_SAMPLE(X, T, δ)\n",
    "    survival_tree = BUILD_SURVIVAL_TREE(bootstrap_sample)\n",
    "\n",
    "FUNCTION BUILD_SURVIVAL_TREE(data):\n",
    "    IF STOPPING_CRITERION(data):\n",
    "        # Leaf contains Kaplan-Meier estimator\n",
    "        km_estimator = KAPLAN_MEIER(data.T, data.δ)\n",
    "        RETURN SURVIVAL_LEAF(km_estimator)\n",
    "    \n",
    "    best_split = FIND_BEST_SURVIVAL_SPLIT(data)\n",
    "    left_data, right_data = PARTITION(data, best_split)\n",
    "    \n",
    "    left_child = BUILD_SURVIVAL_TREE(left_data)\n",
    "    right_child = BUILD_SURVIVAL_TREE(right_data)\n",
    "    \n",
    "    RETURN SURVIVAL_NODE(best_split, left_child, right_child)\n",
    "\n",
    "FUNCTION FIND_BEST_SURVIVAL_SPLIT(data):\n",
    "    # Use log-rank test statistic as splitting criterion\n",
    "    best_score = 0\n",
    "    best_split = None\n",
    "    \n",
    "    FOR feature_j in RANDOM_SAMPLE(features, m_try):\n",
    "        FOR threshold θ:\n",
    "            left_indices = data.X[:, feature_j] <= θ\n",
    "            right_indices = ~left_indices\n",
    "            \n",
    "            # Log-rank test between left and right groups\n",
    "            log_rank_score = LOG_RANK_TEST(\n",
    "                data.T[left_indices], data.δ[left_indices],\n",
    "                data.T[right_indices], data.δ[right_indices]\n",
    "            )\n",
    "            \n",
    "            IF log_rank_score > best_score:\n",
    "                best_score = log_rank_score\n",
    "                best_split = (feature_j, θ)\n",
    "    \n",
    "    RETURN best_split\n",
    "\n",
    "FUNCTION PREDICT_SURVIVAL(x, times):\n",
    "    survival_functions = []\n",
    "    \n",
    "    FOR tree in forest:\n",
    "        leaf = FIND_LEAF(tree, x)\n",
    "        survival_func = leaf.kaplan_meier_estimator\n",
    "        survival_functions.ADD(survival_func)\n",
    "    \n",
    "    # Average survival functions\n",
    "    ensemble_survival = MEAN([sf.evaluate(times) for sf in survival_functions])\n",
    "    RETURN ensemble_survival\n",
    "```\n",
    "\n",
    "### Causal Forests\n",
    "\n",
    "**Heterogeneous Treatment Effect Estimation:**\n",
    "\n",
    "```\n",
    "ALGORITHM: Causal Forest (Generalized Random Forest)\n",
    "INPUT: Features X, treatment W, outcomes Y\n",
    "\n",
    "# Honest splitting: different samples for splitting and estimation\n",
    "FOR each tree m:\n",
    "    bootstrap_sample = BOOTSTRAP_SAMPLE(X, W, Y)\n",
    "    splitting_sample, estimation_sample = SPLIT_SAMPLE(bootstrap_sample, ratio=0.5)\n",
    "    \n",
    "    causal_tree = BUILD_CAUSAL_TREE(splitting_sample, estimation_sample)\n",
    "\n",
    "FUNCTION BUILD_CAUSAL_TREE(splitting_data, estimation_data):\n",
    "    IF STOPPING_CRITERION(splitting_data):\n",
    "        # Estimate treatment effect in leaf using estimation sample\n",
    "        leaf_indices = FIND_ESTIMATION_INDICES(estimation_data, current_region)\n",
    "        treatment_effect = ESTIMATE_CATE(estimation_data[leaf_indices])\n",
    "        RETURN CAUSAL_LEAF(treatment_effect)\n",
    "    \n",
    "    # Find split that maximizes treatment effect heterogeneity\n",
    "    best_split = FIND_BEST_CAUSAL_SPLIT(splitting_data)\n",
    "    \n",
    "    left_splitting, right_splitting = PARTITION(splitting_data, best_split)\n",
    "    left_estimation, right_estimation = PARTITION(estimation_data, best_split)\n",
    "    \n",
    "    left_child = BUILD_CAUSAL_TREE(left_splitting, left_estimation)\n",
    "    right_child = BUILD_CAUSAL_TREE(right_splitting, right_estimation)\n",
    "    \n",
    "    RETURN CAUSAL_NODE(best_split, left_child, right_child)\n",
    "\n",
    "FUNCTION FIND_BEST_CAUSAL_SPLIT(data):\n",
    "    # Splitting criterion: maximize variance of treatment effects across children\n",
    "    best_criterion = 0\n",
    "    best_split = None\n",
    "    \n",
    "    FOR feature_j in RANDOM_SAMPLE(features, m_try):\n",
    "        FOR threshold θ:\n",
    "            left_data = data[data.X[:, feature_j] <= θ]\n",
    "            right_data = data[data.X[:, feature_j] > θ]\n",
    "            \n",
    "            # Estimate treatment effects in each child\n",
    "            tau_left = ESTIMATE_CATE(left_data)\n",
    "            tau_right = ESTIMATE_CATE(right_data)\n",
    "            \n",
    "            # Criterion: weighted variance of treatment effects\n",
    "            n_left, n_right = |left_data|, |right_data|\n",
    "            n_total = n_left + n_right\n",
    "            \n",
    "            criterion = (n_left * n_right / n_total) * (tau_left - tau_right)^2\n",
    "            \n",
    "            IF criterion > best_criterion:\n",
    "                best_criterion = criterion\n",
    "                best_split = (feature_j, θ)\n",
    "    \n",
    "    RETURN best_split\n",
    "\n",
    "FUNCTION ESTIMATE_CATE(data):\n",
    "    # Simple difference-in-means estimator\n",
    "    treated_outcomes = MEAN(data.Y[data.W == 1])\n",
    "    control_outcomes = MEAN(data.Y[data.W == 0])\n",
    "    RETURN treated_outcomes - control_outcomes\n",
    "```\n",
    "\n",
    "### Multi-Armed Bandit Forests\n",
    "\n",
    "**Contextual Bandits with Thompson Sampling:**\n",
    "\n",
    "```\n",
    "ALGORITHM: Contextual Bandit Forest\n",
    "STATE: Forest of contextual trees, reward histories\n",
    "\n",
    "FUNCTION SELECT_ACTION(context_x):\n",
    "    # Thompson sampling: sample from posterior distribution\n",
    "    sampled_rewards = []\n",
    "    \n",
    "    FOR each action a:\n",
    "        # Get leaf predictions for this context-action pair\n",
    "        leaf_rewards = []\n",
    "        FOR tree in forest:\n",
    "            leaf = FIND_LEAF(tree, CONCATENATE(context_x, [a]))\n",
    "            # Sample from leaf's reward distribution\n",
    "            posterior_sample = BETA_SAMPLE(leaf.alpha, leaf.beta)  # For binary rewards\n",
    "            leaf_rewards.ADD(posterior_sample)\n",
    "        \n",
    "        expected_reward = MEAN(leaf_rewards)\n",
    "        sampled_rewards.ADD(expected_reward)\n",
    "    \n",
    "    # Select action with highest sampled reward\n",
    "    best_action = ARGMAX(sampled_rewards)\n",
    "    RETURN best_action\n",
    "\n",
    "FUNCTION UPDATE_FOREST(context_x, action_a, reward_r):\n",
    "    context_action = CONCATENATE(context_x, [action_a])\n",
    "    \n",
    "    FOR tree in forest:\n",
    "        leaf = FIND_LEAF(tree, context_action)\n",
    "        # Update Beta distribution parameters\n",
    "        IF reward_r == 1:\n",
    "            leaf.alpha += 1\n",
    "        ELSE:\n",
    "            leaf.beta += 1\n",
    "        \n",
    "        # Optionally rebuild tree if leaf has enough data\n",
    "        IF leaf.total_updates % rebuild_threshold == 0:\n",
    "            REBUILD_TREE_BRANCH(tree, leaf)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "#### 1. Core Interfaces\n",
    "\n",
    "```\n",
    "INTERFACE: BaseTree\n",
    "METHODS:\n",
    "    fit(X, y, sample_weight=None)\n",
    "    predict(X)\n",
    "    get_leaf_indices(X)\n",
    "    get_n_leaves()\n",
    "    get_depth()\n",
    "\n",
    "INTERFACE: TreeBuilder  \n",
    "METHODS:\n",
    "    build_tree(X, y, sample_weight=None)\n",
    "    find_best_split(node_samples, feature_indices)\n",
    "    create_leaf(node_samples)\n",
    "    should_split(node_samples)\n",
    "\n",
    "INTERFACE: Splitter\n",
    "METHODS:\n",
    "    find_split(X, y, feature_indices, sample_indices)\n",
    "    evaluate_split(feature, threshold, sample_indices)\n",
    "\n",
    "INTERFACE: EnsembleMethod\n",
    "METHODS:\n",
    "    fit(X, y)\n",
    "    predict(X) \n",
    "    predict_proba(X)\n",
    "    get_feature_importance()\n",
    "    get_oob_score()\n",
    "```\n",
    "\n",
    "#### 2. Modular Design Pattern\n",
    "\n",
    "```\n",
    "CLASS RandomForest:\n",
    "    CONSTRUCTOR(\n",
    "        tree_builder: TreeBuilder,\n",
    "        splitter: Splitter, \n",
    "        bootstrap_sampler: BootstrapSampler,\n",
    "        feature_sampler: FeatureSampler\n",
    "    )\n",
    "    \n",
    "    FUNCTION fit(X, y):\n",
    "        self.trees = []\n",
    "        self.oob_predictions = ZEROS(len(y))\n",
    "        self.oob_counts = ZEROS(len(y))\n",
    "        \n",
    "        FOR tree_idx in range(self.n_estimators):\n",
    "            # Get bootstrap sample\n",
    "            sample_indices = self.bootstrap_sampler.sample(len(X))\n",
    "            oob_indices = COMPLEMENT(sample_indices, len(X))\n",
    "            \n",
    "            # Train tree\n",
    "            X_bootstrap = X[sample_indices]\n",
    "            y_bootstrap = y[sample_indices]\n",
    "            \n",
    "            tree = self.tree_builder.build_tree(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update OOB predictions\n",
    "            IF len(oob_indices) > 0:\n",
    "                oob_preds = tree.predict(X[oob_indices])\n",
    "                self.oob_predictions[oob_indices] += oob_preds\n",
    "                self.oob_counts[oob_indices] += 1\n",
    "        \n",
    "        # Finalize OOB predictions\n",
    "        mask = self.oob_counts > 0\n",
    "        self.oob_predictions[mask] /= self.oob_counts[mask]\n",
    "```\n",
    "\n",
    "### Data Structures\n",
    "\n",
    "#### 1. Efficient Tree Representation\n",
    "\n",
    "```\n",
    "STRUCTURE: CompactTree\n",
    "FIELDS:\n",
    "    nodes: Array[TreeNode]           # All nodes in breadth-first order\n",
    "    feature_indices: Array[int16]    # Split features (16-bit for memory)\n",
    "    thresholds: Array[float32]       # Split thresholds (32-bit precision)\n",
    "    left_children: Array[int32]      # Left child indices\n",
    "    right_children: Array[int32]     # Right child indices  \n",
    "    leaf_values: Array[float32]      # Leaf predictions\n",
    "    is_leaf: Array[bool]            # Leaf indicators\n",
    "\n",
    "FUNCTION traverse(x: sample) -> prediction:\n",
    "    node_idx = 0  # Start at root\n",
    "    \n",
    "    WHILE not self.is_leaf[node_idx]:\n",
    "        feature_idx = self.feature_indices[node_idx]\n",
    "        threshold = self.thresholds[node_idx]\n",
    "        \n",
    "        IF x[feature_idx] <= threshold:\n",
    "            node_idx = self.left_children[node_idx]\n",
    "        ELSE:\n",
    "            node_idx = self.right_children[node_idx]\n",
    "    \n",
    "    RETURN self.leaf_values[node_idx]\n",
    "```\n",
    "\n",
    "#### 2. Memory-Mapped Data Loading\n",
    "\n",
    "```\n",
    "STRUCTURE: MemoryMappedDataset\n",
    "FIELDS:\n",
    "    X_memmap: np.memmap          # Feature matrix\n",
    "    y_memmap: np.memmap          # Target vector\n",
    "    chunk_size: int              # For batch processing\n",
    "\n",
    "FUNCTION get_batch(start_idx, end_idx):\n",
    "    # Lazy loading of data chunks\n",
    "    X_batch = self.X_memmap[start_idx:end_idx]\n",
    "    y_batch = self.y_memmap[start_idx:end_idx]\n",
    "    RETURN X_batch, y_batch\n",
    "\n",
    "FUNCTION parallel_bootstrap_sample(n_samples, n_trees, n_workers):\n",
    "    # Generate all bootstrap indices in parallel\n",
    "    with PARALLEL_POOL(n_workers):\n",
    "        bootstrap_indices = PARALLEL_MAP(\n",
    "            lambda _: np.random.choice(n_samples, n_samples, replace=True),\n",
    "            range(n_trees)\n",
    "        )\n",
    "    RETURN bootstrap_indices\n",
    "```\n",
    "\n",
    "### Performance Profiling\n",
    "\n",
    "#### 1. Timing Critical Paths\n",
    "\n",
    "```\n",
    "DECORATOR: @profile_time\n",
    "FUNCTION profile_time(func):\n",
    "    FUNCTION wrapper(*args, **kwargs):\n",
    "        start_time = TIME()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = TIME()\n",
    "        \n",
    "        PROFILER.record(func.__name__, end_time - start_time)\n",
    "        RETURN result\n",
    "    \n",
    "    RETURN wrapper\n",
    "\n",
    "CLASS RandomForestProfiler:\n",
    "    FIELDS:\n",
    "        timings: Dict[str, List[float]]\n",
    "    \n",
    "    @profile_time\n",
    "    FUNCTION fit(self, X, y):\n",
    "        # All fit operations are automatically timed\n",
    "        pass\n",
    "    \n",
    "    @profile_time  \n",
    "    FUNCTION find_best_split(self, node_data):\n",
    "        # Split finding timing\n",
    "        pass\n",
    "        \n",
    "    FUNCTION get_performance_report():\n",
    "        report = {}\n",
    "        FOR operation, times in self.timings.items():\n",
    "            report[operation] = {\n",
    "                'total_time': sum(times),\n",
    "                'average_time': mean(times),\n",
    "                'max_time': max(times),\n",
    "                'call_count': len(times)\n",
    "            }\n",
    "        RETURN report\n",
    "```\n",
    "\n",
    "#### 2. Memory Usage Tracking\n",
    "\n",
    "```\n",
    "FUNCTION estimate_memory_usage(n_samples, n_features, n_trees, max_depth):\n",
    "    # Estimate memory requirements\n",
    "    \n",
    "    # Data storage\n",
    "    data_memory = n_samples * n_features * 4  # 4 bytes per float32\n",
    "    \n",
    "    # Tree storage (compact representation)\n",
    "    max_nodes_per_tree = 2**(max_depth + 1) - 1\n",
    "    nodes_memory_per_tree = max_nodes_per_tree * (\n",
    "        4 +    # feature_index (int16) + padding\n",
    "        4 +    # threshold (float32)  \n",
    "        4 +    # left_child (int32)\n",
    "        4 +    # right_child (int32)\n",
    "        4 +    # leaf_value (float32)\n",
    "        1      # is_leaf (bool)\n",
    "    )\n",
    "    \n",
    "    total_tree_memory = n_trees * nodes_memory_per_tree\n",
    "    \n",
    "    # Bootstrap indices storage  \n",
    "    bootstrap_memory = n_trees * n_samples * 4  # int32 indices\n",
    "    \n",
    "    # OOB tracking\n",
    "    oob_memory = n_samples * (4 + 4)  # predictions + counts\n",
    "    \n",
    "    total_memory = data_memory + total_tree_memory + bootstrap_memory + oob_memory\n",
    "    \n",
    "    RETURN {\n",
    "        'data_mb': data_memory / 1024**2,\n",
    "        'trees_mb': total_tree_memory / 1024**2, \n",
    "        'bootstrap_mb': bootstrap_memory / 1024**2,\n",
    "        'oob_mb': oob_memory / 1024**2,\n",
    "        'total_mb': total_memory / 1024**2\n",
    "    }\n",
    "```\n",
    "\n",
    "### Testing and Validation\n",
    "\n",
    "#### 1. Unit Tests for Core Components\n",
    "\n",
    "```\n",
    "FUNCTION test_bootstrap_sampling():\n",
    "    n_samples = 1000\n",
    "    sampler = BootstrapSampler()\n",
    "    \n",
    "    # Test sample size\n",
    "    indices = sampler.sample(n_samples)\n",
    "    ASSERT len(indices) == n_samples\n",
    "    \n",
    "    # Test with replacement (should have duplicates)\n",
    "    unique_indices = set(indices)\n",
    "    ASSERT len(unique_indices) < n_samples\n",
    "    \n",
    "    # Test coverage (should cover ~63.2% of samples)\n",
    "    coverage = len(unique_indices) / n_samples\n",
    "    ASSERT 0.6 <= coverage <= 0.67\n",
    "\n",
    "FUNCTION test_feature_importance_consistency():\n",
    "    # Generate synthetic data with known feature importance\n",
    "    X = GENERATE_SYNTHETIC_DATA(n_samples=1000, n_features=10, random_state=42)\n",
    "    y = X[:, 0] + 0.5 * X[:, 1] + NOISE()  # Features 0,1 important, rest noise\n",
    "    \n",
    "    rf = RandomForest(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    importance = rf.feature_importances_\n",
    "    \n",
    "    # Most important features should be 0 and 1\n",
    "    top_features = ARGSORT(importance)[-2:]\n",
    "    ASSERT 0 in top_features\n",
    "    ASSERT 1 in top_features\n",
    "    \n",
    "    # Sum should be 1\n",
    "    ASSERT ABS(SUM(importance) - 1.0) < 1e-10\n",
    "\n",
    "FUNCTION test_oob_score_validity():\n",
    "    X, y = LOAD_DATASET('classification')\n",
    "    rf = RandomForest(n_estimators=100)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    oob_score = rf.oob_score_\n",
    "    \n",
    "    # OOB score should be reasonable (0.7-1.0 for good datasets)\n",
    "    ASSERT 0.0 <= oob_score <= 1.0\n",
    "    \n",
    "    # Compare with cross-validation\n",
    "    cv_scores = CROSS_VAL_SCORE(rf, X, y, cv=5)\n",
    "    cv_mean = MEAN(cv_scores)\n",
    "    \n",
    "    # OOB and CV should be similar (within reasonable range)\n",
    "    ASSERT ABS(oob_score - cv_mean) < 0.1\n",
    "```\n",
    "\n",
    "#### 2. Integration Tests\n",
    "\n",
    "```\n",
    "FUNCTION test_end_to_end_regression():\n",
    "    # Complete pipeline test\n",
    "    X, y = MAKE_REGRESSION(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = TRAIN_TEST_SPLIT(X, y, test_size=0.2)\n",
    "    \n",
    "    # Fit model\n",
    "    rf = RandomForest(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Performance metrics\n",
    "    mse = MEAN_SQUARED_ERROR(y_test, y_pred)\n",
    "    r2 = R2_SCORE(y_test, y_pred)\n",
    "    \n",
    "    # Sanity checks\n",
    "    ASSERT mse > 0\n",
    "    ASSERT r2 > 0.8  # Should achieve good performance on synthetic data\n",
    "    ASSERT len(y_pred) == len(y_test)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = rf.feature_importances_\n",
    "    ASSERT len(importance) == X.shape[1]\n",
    "    ASSERT ABS(SUM(importance) - 1.0) < 1e-10\n",
    "\n",
    "FUNCTION test_memory_efficiency():\n",
    "    # Test memory usage doesn't explode with large datasets\n",
    "    n_samples_large = 50000\n",
    "    n_features = 100\n",
    "    \n",
    "    X = np.random.randn(n_samples_large, n_features).astype(np.float32)\n",
    "    y = np.random.randn(n_samples_large).astype(np.float32)\n",
    "    \n",
    "    initial_memory = GET_MEMORY_USAGE()\n",
    "    \n",
    "    rf = RandomForest(n_estimators=10, max_depth=8)  # Small forest to control memory\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    final_memory = GET_MEMORY_USAGE()\n",
    "    memory_increase = final_memory - initial_memory\n",
    "    \n",
    "    # Should not use excessive memory (heuristic: < 10x data size)\n",
    "    data_size = X.nbytes + y.nbytes\n",
    "    ASSERT memory_increase < 10 * data_size\n",
    "```\n",
    "\n",
    "### Configuration Management\n",
    "\n",
    "#### 1. Hierarchical Configuration\n",
    "\n",
    "```\n",
    "CONFIGURATION: RandomForestConfig\n",
    "defaults:\n",
    "  ensemble:\n",
    "    n_estimators: 100\n",
    "    bootstrap: true\n",
    "    oob_score: true\n",
    "    \n",
    "  tree:\n",
    "    criterion: \"gini\"  # or \"entropy\", \"mse\", \"mae\"\n",
    "    max_depth: null\n",
    "    min_samples_split: 2\n",
    "    min_samples_leaf: 1\n",
    "    min_impurity_decrease: 0.0\n",
    "    \n",
    "  features:\n",
    "    max_features: \"sqrt\"  # or \"log2\", int, float\n",
    "    feature_sampling: \"without_replacement\"\n",
    "    \n",
    "  performance:\n",
    "    n_jobs: -1\n",
    "    random_state: null\n",
    "    verbose: 0\n",
    "    \n",
    "  advanced:\n",
    "    class_weight: null\n",
    "    ccp_alpha: 0.0\n",
    "    max_samples: null\n",
    "\n",
    "FUNCTION load_config(config_path, overrides=None):\n",
    "    base_config = LOAD_YAML(config_path)\n",
    "    \n",
    "    IF overrides:\n",
    "        config = DEEP_MERGE(base_config, overrides)\n",
    "    ELSE:\n",
    "        config = base_config\n",
    "    \n",
    "    RETURN validate_config(config)\n",
    "\n",
    "FUNCTION validate_config(config):\n",
    "    # Type checking and constraint validation\n",
    "    IF config.ensemble.n_estimators <= 0:\n",
    "        RAISE ValueError(\"n_estimators must be positive\")\n",
    "    \n",
    "    IF config.tree.max_depth is not None AND config.tree.max_depth <= 0:\n",
    "        RAISE ValueError(\"max_depth must be positive or None\")\n",
    "    \n",
    "    # Feature sampling validation\n",
    "    max_features = config.features.max_features\n",
    "    IF isinstance(max_features, str):\n",
    "        ASSERT max_features in [\"sqrt\", \"log2\"]\n",
    "    ELIF isinstance(max_features, int):\n",
    "        ASSERT max_features > 0\n",
    "    ELIF isinstance(max_features, float):\n",
    "        ASSERT 0.0 < max_features <= 1.0\n",
    "    \n",
    "    RETURN config\n",
    "```\n",
    "\n",
    "#### 2. Experiment Tracking Integration\n",
    "\n",
    "```\n",
    "CLASS ExperimentTracker:\n",
    "    FUNCTION __init__(self, experiment_name, backend=\"mlflow\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.backend = backend\n",
    "        self.run_id = GENERATE_RUN_ID()\n",
    "    \n",
    "    FUNCTION log_config(self, config):\n",
    "        FOR key, value in FLATTEN_DICT(config).items():\n",
    "            self.backend.log_param(key, value)\n",
    "    \n",
    "    FUNCTION log_metrics(self, metrics_dict, step=None):\n",
    "        FOR metric_name, metric_value in metrics_dict.items():\n",
    "            self.backend.log_metric(metric_name, metric_value, step)\n",
    "    \n",
    "    FUNCTION log_feature_importance(self, feature_names, importance_values):\n",
    "        importance_dict = dict(zip(feature_names, importance_values))\n",
    "        self.backend.log_dict(importance_dict, \"feature_importance.json\")\n",
    "    \n",
    "    FUNCTION log_model(self, model, model_name=\"random_forest\"):\n",
    "        self.backend.log_model(model, model_name)\n",
    "\n",
    "# Usage example\n",
    "tracker = ExperimentTracker(\"random_forest_optimization\")\n",
    "tracker.log_config(config)\n",
    "\n",
    "rf = RandomForest(**config)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "metrics = {\n",
    "    \"oob_score\": rf.oob_score_,\n",
    "    \"test_accuracy\": ACCURACY_SCORE(y_test, rf.predict(X_test)),\n",
    "    \"training_time\": training_time,\n",
    "    \"n_nodes_total\": sum(tree.get_n_leaves() for tree in rf.estimators_)\n",
    "}\n",
    "\n",
    "tracker.log_metrics(metrics)\n",
    "tracker.log_feature_importance(feature_names, rf.feature_importances_)\n",
    "tracker.log_model(rf)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion and Best Practices\n",
    "\n",
    "### Algorithm Selection Guidelines\n",
    "\n",
    "**Decision Framework:**\n",
    "```\n",
    "Problem Type Assessment:\n",
    "├─ Tabular data, moderate size → Random Forest\n",
    "├─ Very large dataset → ExtraTrees or Subbagging  \n",
    "├─ High-dimensional, sparse → Random Subspaces + Feature Hashing\n",
    "├─ Imbalanced classes → Balanced Random Forest\n",
    "├─ Streaming data → Online Random Forest\n",
    "├─ Anomaly detection → Isolation Forest\n",
    "├─ Survival analysis → Random Survival Forest\n",
    "└─ Causal inference → Causal Forest\n",
    "\n",
    "Performance Requirements:\n",
    "├─ Maximum accuracy → Random Forest with hyperparameter tuning\n",
    "├─ Speed priority → ExtraTrees with fewer estimators\n",
    "├─ Memory constraints → Subbagging with compact trees\n",
    "└─ Interpretability → Fewer, deeper trees with feature importance\n",
    "```\n",
    "\n",
    "### Hyperparameter Tuning Strategy\n",
    "\n",
    "**Progressive Tuning Approach:**\n",
    "1. **Start with defaults:** Get baseline performance\n",
    "2. **Tune ensemble size:** Find optimal n_estimators via OOB score\n",
    "3. **Tune tree complexity:** max_depth, min_samples_split, min_samples_leaf\n",
    "4. **Tune feature sampling:** m_try parameter\n",
    "5. **Advanced tuning:** class_weight, bootstrap strategy\n",
    "\n",
    "**Recommended Grid Search:**\n",
    "```\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5]\n",
    "}\n",
    "```\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "**Overfitting Prevention:**\n",
    "- Use OOB score for early stopping\n",
    "- Limit tree depth and increase min_samples_leaf\n",
    "- Enable bootstrap (don't use full dataset per tree)\n",
    "- Regularize with min_impurity_decrease\n",
    "\n",
    "**Performance Optimization:**\n",
    "- Use feature selection to reduce dimensionality\n",
    "- Enable parallel processing (n_jobs=-1)\n",
    "- Consider subsampling for very large datasets\n",
    "- Use memory-mapped arrays for huge datasets\n",
    "\n",
    "**Interpretation Best Practices:**\n",
    "- Use permutation importance over MDI when possible\n",
    "- Report confidence intervals for importance scores\n",
    "- Visualize partial dependence plots for top features\n",
    "- Use SHAP for instance-level explanations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
