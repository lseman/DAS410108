{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed106f8",
   "metadata": {},
   "source": [
    "## Theorem: Ensemble methods are no worse than single models\n",
    "\n",
    "**Statement:** For any ensemble of predictors, the expected squared error is no greater than the average expected squared error of individual predictors, with equality only in degenerate cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Notation\n",
    "\n",
    "Let:\n",
    "- $f^*(x)$ = true target function\n",
    "- $\\hat{f}_i(x)$ = prediction from model $i$, where $i = 1, 2, \\ldots, M$\n",
    "- $\\hat{f}_{\\text{ens}}(x) = \\frac{1}{M}\\sum_{i=1}^M \\hat{f}_i(x)$ = ensemble prediction (simple averaging)\n",
    "- $\\mathbb{E}[\\cdot]$ = expectation over training sets\n",
    "- $\\text{Var}[\\cdot]$ = variance\n",
    "- $\\text{Cov}[\\cdot, \\cdot]$ = covariance\n",
    "\n",
    "---\n",
    "\n",
    "## Proof Part 1: Basic Variance Reduction\n",
    "\n",
    "### Individual Model Error\n",
    "For any individual model $i$:\n",
    "$$\\mathbb{E}[(\\hat{f}_i(x) - f^*(x))^2] = \\text{MSE}_i$$\n",
    "\n",
    "### Ensemble Error\n",
    "For the ensemble:\n",
    "$$\\mathbb{E}[(\\hat{f}_{\\text{ens}}(x) - f^*(x))^2] = \\mathbb{E}\\left[\\left(\\frac{1}{M}\\sum_{i=1}^M \\hat{f}_i(x) - f^*(x)\\right)^2\\right]$$\n",
    "\n",
    "Expanding:\n",
    "$$= \\mathbb{E}\\left[\\left(\\frac{1}{M}\\sum_{i=1}^M (\\hat{f}_i(x) - f^*(x))\\right)^2\\right]$$\n",
    "\n",
    "$$= \\frac{1}{M^2} \\mathbb{E}\\left[\\left(\\sum_{i=1}^M (\\hat{f}_i(x) - f^*(x))\\right)^2\\right]$$\n",
    "\n",
    "### Step-by-Step Expansion\n",
    "\n",
    "**Step 1:** Let $e_i(x) = \\hat{f}_i(x) - f^*(x)$ (error of model $i$)\n",
    "\n",
    "$\\mathbb{E}[(\\hat{f}_{\\text{ens}}(x) - f^*(x))^2] = \\mathbb{E}\\left[\\left(\\frac{1}{M}\\sum_{i=1}^M e_i(x)\\right)^2\\right]$\n",
    "\n",
    "**Step 2:** Expand the square using $(a + b + c + \\ldots)^2 = a^2 + b^2 + c^2 + \\ldots + 2ab + 2ac + 2bc + \\ldots$\n",
    "\n",
    "$= \\frac{1}{M^2} \\mathbb{E}\\left[\\left(\\sum_{i=1}^M e_i(x)\\right)^2\\right]$\n",
    "\n",
    "$= \\frac{1}{M^2} \\mathbb{E}\\left[\\sum_{i=1}^M e_i(x)^2 + \\sum_{i=1}^M \\sum_{\\substack{j=1 \\\\ j \\neq i}}^M e_i(x) e_j(x)\\right]$\n",
    "\n",
    "**Step 3:** Separate diagonal and off-diagonal terms\n",
    "\n",
    "$= \\frac{1}{M^2} \\mathbb{E}\\left[\\sum_{i=1}^M e_i(x)^2 + \\sum_{i \\neq j} e_i(x) e_j(x)\\right]$\n",
    "\n",
    "**Step 4:** Apply linearity of expectation\n",
    "\n",
    "$= \\frac{1}{M^2} \\left[\\sum_{i=1}^M \\mathbb{E}[e_i(x)^2] + \\sum_{i \\neq j} \\mathbb{E}[e_i(x) e_j(x)]\\right]$\n",
    "\n",
    "**Step 5:** Recognize terms\n",
    "- $\\mathbb{E}[e_i(x)^2] = \\mathbb{E}[(\\hat{f}_i(x) - f^*(x))^2] = \\text{MSE}_i$\n",
    "- $\\mathbb{E}[e_i(x) e_j(x)] = \\text{Cov}[e_i(x), e_j(x)]$ (since errors may not be zero-mean)\n",
    "\n",
    "$= \\frac{1}{M^2} \\left[\\sum_{i=1}^M \\text{MSE}_i + \\sum_{i \\neq j} \\text{Cov}[e_i(x), e_j(x)]\\right]$\n",
    "\n",
    "---\n",
    "\n",
    "## Proof Part 2: Bias-Variance Decomposition for Ensembles\n",
    "\n",
    "### Individual Bias-Variance Decomposition\n",
    "For model $i$:\n",
    "$$\\text{MSE}_i = \\text{Bias}_i^2 + \\text{Var}_i + \\sigma^2$$\n",
    "\n",
    "where:\n",
    "- $\\text{Bias}_i^2 = (\\mathbb{E}[\\hat{f}_i(x)] - f^*(x))^2$\n",
    "- $\\text{Var}_i = \\mathbb{E}[(\\hat{f}_i(x) - \\mathbb{E}[\\hat{f}_i(x)])^2]$\n",
    "- $\\sigma^2$ = irreducible error\n",
    "\n",
    "### Ensemble Bias-Variance Decomposition\n",
    "Let $\\mu_i = \\mathbb{E}[\\hat{f}_i(x)]$ and $\\mu_{\\text{ens}} = \\frac{1}{M}\\sum_{i=1}^M \\mu_i$.\n",
    "\n",
    "**Ensemble Bias:**\n",
    "$$\\text{Bias}_{\\text{ens}}^2 = (\\mu_{\\text{ens}} - f^*(x))^2 = \\left(\\frac{1}{M}\\sum_{i=1}^M \\mu_i - f^*(x)\\right)^2$$\n",
    "\n",
    "**Ensemble Variance:**\n",
    "$$\\text{Var}_{\\text{ens}} = \\text{Var}\\left[\\frac{1}{M}\\sum_{i=1}^M \\hat{f}_i(x)\\right]$$\n",
    "\n",
    "Using variance properties:\n",
    "$$\\text{Var}_{\\text{ens}} = \\frac{1}{M^2} \\text{Var}\\left[\\sum_{i=1}^M \\hat{f}_i(x)\\right]$$\n",
    "\n",
    "$$= \\frac{1}{M^2} \\left[\\sum_{i=1}^M \\text{Var}[\\hat{f}_i(x)] + \\sum_{i \\neq j} \\text{Cov}[\\hat{f}_i(x), \\hat{f}_j(x)]\\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "## Proof Part 3: The Key Result\n",
    "\n",
    "### Case 1: Homogeneous Models (Equal Performance)\n",
    "Assume all models have:\n",
    "- Equal bias: $\\text{Bias}_i = B$ for all $i$\n",
    "- Equal variance: $\\text{Var}_i = V$ for all $i$\n",
    "- Pairwise correlation: $\\text{Corr}[\\hat{f}_i(x), \\hat{f}_j(x)] = \\rho$ for all $i \\neq j$\n",
    "\n",
    "Then: $\\text{Cov}[\\hat{f}_i(x), \\hat{f}_j(x)] = \\rho V$\n",
    "\n",
    "**Ensemble Bias:**\n",
    "$$\\text{Bias}_{\\text{ens}}^2 = B^2$$\n",
    "(Averaging doesn't change bias if all models have the same bias)\n",
    "\n",
    "**Ensemble Variance:**\n",
    "$$\\text{Var}_{\\text{ens}} = \\frac{1}{M^2}[M \\cdot V + M(M-1) \\cdot \\rho V]$$\n",
    "\n",
    "$$= \\frac{V}{M^2}[M + M(M-1)\\rho]$$\n",
    "\n",
    "$$= \\frac{V}{M}[1 + (M-1)\\rho]$$\n",
    "\n",
    "$$= \\frac{V(1-\\rho)}{M} + \\rho V$$\n",
    "\n",
    "### The Fundamental Result\n",
    "$$\\boxed{\\text{Var}_{\\text{ens}} = \\rho V + \\frac{(1-\\rho)V}{M}}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **First term** $\\rho V$: Irreducible variance due to correlation\n",
    "- **Second term** $\\frac{(1-\\rho)V}{M}$: Variance reduction from averaging\n",
    "\n",
    "### Comparison with Individual Models\n",
    "Individual model MSE: $\\text{MSE}_{\\text{individual}} = B^2 + V + \\sigma^2$\n",
    "\n",
    "Ensemble MSE: $\\text{MSE}_{\\text{ensemble}} = B^2 + \\rho V + \\frac{(1-\\rho)V}{M} + \\sigma^2$\n",
    "\n",
    "**Variance reduction factor:**\n",
    "$$\\frac{\\text{Var}_{\\text{ens}}}{\\text{Var}_{\\text{individual}}} = \\rho + \\frac{(1-\\rho)}{M} \\leq 1$$\n",
    "\n",
    "**Equality holds** if and only if $M = 1$ or $\\rho = 1$ (perfectly correlated models).\n",
    "\n",
    "---\n",
    "\n",
    "## Proof Part 4: General Case (Heterogeneous Models)\n",
    "\n",
    "For models with different performance characteristics:\n",
    "\n",
    "Let $\\bar{V} = \\frac{1}{M}\\sum_{i=1}^M V_i$ and $\\bar{\\rho} = \\frac{2}{M(M-1)}\\sum_{i<j} \\rho_{ij}$\n",
    "\n",
    "**Jensen's Inequality Application:**\n",
    "Since variance is a convex function of the prediction:\n",
    "\n",
    "$$\\text{Var}_{\\text{ens}} \\leq \\bar{\\rho}\\bar{V} + \\frac{(1-\\bar{\\rho})\\bar{V}}{M}$$\n",
    "\n",
    "with equality when all individual variances are equal.\n",
    "\n",
    "---\n",
    "\n",
    "## Proof Part 5: Optimality Conditions\n",
    "\n",
    "### When is Ensemble Optimal?\n",
    "\n",
    "**Theorem:** The ensemble $\\hat{f}_{\\text{ens}} = \\sum_{i=1}^M w_i \\hat{f}_i$ with weights $w_i$ is optimal when:\n",
    "\n",
    "$$w_i = \\frac{\\sum_{j \\neq i} \\sigma_j^2 - \\sum_{j \\neq i} \\sigma_{ij}}{\\sum_{k,\\ell} (\\sigma_k^2 - \\sigma_{k\\ell})}$$\n",
    "\n",
    "where $\\sigma_i^2 = \\text{Var}[\\hat{f}_i]$ and $\\sigma_{ij} = \\text{Cov}[\\hat{f}_i, \\hat{f}_j]$.\n",
    "\n",
    "**Special Case - Equal Weights:**\n",
    "When all models have equal variance and equal pairwise correlation, uniform weighting ($w_i = \\frac{1}{M}$) is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "## Numerical Example\n",
    "\n",
    "Consider 3 models with:\n",
    "- Individual variance: $V = 1.0$\n",
    "- Correlation: $\\rho = 0.6$\n",
    "\n",
    "**Individual model variance:** $V = 1.0$\n",
    "\n",
    "**Ensemble variance:** \n",
    "$$\\text{Var}_{\\text{ens}} = 0.6 \\times 1.0 + \\frac{(1-0.6) \\times 1.0}{3} = 0.6 + 0.133 = 0.733$$\n",
    "\n",
    "**Variance reduction:** $\\frac{0.733}{1.0} = 73.3\\%$ of original variance\n",
    "\n",
    "**Improvement:** $1 - 0.733 = 26.7\\%$ variance reduction\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. Correlation is the Enemy\n",
    "$$\\lim_{\\rho \\to 1} \\text{Var}_{\\text{ens}} = V$$\n",
    "(No improvement when models are perfectly correlated)\n",
    "\n",
    "$$\\lim_{\\rho \\to 0} \\text{Var}_{\\text{ens}} = \\frac{V}{M}$$\n",
    "(Maximum improvement when models are uncorrelated)\n",
    "\n",
    "### 2. More Models Help (If Uncorrelated)\n",
    "$$\\lim_{M \\to \\infty} \\text{Var}_{\\text{ens}} = \\rho V$$\n",
    "\n",
    "### 3. Diversity Matters More Than Accuracy\n",
    "Better to ensemble 3 diverse models with 80% accuracy than 3 identical models with 90% accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "### Ensemble Design Principles\n",
    "\n",
    "1. **Maximize Diversity:** Use different:\n",
    "   - Algorithms (tree + SVM + neural network)\n",
    "   - Training data (bagging, cross-validation folds)\n",
    "   - Features (random subspaces)\n",
    "   - Hyperparameters\n",
    "\n",
    "2. **Balance Accuracy and Diversity:**\n",
    "   - Don't include terrible models (negative correlation with truth)\n",
    "   - Don't include identical models (correlation = 1)\n",
    "\n",
    "3. **Optimal Ensemble Size:**\n",
    "   - Diminishing returns as $M$ increases\n",
    "   - Computational cost grows linearly with $M$\n",
    "   - Sweet spot typically around $M = 5-20$ for most problems\n",
    "\n",
    "### Real-World Validation\n",
    "\n",
    "This theory explains why:\n",
    "- **Random Forest** works (different trees via bootstrapping + feature sampling)\n",
    "- **Bagging** reduces overfitting (reduces variance)\n",
    "- **Boosting** can work (sequential models focus on different errors)\n",
    "- **Model stacking** is effective (different algorithm types)\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**Mathematical Guarantee:** Ensembles are **provably no worse** than individual models, with improvement guaranteed unless models are perfectly correlated.\n",
    "\n",
    "**The ensemble advantage comes from the fundamental statistical principle:**\n",
    "$$\\boxed{\\text{Averaging reduces variance while preserving bias}}$$\n",
    "\n",
    "This is not just an empirical observation—it's a mathematical certainty rooted in basic properties of expectation and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec574b32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d182635",
   "metadata": {},
   "source": [
    "# Mathematical Theory of Bagging and Boosting\n",
    "\n",
    "## Part I: Bootstrap Aggregating (Bagging) - Complete Mathematical Analysis\n",
    "\n",
    "### 1.1 Bootstrap Sampling Theory\n",
    "\n",
    "**Definition:** Given dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, a bootstrap sample $\\mathcal{D}_b$ is created by sampling $n$ points **with replacement** from $\\mathcal{D}$.\n",
    "\n",
    "**Fundamental Question:** What's the probability that any particular sample $(x_i, y_i)$ appears in bootstrap sample $\\mathcal{D}_b$?\n",
    "\n",
    "### 1.2 The 63.2% Rule - Mathematical Proof\n",
    "\n",
    "**Theorem:** The probability that sample $i$ appears at least once in a bootstrap sample is approximately $1 - \\frac{1}{e} \\approx 0.632$.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $X_{i,j}$ be the indicator random variable:\n",
    "$$X_{i,j} = \\begin{cases} \n",
    "1 & \\text{if sample } i \\text{ is selected in draw } j \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The probability that sample $i$ is **not** selected in any single draw:\n",
    "$$P(X_{i,j} = 0) = \\frac{n-1}{n}$$\n",
    "\n",
    "The probability that sample $i$ is **not** selected in any of the $n$ draws:\n",
    "$$P(\\text{sample } i \\text{ not in bootstrap}) = \\left(\\frac{n-1}{n}\\right)^n$$\n",
    "\n",
    "Therefore, the probability that sample $i$ **is** selected at least once:\n",
    "$$P(\\text{sample } i \\text{ in bootstrap}) = 1 - \\left(\\frac{n-1}{n}\\right)^n$$\n",
    "\n",
    "**Taking the limit as $n \\to \\infty$:**\n",
    "$$\\lim_{n \\to \\infty} \\left(\\frac{n-1}{n}\\right)^n = \\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right)^n = \\frac{1}{e}$$\n",
    "\n",
    "Therefore:\n",
    "$$\\boxed{P(\\text{sample in bootstrap}) = 1 - \\frac{1}{e} \\approx 0.632}$$\n",
    "\n",
    "**Corollary:** About 36.8% of samples are **out-of-bag** (OOB) for each bootstrap sample.\n",
    "\n",
    "### 1.3 Variance Reduction Mechanism in Bagging\n",
    "\n",
    "**Setup:**\n",
    "- True function: $f^*(x)$\n",
    "- Individual predictor: $\\hat{f}_b(x)$ trained on bootstrap sample $b$\n",
    "- Bagging predictor: $\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b(x)$\n",
    "\n",
    "**Key Insight:** Bootstrap samples create **diversity** in the training sets, leading to different predictors.\n",
    "\n",
    "### 1.4 Detailed Variance Analysis\n",
    "\n",
    "**Individual Model Bias and Variance:**\n",
    "\n",
    "For any bootstrap-trained model $b$:\n",
    "$$\\mathbb{E}_{\\mathcal{D}}[\\hat{f}_b(x)] = \\mu(x) \\quad \\text{(expectation over all possible datasets)}$$\n",
    "$$\\text{Var}_{\\mathcal{D}}[\\hat{f}_b(x)] = \\sigma^2(x)$$\n",
    "\n",
    "**Bagging Bias:**\n",
    "$$\\mathbb{E}_{\\mathcal{D}}[\\hat{f}_{\\text{bag}}(x)] = \\mathbb{E}_{\\mathcal{D}}\\left[\\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b(x)\\right] = \\frac{1}{B} \\sum_{b=1}^B \\mathbb{E}_{\\mathcal{D}}[\\hat{f}_b(x)] = \\mu(x)$$\n",
    "\n",
    "**Key Result 1:** Bagging preserves bias exactly.\n",
    "\n",
    "**Bagging Variance:**\n",
    "\n",
    "Under the assumption that bootstrap samples are **approximately independent**:\n",
    "$$\\text{Var}_{\\mathcal{D}}[\\hat{f}_{\\text{bag}}(x)] = \\text{Var}_{\\mathcal{D}}\\left[\\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b(x)\\right]$$\n",
    "\n",
    "If bootstrap predictors were completely independent:\n",
    "$$= \\frac{1}{B^2} \\sum_{b=1}^B \\text{Var}_{\\mathcal{D}}[\\hat{f}_b(x)] = \\frac{1}{B^2} \\cdot B \\cdot \\sigma^2(x) = \\frac{\\sigma^2(x)}{B}$$\n",
    "\n",
    "**However,** bootstrap samples are **not** independent because they're drawn from the same original dataset.\n",
    "\n",
    "### 1.5 Accounting for Bootstrap Correlation\n",
    "\n",
    "Let $\\rho_{\\text{boot}}$ be the correlation between predictions from different bootstrap samples.\n",
    "\n",
    "**More accurate variance formula:**\n",
    "$$\\text{Var}_{\\mathcal{D}}[\\hat{f}_{\\text{bag}}(x)] = \\frac{\\sigma^2(x)}{B} + \\rho_{\\text{boot}} \\sigma^2(x) \\left(1 - \\frac{1}{B}\\right)$$\n",
    "\n",
    "As $B \\to \\infty$:\n",
    "$$\\lim_{B \\to \\infty} \\text{Var}_{\\mathcal{D}}[\\hat{f}_{\\text{bag}}(x)] = \\rho_{\\text{boot}} \\sigma^2(x)$$\n",
    "\n",
    "**Key Result 2:** Bagging can only reduce variance down to the correlation level between bootstrap predictors.\n",
    "\n",
    "### 1.6 Why Bagging Works Best for High-Variance Models\n",
    "\n",
    "**Theorem:** Bagging provides the most benefit for predictors with high variance and low bias.\n",
    "\n",
    "**Proof by MSE decomposition:**\n",
    "\n",
    "Individual model MSE:\n",
    "$$\\text{MSE}_{\\text{individual}} = \\text{Bias}^2 + \\sigma^2(x) + \\text{noise}^2$$\n",
    "\n",
    "Bagging MSE:\n",
    "$$\\text{MSE}_{\\text{bag}} = \\text{Bias}^2 + \\rho_{\\text{boot}} \\sigma^2(x) + \\text{noise}^2$$\n",
    "\n",
    "**Improvement:**\n",
    "$$\\text{MSE}_{\\text{individual}} - \\text{MSE}_{\\text{bag}} = (1 - \\rho_{\\text{boot}}) \\sigma^2(x)$$\n",
    "\n",
    "**Conclusion:** Improvement is proportional to the original variance $\\sigma^2(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## Part II: Boosting - Sequential Error Correction Theory\n",
    "\n",
    "### 2.1 AdaBoost Mathematical Framework\n",
    "\n",
    "**Core Principle:** Train weak learners sequentially, where each learner focuses on the mistakes of previous learners.\n",
    "\n",
    "**Algorithm Setup:**\n",
    "- Training data: $\\{(x_i, y_i)\\}_{i=1}^n$ where $y_i \\in \\{-1, +1\\}$\n",
    "- Sample weights: $w_i^{(t)}$ at iteration $t$\n",
    "- Weak learner: $h_t: \\mathcal{X} \\to \\{-1, +1\\}$\n",
    "- Final classifier: $H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)$\n",
    "\n",
    "### 2.2 AdaBoost Algorithm with Mathematical Justification\n",
    "\n",
    "**Initialization:** $w_i^{(1)} = \\frac{1}{n}$ for all $i$\n",
    "\n",
    "**For $t = 1, 2, \\ldots, T$:**\n",
    "\n",
    "**Step 1:** Train weak learner $h_t$ on weighted data\n",
    "$$h_t = \\arg\\min_h \\sum_{i=1}^n w_i^{(t)} \\mathbb{1}(h(x_i) \\neq y_i)$$\n",
    "\n",
    "**Step 2:** Compute weighted error\n",
    "$$\\epsilon_t = \\frac{\\sum_{i=1}^n w_i^{(t)} \\mathbb{1}(h_t(x_i) \\neq y_i)}{\\sum_{i=1}^n w_i^{(t)}}$$\n",
    "\n",
    "**Step 3:** Compute classifier weight\n",
    "$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "**Step 4:** Update sample weights\n",
    "$$w_i^{(t+1)} = w_i^{(t)} \\exp(-\\alpha_t y_i h_t(x_i))$$\n",
    "\n",
    "### 2.3 Why These Formulas? - The Exponential Loss Perspective\n",
    "\n",
    "**Key Insight:** AdaBoost can be viewed as **coordinate descent** on the exponential loss function.\n",
    "\n",
    "**Exponential Loss:** $L(y, f(x)) = e^{-yf(x)}$\n",
    "\n",
    "**Objective:** Minimize\n",
    "$$\\mathcal{L}(F) = \\sum_{i=1}^n e^{-y_i F(x_i)}$$\n",
    "where $F(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$.\n",
    "\n",
    "### 2.4 Derivation of AdaBoost Updates\n",
    "\n",
    "**At iteration $t$, we have:** $F_{t-1}(x) = \\sum_{s=1}^{t-1} \\alpha_s h_s(x)$\n",
    "\n",
    "**Goal:** Find $\\alpha_t$ and $h_t$ to minimize:\n",
    "$$\\mathcal{L}(F_{t-1} + \\alpha_t h_t) = \\sum_{i=1}^n e^{-y_i(F_{t-1}(x_i) + \\alpha_t h_t(x_i))}$$\n",
    "\n",
    "$$= \\sum_{i=1}^n e^{-y_i F_{t-1}(x_i)} e^{-y_i \\alpha_t h_t(x_i)}$$\n",
    "\n",
    "**Let $w_i^{(t)} = e^{-y_i F_{t-1}(x_i)}$ (these are the sample weights!)**\n",
    "\n",
    "$$\\mathcal{L}(F_{t-1} + \\alpha_t h_t) = \\sum_{i=1}^n w_i^{(t)} e^{-y_i \\alpha_t h_t(x_i)}$$\n",
    "\n",
    "### 2.5 Optimal $\\alpha_t$ Derivation\n",
    "\n",
    "**Separate correct and incorrect predictions:**\n",
    "$$\\mathcal{L} = \\sum_{i: h_t(x_i) = y_i} w_i^{(t)} e^{-\\alpha_t} + \\sum_{i: h_t(x_i) \\neq y_i} w_i^{(t)} e^{\\alpha_t}$$\n",
    "\n",
    "**Let:**\n",
    "- $W_{\\text{correct}} = \\sum_{i: h_t(x_i) = y_i} w_i^{(t)}$\n",
    "- $W_{\\text{incorrect}} = \\sum_{i: h_t(x_i) \\neq y_i} w_i^{(t)}$\n",
    "\n",
    "$$\\mathcal{L} = W_{\\text{correct}} e^{-\\alpha_t} + W_{\\text{incorrect}} e^{\\alpha_t}$$\n",
    "\n",
    "**Taking derivative and setting to zero:**\n",
    "$$\\frac{d\\mathcal{L}}{d\\alpha_t} = -W_{\\text{correct}} e^{-\\alpha_t} + W_{\\text{incorrect}} e^{\\alpha_t} = 0$$\n",
    "\n",
    "**Solving:**\n",
    "$$W_{\\text{correct}} e^{-\\alpha_t} = W_{\\text{incorrect}} e^{\\alpha_t}$$\n",
    "$$\\frac{W_{\\text{correct}}}{W_{\\text{incorrect}}} = e^{2\\alpha_t}$$\n",
    "$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{W_{\\text{correct}}}{W_{\\text{incorrect}}}\\right)$$\n",
    "\n",
    "**Since $\\epsilon_t = \\frac{W_{\\text{incorrect}}}{W_{\\text{correct}} + W_{\\text{incorrect}}}$:**\n",
    "\n",
    "$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "**This derives the AdaBoost $\\alpha_t$ formula!**\n",
    "\n",
    "### 2.6 AdaBoost Convergence Theory\n",
    "\n",
    "**Training Error Bound:**\n",
    "\n",
    "**Theorem:** The training error of AdaBoost after $T$ rounds is bounded by:\n",
    "$$\\text{Error}_{\\text{train}} \\leq \\prod_{t=1}^T 2\\sqrt{\\epsilon_t(1-\\epsilon_t)}$$\n",
    "\n",
    "**Proof:**\n",
    "The number of misclassified training examples is:\n",
    "$$\\sum_{i=1}^n \\mathbb{1}(H(x_i) \\neq y_i) \\leq \\sum_{i=1}^n e^{-y_i F(x_i)}$$\n",
    "\n",
    "From the exponential loss minimization:\n",
    "$$\\sum_{i=1}^n e^{-y_i F(x_i)} = \\prod_{t=1}^T Z_t$$\n",
    "\n",
    "where $Z_t = \\sum_{i=1}^n w_i^{(t)} e^{-y_i \\alpha_t h_t(x_i)}$ is the normalization factor.\n",
    "\n",
    "**Computing $Z_t$:**\n",
    "$$Z_t = e^{-\\alpha_t} \\sum_{i: h_t(x_i)=y_i} w_i^{(t)} + e^{\\alpha_t} \\sum_{i: h_t(x_i) \\neq y_i} w_i^{(t)}$$\n",
    "$$= e^{-\\alpha_t}(1-\\epsilon_t) + e^{\\alpha_t}\\epsilon_t$$\n",
    "\n",
    "**Substituting $\\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$:**\n",
    "$$Z_t = 2\\sqrt{\\epsilon_t(1-\\epsilon_t)}$$\n",
    "\n",
    "**Key Insight:** If each weak learner has error $\\epsilon_t < \\frac{1}{2}$, then $Z_t < 1$, and training error decreases exponentially!\n",
    "\n",
    "### 2.7 Gradient Boosting - Functional Gradient Descent\n",
    "\n",
    "**Generalization:** Instead of exponential loss, minimize any differentiable loss $L(y, f(x))$.\n",
    "\n",
    "**Framework:**\n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$$\n",
    "\n",
    "where $h_m$ approximates the negative gradient:\n",
    "$$h_m(x) \\approx -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}$$\n",
    "\n",
    "### 2.8 Gradient Boosting for Regression (MSE Loss)\n",
    "\n",
    "**Loss function:** $L(y, f(x)) = \\frac{1}{2}(y - f(x))^2$\n",
    "\n",
    "**Gradient:** $\\frac{\\partial L}{\\partial f} = -(y - f(x)) = -r$ (negative residual)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize: $F_0(x) = \\bar{y}$\n",
    "2. For $m = 1$ to $M$:\n",
    "   - Compute residuals: $r_{i,m} = y_i - F_{m-1}(x_i)$\n",
    "   - Fit tree $h_m$ to residuals: $h_m = \\arg\\min_h \\sum_i (r_{i,m} - h(x_i))^2$\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\n",
    "\n",
    "**This is exactly fitting to residuals!**\n",
    "\n",
    "### 2.9 Why Boosting Reduces Bias\n",
    "\n",
    "**Theorem:** Under weak learning assumptions, boosting can reduce bias to arbitrarily small levels.\n",
    "\n",
    "**Weak Learning Assumption:** Each $h_t$ has error $\\epsilon_t \\leq \\frac{1}{2} - \\gamma$ for some $\\gamma > 0$.\n",
    "\n",
    "**Proof Sketch:**\n",
    "1. Each weak learner makes progress (however small) toward the correct answer\n",
    "2. Sequential combination allows complex decision boundaries\n",
    "3. Training error decreases exponentially (proven above)\n",
    "4. With enough rounds, can fit training data perfectly (bias → 0)\n",
    "\n",
    "**Trade-off:** Reducing bias may increase variance (overfitting risk).\n",
    "\n",
    "---\n",
    "\n",
    "## Part III: Bagging vs Boosting - Theoretical Comparison\n",
    "\n",
    "### 3.1 Bias-Variance Trade-offs\n",
    "\n",
    "| Method | Bias Effect | Variance Effect | Best For |\n",
    "|--------|-------------|-----------------|----------|\n",
    "| **Bagging** | Preserves bias | Reduces variance | High-variance, low-bias models |\n",
    "| **Boosting** | Reduces bias | May increase variance | High-bias, low-variance models |\n",
    "\n",
    "### 3.2 Mathematical Justification\n",
    "\n",
    "**Bagging MSE:**\n",
    "$$\\text{MSE}_{\\text{bag}} = \\text{Bias}_{\\text{individual}}^2 + \\rho \\cdot \\text{Var}_{\\text{individual}} + \\sigma^2$$\n",
    "\n",
    "**Boosting MSE (simplified):**\n",
    "$$\\text{MSE}_{\\text{boost}} = \\text{Bias}_{\\text{reduced}}^2 + \\text{Var}_{\\text{increased}} + \\sigma^2$$\n",
    "\n",
    "### 3.3 Convergence Properties\n",
    "\n",
    "**Bagging:** Converges as $B \\to \\infty$ to the **average** of infinite bootstrap predictors.\n",
    "\n",
    "**Boosting:** With proper regularization, converges to the **maximum margin** classifier.\n",
    "\n",
    "**AdaBoost Margin Theorem:** AdaBoost maximizes the minimum margin of training examples, leading to good generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## Part IV: Practical Implications\n",
    "\n",
    "### 4.1 When to Use Which Method\n",
    "\n",
    "**Use Bagging when:**\n",
    "- Base model has high variance (e.g., deep decision trees)\n",
    "- Want to parallelize training\n",
    "- Need stable, robust predictions\n",
    "- Have noisy data\n",
    "\n",
    "**Use Boosting when:**\n",
    "- Base model has high bias (e.g., shallow trees, linear models)\n",
    "- Can afford sequential training\n",
    "- Need maximum predictive accuracy\n",
    "- Data is relatively clean\n",
    "\n",
    "### 4.2 Hyperparameter Effects\n",
    "\n",
    "**Bagging:**\n",
    "- More estimators → Lower variance (diminishing returns)\n",
    "- Deeper base models → Better individual performance, still variance reduction\n",
    "\n",
    "**Boosting:**\n",
    "- More rounds → Lower bias but higher overfitting risk\n",
    "- Lower learning rate → Slower convergence but better generalization\n",
    "- Shallow base models → Better weak learners for boosting\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The mathematical theory shows that:\n",
    "\n",
    "1. **Bagging works** by creating diverse predictors through bootstrap sampling, reducing variance while preserving bias\n",
    "2. **Boosting works** by sequential error correction, focusing on difficult examples to reduce bias\n",
    "3. Both methods have **theoretical guarantees** under appropriate assumptions\n",
    "4. The choice between them depends on the **bias-variance characteristics** of your base model\n",
    "\n",
    "These aren't just empirical techniques—they're **mathematically principled** approaches to ensemble learning with proven theoretical foundations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
